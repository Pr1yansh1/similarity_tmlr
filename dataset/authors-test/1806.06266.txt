On Strategyproof Conference Peer Review
Yichong Xu˚

yichongx@cs.cmu.edu

Machine Learning Department
Carnegie Mellon University, Pittsburgh, PA, USA

Han Zhao˚

han.zhao@cs.cmu.edu

arXiv:1806.06266v3 [cs.GT] 1 Feb 2020

Machine Learning Department
Carnegie Mellon University, Pittsburgh, PA, USA

Xiaofei Shi

xiaofeis@andrew.cmu.edu

Department of Mathematical Sciences
Carnegie Mellon University, Pittsburgh, PA, USA

Jeremy Zhang

jbz@andrew.cmu.edu

Computer Science Department
Carnegie Mellon University, Pittsburgh, PA, USA

Nihar B. Shah

nihars@cs.cmu.edu

Machine Learning Department and Computer Science Department
Carnegie Mellon University, Pittsburgh, PA, USA

Abstract
We consider peer review in a conference setting where there is typically an overlap
between the set of reviewers and the set of authors. This overlap can incentivize strategic
reviews to influence the final ranking of one’s own papers. In this work, we address
this problem through the lens of social choice, and present a theoretical framework for
strategyproof and efficient peer review. We first present and analyze an algorithm for
reviewer-assignment and aggregation that guarantees strategyproofness and a natural
efficiency property called unanimity, when the authorship graph satisfies a simple property.
Our algorithm is based on the so-called partitioning method, and can be thought as a
generalization of this method to conference peer review settings. We then empirically show
that the requisite property on the authorship graph is indeed satisfied in the submission data
from the ICLR conference, and further demonstrate a simple trick to make the partitioning
method more practically appealing for conference peer review. Finally, we complement our
positive results with negative theoretical results where we prove that under various ways of
strengthening the requirements, it is impossible for any algorithm to be strategyproof and
efficient.

1. Introduction
Peer review serves as an effective solution for quality evaluation in reviewing processes,
especially in academic paper review (Dörfler et al., 2017; Shah et al., 2017) and massive
open online courses (MOOCs) (Díez Peláez et al., 2013; Piech et al., 2013; Shah et al., 2013).
However, despite its scalability, competitive peer review faces the serious challenge of being
vulnerable to strategic manipulations (Alon et al., 2011; Anderson et al., 2007; Kahng et al.,
2017; Kurokawa et al., 2015; Thurner and Hanel, 2011). By giving lower scores to competitive
∗. Equal contribution.

Xu, Zhao, Shi, Zhang & Shah

submissions, reviewers may be able to increase the chance that their own submissions get
accepted. For instance, a recent experimental study (Balietti et al., 2016) on peer review of
art, published in the Proceedings of the National Academy of Sciences (USA), concludes
“...competition incentivizes reviewers to behave strategically, which reduces the
fairness of evaluations and the consensus among referees.”
As noted by Thurner and Hanel (2011), even a small number of selfish, strategic reviewers
can drastically reduce the quality of scientific standard. In the context of conference peer
review, Langford (2008) calls academia inherently adversarial:
“It explains why your paper was rejected based on poor logic. The reviewer wasn’t
concerned with research quality, but rather with rejecting a competitor.”
Langford states that a number of people agree with this viewpoint. Thus the importance
of peer review in academia and its considerable influence over the careers of researchers
significantly underscores the need to design peer review systems that are insulated from
strategic manipulations.
In this work, we present a higher-level framework to address the problem of strategic
behavior in conference peer review. We present an informal description of the framework here
and formalize it later in the paper. The problem setting comprises a number of submitted
papers and a number of reviewers. We are given a graph which we term as the “conflict
graph”. The conflict graph is a bipartite graph with the reviewers and papers as the two
partitions of vertices, and an edge between any reviewer and paper if that reviewer has a
conflict with that paper. Conflicts may arise due to authorship (the reviewer is an author
of the paper) or other reasons such as being associated to the same institution. Given this
conflict graph, there are two design steps in the peer review procedure: (i) assigning each
paper to a subset of reviewers, and (ii) aggregating the reviews provided by the reviewers
to give a final evaluation of each paper. Under our framework, the goal is to design these
two steps of the peer-review procedure that satisfies two properties – strategyproofness and
efficiency.
Our goal is to design peer-review procedures that are strategyproof with respect to the
given conflict graph. A peer-review procedure is said to be strategyproof if no reviewer can
change the outcome for any paper(s) with which she/he has a conflict. This definition is
formalized later in the paper. Strategyproofness not only reassures the authors that the
review process is fair, but also ensures that the authors receive proper feedback for their
work. We note that a strategyproof peer-review procedure alone is inadequate with respect
to any practical requirements – simply giving out a fixed, arbitrary evaluation makes the
peer-review procedure strategyproof.
Consequently, in addition to requiring strategyproofness, our framework measures the
peer-review procedure with another yardstick – that of efficiency. Informally, the efficiency
of a peer-review procedure is a measurement of how well the final outcome reflects reviewers’
assessments of the quality of the submissions, or a measurement of the accuracy in terms of
the final acceptance decisions. There are several ways to define efficiency – from a social
choice perspective or a statistical perspective. In this paper, we consider efficiency in terms
of the notion of unanimity in social choice theory: an agreement among all reviewers must
be reflected in the final aggregation.
2

On Strategyproof Conference Peer Review

In addition to the conceptual contribution based on this framework, we make several
technical contributions towards this important problem. We first design a peer review
algorithm which theoretically guarantees strategyproofness along with a notion of efficiency
that we term “group unanimity”. Our result requires only a mild assumption on the conflict
graph of the peer-review design task. We show this assumption indeed holds true in practice
via an empirical analysis of the submissions made to the International Conference on Learning
Representations (ICLR) conference1 . Our algorithm is based on the popular partitioning
method, and our positive results can be regarded as generalizing it to the setting of conference
peer review. We further demonstrate a simple trick to make the partitioning method more
practically appealing for conference peer review and validate it on the ICLR data.
We then complement our positive results with negative results showing that one cannot
expect to meet requirements that are much stronger than that provided by our algorithm. In
particular, we show that under mild assumptions on the authorships, there is no algorithm
that can be both strategyproof and “pairwise unanimous”. Pairwise unanimity is a stronger
notion of efficiency than group unanimity, and is also known as Pareto efficiency in the
literature of social choice (Brandt et al., 2016). We show that our negative result continues
to hold even when the notion of strategyproofness is made extremely weak. We then provide
a conjecture and insightful results on the impossibility when the assignment satisfies a simple
“connectivity” condition. Finally, we connect back to the traditional settings in social choice
theory, and show an impossibility when every reviewer reviews every paper. These negative
results highlight the intrinsic hardness in designing strategyproof conference review systems.

2. Related Work
As early as in the 1970s, Gibbard and Satterthwaite had already been aware of the importance
of a healthy voting rule that is strategyproof in the setting of social choice (Gibbard, 1973;
Satterthwaite, 1975). Nowadays, the fact that prominent peer review mechanisms such as
the one used by the National Science Foundation (Hazelrigg, 2013) and the one for time
allocation on telescope (Merrifield and Saari, 2009) are manipulable has further called for
strategyproof peer review mechanisms.
Our work is most closely related to a series of works on strategyproof peer selection (Alon
et al., 2011; Aziz et al., 2016, 2019; De Clippel et al., 2008; Fischer and Klimm, 2015;
Holzman and Moulin, 2013; Kahng et al., 2017; Kurokawa et al., 2015), where agents cannot
benefit from misreporting their preferences over other agents.2 De Clippel et al. (2008)
consider strategyproof decision making under the setting where a divisible resource is shared
among a set of agents. Later, Alon et al. (2011); Holzman and Moulin (2013) consider
strategyproof peer approval voting where each agent nominates a subset of agents and the
goal is to select one agent with large approvals. Alon et al. (2011) propose a randomized
strategyproof mechanism using partitioning that achieves provable approximate guarantee
to the deterministic but non-strategyproof mechanism that simply selects the agent with
1. https://openreview.net/group?id=ICLR.cc/2017/conference
2. Some past literature refers to this requirement as ensuring that agents are “impartial”. However, the term
“impartial” also has connotations on (possibly implicit) biases due to extraneous factors such as some
features about the agents (Hojat et al., 2003). In this paper, we deliberately use the term “strategyproof”
in order to make the scope of our contribution clear in that we do not address implicit biases.

3

Xu, Zhao, Shi, Zhang & Shah

maximum approvals. Bousquet et al. (2014) and Fischer and Klimm (2015) further extended
and analyzed this mechanism to provide an optimal approximate ratio in expectation.
Although the first partitioning-based mechanism partitions all the voters into two disjoint
subsets, this has been recently extended to k-partition by Kahng et al. (2017). In all these
works, each agent is essentially required to evaluate all the other agents except herself. This is
impractical for conference peer review, where each reviewer only has limited time and energy
to review a small subset of submissions. In light of such constraints, Kurokawa et al. (2015)
propose an impartial mechanism (Credible Subset) and provide associated approximation
guarantees for a setting in which each agent is only required to review a few other agents.
Credible Subset is a randomized mechanism that outputs a subset of k agents, but it has
non-zero probability returns an empty set. Based on the work of De Clippel et al. (2008),
Aziz et al. (2016) propose a mechanism for peer selection, termed as Dollar Partition, which is
strategyproof and satisfies a natural monotonicity property. Empirically the authors showed
that Dollar Partition outperforms Credible Subset consistently and in the worst case is better
than partition-based approach. However, even if the target output size is k, Dollar Partition
may return a subset of size strictly larger than k. This problem has recently been fixed
by the Exact Dollar Partition mechanism (Aziz et al., 2019), which empirically selects more
high-quality agents more often and consistently than Credible Subset. Our positive results,
specifically our Divide-and-Rank algorithm presented subsequently, borrows heavily from this
line of literature. That said, our work addresses the application of conference peer review
which is more general and challenging as compared to the settings considered in past works.
Our setting of conference peer review is more challenging as compared to these past
works as each reviewer may author multiple papers and moreover each paper may have
multiple authors as reviewers. Specifically, the conflict graph under conference peer review is
a general bipartite graph, where conflicts between reviewers and papers can arise not only
because of authorships, but also advisor-advisee relationships, institutional conflicts, etc. In
contrast, past works focus on applications of peer-grading and grant proposal review, and
hence consider only one-to-one conflict graphs (that is, where every reviewer is conflicted
with exactly one paper).
Apart from the most important difference mentioned above, there are a couple of other
differences of this work as compared to some past works. In this paper we focus on ordinal
preferences where each reviewer is asked to give a total ranking of the assigned papers, as
opposed to providing numeric ratings. We do so inspired by past literature (Barnett, 2003;
Douceur, 2009; Shah et al., 2016, 2013; Stewart et al., 2005; Tsukida and Gupta, 2011) which
highlights the benefits of ordinal data in terms of avoiding biases as well as allowing for a
more direct comparison between papers. Secondly, while most previous mechanisms either
output a single paper or a subset of papers, we require our mechanism to output a total
ranking over all papers. We consider this requirement since this automated output in practice
will be used by the program chairs as a guideline to make their decisions, and this more
nuanced data comprising the ranking of the papers can be more useful towards this goal.
A number of papers study various other aspects of conference peer review, and we mention
the most relevant ones here. Several works (Charlin and Zemel, 2013; Garg et al., 2010;
Hartvigsen et al., 1999; Stelmakh et al., 2019b) design algorithms for assigning reviewers to
papers under various objectives, and these objectives and algorithms may in fact be used
as alternative definitions of the objective of “efficiency” studied in the present paper. The
4

On Strategyproof Conference Peer Review

papers Ge et al. (2013); Roos et al. (2011); Wang and Shah (2019) consider review settings
where reviewers provide scores to each paper, with the aim of addressing the problems of
miscalibration in these scores. Stelmakh et al. (2019a); Tomkins et al. (2017) study biases
in peer review, Noothigattu et al. (2018) address issues of subjectivity, Gao et al. (2019)
investigate rebuttals, and Fiez et al. (2019) improve the efficiency of the bidding process.
Experiments and empirical evaluations of conference peer reviews can be found in Connolly
et al. (2014); Gao et al. (2019); Lawrence and Cortes (2014); Mathieus (2008); Noothigattu
et al. (2018); Shah et al. (2017); Tomkins et al. (2017).

3. Problem setting
In this section, we first give a brief introduction to the setting of our problem, and then
introduce the notation used in the paper. We then formally define various concepts and
properties to be discussed in the subsequent sections.
Modern review process is governed by four key steps: (i) a number of papers are submitted
for review; (ii) each paper is assigned to a set of reviewers; (iii) reviewers provide their
feedback on the papers they are reviewing; and (iv) the feedback from all reviewers is
aggregated to make final decisions on the papers. Let m be the number of reviewers and n
be the number of submitted papers. Define R :“ tr1 , . . . , rm u to be the set of m reviewers
and P :“ tp1 , . . . , pn u to be the set of n submitted papers.
The review process must deal with conflicts of interest. To characterize conflicts of
interest, we use a bipartite graph C with vertices pR, Pq, where an edge is connected between
a reviewer r and a paper p if there exists some conflict of interests between reviewer r and
paper p. Reviewers who do not have conflicts of interest with any paper are nodes with no
edges. Given the set of submitted papers and reviewers, this graph is fixed and cannot be
controlled. Note that the conflict graph C defined above can be viewed as a generalization of
the authorship graph in the previously-studied settings (Alon et al., 2011; Aziz et al., 2016;
Fischer and Klimm, 2015; Holzman and Moulin, 2013; Kahng et al., 2017; Kurokawa et al.,
2015; Merrifield and Saari, 2009) of peer grading and grant proposal review, where each
reviewer (paper) is connected to at most one paper (reviewer).
The review process is modeled by a second bipartite graph G, termed as review graph,
that also has the reviewers and papers pR, Pq as its vertices. This review graph has an edge
between a reviewer and a paper if that reviewer reviews that paper. For every reviewer
ri pi P rmsq,3 we let Pi Ď P denote the set of papers assigned to this reviewer for review,
or in other words, the neighborhood of node ri in the bipartite graph G. The program
chairs of the conference are free to choose this graph, but subject to certain constraints and
preferences. To ensure balanced workloads across reviewers, we require that every reviewer
is assigned at most µ papers for some integers 1 ď µ ď n. In other words, every node in R
has at most µ neighbors (in P) in graph G. Additionally, each paper must be reviewed by
a certain minimum number of reviewers, and we denote this minimum number as λ. Thus
every node in the set P must have at least λ neighbors (in R) in the graph G. For any
(directed or undirected) graph H, we let the notation EH denote the set of (directed or
undirected, respectively) edges in graph H.
3. We use the standard notation rκs to represent the set t1, . . . , κu for any positive integer κ.

5

Xu, Zhao, Shi, Zhang & Shah

At the end of the reviewing period, each reviewer provides a total ranking of the papers
that she/he reviewed. For any set of papers P 1 Ď P, we let ΠpP 1 q denote the set of all
permutations of papers in P 1 . Furthermore, for any paper pj P P 1 and any permutation
πpP 1 q P ΠpP 1 q, we let πj pP 1 q denote the position of paper pj in the permutation πpP 1 q. At the
end of the reviewing period, each reviewer ri pi P rmsq submits a total ranking π piq pPi q P ΠpPi q
of the papers in Pi . We define a (partial) ranking profile π :“ pπ p1q pP1 q, . . . , π pmq pPm qq as
the collection of rankings from all the reviewers. When the assignment P1 , . . . , Pm of papers
to reviewers is fixed, we use the shorthand pπ p1q , . . . , π pmq q for profile π. For any subset of
papers P 1 Ď P, we let π P 1 denote the restriction of π to only the induced rankings on P 1 .
Finally, when the ranking under consideration is clear from context, we use the notation
p ą p1 to say that paper p is ranked higher than paper p1 in the ranking.
Under this framework, the goal is to jointly design: (a) a paper-reviewer assignment
scheme,
that is, edges of the graph G, and (b) an associated review aggregation rule
ś
f: m
ΠpP
i q Ñ ΠpPq which maps from the ranking profile to an aggregate total ranking
i“1
of all papers.4 For any aggregation function f , we let fj pπq be the position of paper pj when
the input to f is the profile π.
We note that although we assume ordinal feedback from the reviewers, our results continue
to hold if we have review scores as our input instead of rankings; our framework is flexible
enough to take the scores into account (cf. Section 4.1).
In what follows we define strategyproofness and efficiency that any conference review
mechanism f should satisfy under our paper-review setting. Inspired by the theory of social
choice, in this paper we define the notion of efficiency via two variants of “unanimity”, and
we also discuss two natural notions of strategyproofness.
3.1 Strategyproofness
Intuitively, strategyproofness means that a reviewer cannot benefit from being dishonest.
In the context of conference review, strategyproofness is defined with respect to a given
conflict graph C; we recall the notation EC as the set of edges of graph C. It means that a
reviewer cannot change the position of her conflicting papers, by manipulating the ranking
she provides.
Definition 3.1 (Strategyproofness, SP). A review process pG, f q is called strategyproof
with respect to a conflict graph C if for every reviewer ri P R and paper pj P P such that
pri , pj q P EC the following condition holds: for every pair of profiles (under assignment G) that
differ only in the ranking given by reviewer ri , the position of pj is unchanged.5 Formally, @π “
1
pπ p1q , . . . , π pi´1q , π piq , π pi`1q , . . . , π pmq q and π 1 “ pπ p1q , . . . , π pi´1q , π piq , π pi`1q , . . . , π pmq q, it
must be that fj pπq “ fj pπ 1 q.
A strategyproof peer review procedure alone is inadequate with respect to any practical
requirements – simply giving out a fixed, arbitrary evaluation makes the peer review procedure
4. To be clear, the function f is tied to the assignment graph G. The graph G specifies the sets pP1 , . . . , Pm q,
and then the function f takes permutations of these sets of papers as its inputs. We omit this from the
notation for brevity.
5. A related (and weaker) definition of strategyproof is that the position of any pj cannot be improved.
It is easy to show that any mechanism that satisfies the weaker notion can also satisfy our notion of
strategyproofness.

6

On Strategyproof Conference Peer Review

strategyproof. We therefore consider efficiency of the procedure in the next section, to ensure
that the authors receive meaningful and helpful feedback for their work.
3.2 Efficiency (unanimity)
Consequently, in addition to requiring strategyproofness, we measure the peer review procedure with another yardstick – efficiency. The peer review procedure needs to not only
reassure the authors that the review process is fair, but also ensure that the authors receive
proper feedback for their work in an efficient way.
In this work, we consider efficiency of a peer-review process in terms of the notion of
unanimity. Unanimity is one of the most prevalent and classic properties to measure the
efficiency of a voting system in the theory of social choice (Fishburn, 2015). At a colloquial
level, unanimity states that when there is a common agreement among all reviewers, then
the aggregation of their opinions must also respect this agreement. In this paper we discuss
two kinds of unanimity, termed group unanimity (GU) and pairwise unanimity (PU). Both
kinds of unanimity impose requirements on the aggregation function for any given reviewer
assignment.
We first define group unanimity:
Definition 3.2 (Group Unanimity, GU). We define pG, f q to be group unanimous (GU)
if the following condition holds for every possible profile π. If there is a non-empty set of
papers P 1 Ă P such that every reviewer ranks the papers she reviewed from P 1 higher than
those she reviewed from PzP 1 , then f pπq must have px ą py for every pair of papers px P P 1
and py P PzP 1 such that at least one reviewer has reviewed both px and py .
Intuitively, group unanimity says that if papers can be partitioned into two sets such
that every reviewer who has reviewed papers from both sets agrees that the papers she has
reviewed from the first set are better than what she reviewed from the second set, then the
final output ranking should respect this agreement.
Our second notion of unanimity, termed pairwise unanimity, is a local refinement of group
unanimity. This notion is identical to the classical notion of unanimity stated in Arrow’s
impossibility theorem (Arrow, 1950) – the classical unanimity considers every reviewer to
review all papers (that is, Pi “ P, @i P rms), whereas our notion is also defined for settings
where reviewers may review only subsets of papers.
Definition 3.3 (Pairwise Unanimity, PU). We define pG, f q to be pairwise unanimous
(PU) if the following condition holds for every possible profile π and every pair of papers
pj1 , pj2 P P: If at least one reviewer has reviewed both pj1 and pj2 and all the reviewers that
have reviewed pj1 and pj2 agree on pj1 ą pj2 , then fj1 pπq ą fj2 pπq.
An important property is that pairwise unanimity is stronger than group unanimity.
Proposition 3.4. If pG, f q is pairwise unanimous, then pG, f q is also group unanimous.
The proof of this proposition is provided in Section 7.1.
7

Xu, Zhao, Shi, Zhang & Shah

4. Positive Theoretical Results and Algorithm
In this section we consider the design of reviewer assignments and aggregation rules for
strategyproofness and group unanimity (efficiency). It is not hard to see that strategyproofness
and group unanimity cannot be simultaneously guaranteed for arbitrary conflict graphs C,
for instance, when C is a fully-connected bipartite graph. Prior works on this topic consider a
specific class of conflict graphs — those with one-to-one relations between papers and reviewers
— which do not capture conference peer review settings. We consider a more general class
of conflict graphs and present an algorithm based on the partitioning-based method (Alon
et al., 2011), which we show can achieve group unanimous and strategyproofness.
We then empirically demonstrate, using submission data from the ICLR conference, that
this class of conflict graphs is indeed representative of peer review settings. We observe that
the quality of the reviewer assignment under our method (that guarantees strategyproofness)
is only slightly lower as compared to the optimal quality in the absence of strategyproofing
requirements. Finally, we present a simple trick to significantly improve the practical appeal
of our algorithm (and more generally the partitioning method) to conference peer review.
4.1 The Divide-and-Rank Algorithm
We now present our “Divide-and-Rank” framework consisting of the reviewer assignment
algorithm (Algorithm 1) and the rank aggregation algorithm (Algorithm 2). At a high level,
our algorithm performs a partition of the reviewers and papers for assignment, and aggregates
the reviews by computing a ranking which is consistent with any group agreements. The
Divide-and-Rank algorithm works for a general conflict graph C as long as the conflict graph
can be divided into two reasonably-sized disconnected components.
Importantly, the framework is simple yet flexible in that the assignment within each
partition and the aggregation among certain groups of papers can leverage any existing
algorithm for assignment and aggregation respectively, which is useful as it allows to further
optimize various other metrics in addition to strategyproofness and unanimity.
Below we describe our framework in more detail. We first introduce the assignment
procedure in Algorithm 1.
The Divide-and-Rank assignment algorithm begins by partitioning the conflict graph into
two disconnected components such that (1) they meet the requirements specified by µ and
λ; and (2) the two disconnected components have roughly equal size in terms of number of
nodes. This is achieved using the subroutine Partition. In more detail, Partition first runs a
breadth-first-search (BFS) algorithm to partition the original conflict graph into K connected
components, where the kth connected component contains rk ě 0 reviewers and pk ě 0
papers. Next, the algorithm performs a dynamic programming to compute all the possible
subset sums, i.e., sum of the number of reviewers and the number of papers in a given subset,
achievable by the K connected components. Here T rk, r, ps “ 1 means that there exists a
partition of the first k components such that one side of the partition has r reviewers and p
papers, and 0 otherwise. The last step is to check whether there exists a subset C satisfying
the constraint given by λ and µ, and if so, runs a standard backtracking algorithm along the
table to find the actual subset C. Clearly the Partition runs in OpKnmq, and since K ď nm,
it runs in polynomial time in the size of the input conflict graph C.
8

On Strategyproof Conference Peer Review

Algorithm 1 Divide-and-Rank assignment
Input: conflict graph C, parameters λ, µ, assignment algorithm A
Output: an assignment of reviewers to papers
1: pRC , PC q, pRCs , PCs q Ð PartitionpC, λ, µq
2: use algorithm A to assign papers PCs to reviewers RC
3: use algorithm A to assign papers PC to reviewers RCs
4: return the union of assignments from step 2 and 3
5:
6: procedure Partition(conflict graph C, parameters λ, µ)

run a BFS on C to get connected K components tpRk , Pk quK
k“1
8:
let rk “ |Rk |, pk “ |Pk |, @k P rKs
9:
initialize a table T r¨, ¨, ¨s P t0, 1uKˆpm`1qˆpn`1q so that T r1, r1 , p1 s “ T r1, 0, 0s “ 1,
otherwise 0
10:
for k “ 2 to K do
11:
T rk, r, ps “ T rk ´ 1, r, ps _ T rk ´ 1, r ´ rk , p ´ pk s, @0 ď r ď m, 0 ď p ď n
12:
end for
p
µ
13:
for 0 ď r ď m, 0 ď p ď n, if there is no T rK, r, ps “ 1 such that maxt m´r
, n´p
r u ď λ,
return error
14:
use the standard backtracking in the table T r¨, ¨, ¨s to return pRC , PC q and pRCs , PCs q
15: end procedure
7:

In the next step, the algorithm assigns papers to reviewers in a fashion that guarantees
each paper is going to be reviewed by at least λ reviewers and each reviewer reviews at most
µ papers. The assignment of papers in any individual component (to reviewers in the other
component) can be done using any assignment algorithm (taken as an input A) as long as the
algorithm can satisfy the pµ, λq-requirements. Possible choices for the algorithm A include
the popular Toronto paper matching system (Charlin and Zemel, 2013) and others (Garg
et al., 2010; Hartvigsen et al., 1999; Stelmakh et al., 2019b). We can also use the typical
reviewer bidding system, while constraining the reviewers in RC to review PCs and RCs to
review PC .
We then introduce to the aggregation procedure in Algorithm 2. At a high level, the
papers in each component are aggregated separately using the subroutine Contract-andSort. This aggregation in Contract-and-Sort is performed by a topological ordering of all
strongly connected components (SCCs) according to the reviews, and then ranking the
papers within each set using any arbitrary aggregation algorithm (taken as an input B).6
Possible choices for the algorithm B include the modified Borda count (Emerson, 2013),
Plackett-Luce aggregation (Hajek et al., 2014), or others (Caragiannis et al., 2017) Moving
back to Algorithm 2, the two rankings returned by Contract-and-Sort respectively for the
two components are simply interlaced to obtain a total ranking over all the papers: the slots
for C are reserved in set I, and rnszI contain the slots for the remaining papers. In our
extended version of the paper we also show that the interleaving only causes a small change
w.r.t an underlying optimal ranking.
The following theorem now shows that Divide-and-Rank satisfies group unanimity and is
also strategyproof, detailed proof is in Section 7.2.
6. In the case where there are multiple topological orderings, any one of them suffices.

9

Xu, Zhao, Shi, Zhang & Shah

Algorithm 2 Divide-and-Rank aggregation
Input: profile π “ pπ p1q pP1 q, . . . , π pmq pPm qq, groups pRC , PC q, pRCs , PCs q with |PC | ě |PCs |,
aggregation algorithm B
Output: total ranking of all papers
1: compute π C as the restriction of profile π to only papers in PC , and π C̄ as the restriction
of profile π to only papers in PCs
2: πC Ð Contract-and-Sortpπ C , Bq
3: πCs Ð Contract-and-Sortpπ
´Y
] Y
]C̄ , Bq ¯
, |P2nC | , ..., n
5: return total ranking obtained by filling papers in PC into positions in I in order given
by πC , and papers in PCs into positions in rnszI in order given by πCs
4: define I “

n
|PC |

6:

r , aggregation algorithm B)
7: procedure Contract-and-Sort(profile π
r as its vertices and no edges
build a directed graph Gπr with the papers in π
1
9:
for each i P rm s do
10:
denoting π piq “ ppi1 ą . . . ą piti ), add a directed edge from pij to pij`1 in Gπr ,
@j P rti ´ 1s
11:
end for
12:
for every ordered pair ppj1 , pj2 q P EGπr , replace multiple edges from pj1 to pj2 with a
single edge
13:
compute a topological ordering of the strongly connected components (SCCs) in Gπr
14:
for every SCC in Gπr , compute a permutation of the papers in the component using
algorithm B
15:
return the permutation of all papers that is consistent with the topological ordering
of the SCCs and the permutations within the SCCs
16: end procedure
8:

Theorem 4.1. Suppose the vertices of C can be partitioned into two groups pRC , PC q and
|PC | |PCs | (
pRCs , PCs q such that there are no edges in C across the groups and that max |R
,
ď µλ .
s | |RC |
C
Then Divide-and-Rank is group unanimous and strategyproof.
Remark. Our Divide-and-Rank framework aptly handles the various nuances of real-world
conferences peer review, which render other algorithms inapplicable. This includes the
aspects that each reviewer can write multiple papers and each paper can have multiple
authors, and furthermore that each reviewer may review only a subset of papers. Even under
this challenging setting, our algorithm guarantees that no reviewer can influence the ranking
of her own paper via strategic behavior, and it is efficient from a social choice perspective.
Further, we delve a little deeper into the interleaving step (Step 5) of the aggregation
algorithm. At first glance, this interleaving – performed independent of the reviewers’ reports
– may be a cause of concern. Indeed, assuming there is some ground truth ranking of all
papers and even under the assumption that the outputs of the Contract-and-Sort procedure
are consistent with this ranking, the worst case scenario is where the interleaving causes
papers to be placed at a positions that are Θpnq away from their respective positions in the
true ranking. We show that, however, such a worst case scenario is unlikely to arise, when
10

On Strategyproof Conference Peer Review

the ground truth ranking is independent of the conflict graph. We summarize our findings in
the following proposition, with the proof provided later in Section 7.3.
Proposition 4.2. Suppose C satisfies the conditions given in Theorem 4.1 and there exists
|P|
a constant c ě 2 such that maxt |P
, |P|s | u ď c. Assume the ground-truth ranking π ˚ is
C | |PC
chosen uniformly at random from all permutations in ΠpPq independent of C, and that the
two partial outputs of Contract-and-Sort in Algorithm 2 respect π ˚ . Let the output ranking
p. Then for every n ě 4c{ log 2, for any δ P p0, 1q, with probability at
of Divide-and-Rank be π
least 1 ´ δ, we have:
a
pi | ď 2 nc ¨ logp2n{δq.
max |πi˚ ´ π
1ďiďn

Proposition 4.2 shows thata
the maximum deviation between the aggregated ranking and the
ground truth ranking is Op n logpn{δqq with high probability. Hence when n is large enough,
such deviation is negligible when program chairs of conferences need to make accept/reject
decisions, where the number of accepted papers usually scales linearly with n.
Extension to review scores. Our framework extends to a score-based setting, wherein
each reviewer ri provides their opinion as a score oij for every paper pj P Pi . The assignment
algorithm remains the same in this setting; for aggregation, we can use the same procedure
with the ranking induced by the review scores. The only difference is that in step 10 of
Contract-and-Sort, we add an edge between every pair of papers pj1 Ñ pj2 such that oij1 ą oij2 .
This makes sure that the graph reflects the opinion of the reviewer and does not impose
constraints on papers that are equally rated. In the score-based setting, the aggregation
algorithm B is allowed to leverage the review scores for a more granularized ranking (e.g.,
mean scores).

5. Empirical evaluations
In this section, we perform certain empirical evaluations regarding the feasibility and performance of our Divide-and-Rank algorithm based on data from the ICLR conference7 . Recall
that the Divide-and-Rank algorithm restricts the assignment of reviewers to papers according
to a partition of reviewers and papers into two disconnected groups. By means of these
empirical evaluations, we investigate the following questions:
Q1. Is such a partition feasible?
Q2. How can one impart more flexibility to the partition (which can allow for better
assignments)?
Q3. How does the quality of the assignment compare with standard settings without
strategyproofness?
Q4. One may envisage that reviewers that are more related to the topic of a paper would be
more likely to be connected (in the conflict graph) to that paper. Under Divide-and-Rank,
7. The code and data is available at https://github.com/xycforgithub/StrategyProof_Conference_
Review.

11

Xu, Zhao, Shi, Zhang & Shah

such a reviewer will be barred from being assigned to such a related paper. How much
does such a restriction of the assignment between connected reviewers-papers hurt the
assignment quality as compared to assignment under a uniform random partition of
reviewers and papers?
The most prominent type of conflicts is authorships, and throughout this section we restrict
attention to the authorship conflict graph.
5.1 Analysis of the Conflict Graph on ICLR 2017 submissions (Q1 and Q2)
We address questions Q1 and Q2 using data from the ICLR 2017 conference. In a nutshell:
A1. Yes, partitioning is feasible.
A2. We show that removing only a small number of reviewers can result in a dramatic
reduction in the size of the largest component in the conflict graph thereby providing
great flexibility towards partitioning the papers and reviewers. For instance, removing
only 3.5% of all authors from the reviewer pool reduces the size of the largest component
(in terms of number of papers) by 86%.
We analyze all papers submitted to the ICLR 2017 conference with the given authorship
relationship as the conflict graph. ICLR 2017 received 489 submissions by 1417 authors;
we believe this dataset is a good representative of a medium-sized modern conference. In
the analysis of this dataset, we instantiate the conflict graph as the authorship graph. It is
important to note that we consider only the set of authors as the entire reviewer pool (since
we do not have access to the actual reviewer identities). Adding reviewers from outside the
set of authors would only improve the results since these additional reviewers will have no
edges in the authorship conflict graph.
Table 1: Statistics of ICLR 2017 submissions.

Description

Number

Number of submitted papers
Number of distinct authors
Mean # papers written per author
Maximum # papers written by an author
Number of connected components
#authors; #papers in largest connected component
#authors; #papers in second largest connected component

489
1417
1.27
14
253
371; 133
65; 20

We first investigate the existence of (moderately sized) components in the conflict graph.
Our analysis shows that the authorship conflict graph is disconnected, and moreover, has
more than 250 components. The largest connected component (CC) contains 133 (that is,
about 27%) of all papers, and the second largest CC is much smaller. We tabulate the results
from our analysis in Table 1. These statistics indeed verify our assumption in Theorem 4.1
that the conflict graph is disconnected and can be divided into two disconnected parts of
similar size.
12

On Strategyproof Conference Peer Review

The partitioning method has previously been considered for the problem of peer grading (Kahng et al., 2017). The peer grading setting is homogeneous in that each reviewer
(student) goes through the same course and hence any paper (homework) can be assigned
to any reviewer. In peer review, however, different reviewers typically have different areas
of expertise and hence their abilities to review any paper varies by the subject area of the
paper. In order to accommodate this diversity in area of expertise in peer review, one must
have a greater flexibility in terms of assigning papers to reviewers. In our analysis in Table 1
we saw that the largest connected component comprises 372 authors and 133 papers. It is
reasonable to expect that a large number of reviewers with expertise required to review these
133 papers may fall in the same connected component, meaning that a naïve application of
Divide-and-Rank to this data would assign these 133 papers to reviewers who may have a
lower expertise for these papers. This is indeed a concern, and in what follows, we discuss a
simple yet effective way to ameliorate this problem.
A simple yet (as we demonstrate below) effective idea is to remove some authors from
the reviewer pool. Empirically using the ICLR 2017 data, we show that by removing only a
small number of authors from the reviewer pool, we can make the conflict graph considerably
more sparse, thereby allowing for a significantly more flexible application of our algorithm
Divide-and-Rank (or more generally, any partition-based algorithm). We use the simple
heuristic of removing the authors with the maximum degree in the (authorship) conflict
graph. We then study the resulting conflict graph (containing all submitted papers but only
the remaining reviewers) in terms of the numbers and sizes of the connected components.
We present the results in Table 2. We see that on removing only a small fraction of authors
— 50 authors which is only about 3.5% of all authors — the number of papers in the largest
connected component reduces by 86% to just 18. Likewise, the number of authors in the
largest connected component reduces to 55 from 371 originally.
Table 2: Statistics of the conflict graph on removing a small number (ă 7%) of authors from the
reviewer pool comprising the 1417 authors.

#Authors removed from reviewer pool
0
5
10
15
20
50 100
Number of Components
Number of Authors in Largest CC
Number of Papers in Largest CC

253
371
133

268
313
114

278
304
110

292
228
82

302
205
74

334
55
18

389
28
8

5.2 Analysis of the Partition Algorithm on ICLR 2018 submissions (Q3 and
Q4)
In the previous section, we empirically verified that we can partition the reviewers and
papers into two disconnected groups. A natural question that arises is how such a partition
affects to the overall reviewer-paper matching process, i.e., will the partition cause a great
loss in quality of the assignment algorithm used in practice? We empirically investigate
this question by performing experiments using data from the ICLR 2018 conference (where
we also use ICLR 2017 as a reference point later). In a nutshell, using the popular “mean
similarity score” as a measure of the quality of the assignment (detailed below), we see that:
13

Xu, Zhao, Shi, Zhang & Shah

A3. In comparison to when there is no strategyproofing, the quality of the assignment
reduces by 11% when it is made strategyproof using the Partition algorithm.
A4. The utility under Partition is only marginally lower than when the partition is done
uniformly at random. Thus Partition is roughly equivalent to shrinking the conference
size (randomly) to a half.

10

6

10

5

10

4

10

3

10

2

10

1

10

0

Mean Similarity Score: 0.03

Frequency

Frequency

We now describe the experiment in more detail. We follow the assignment framework
used popularly in practice, which comprises of two phases. The first phase computes a
similarity score for every (reviewer, paper) pair. A higher similarity score is interpreted as a
higher envisaged quality of review. As in the case of ICLR 2017 above, we set the collection
of all authors as the reviewer pool since the actual identities of the collection of reviewers
are not available. We then compute a similarity between every reviewer-paper pair based
on the text of the paper and the contents of the reviewer’s published papers. We refer the
reader to Appendix A for details of this construction.
Here are some basic statistics about the computed similarity matrix. In Figure 1a we
plot the histogram of the computed similarity scores between 911 papers and 2435 reviewers.
The mean of the similarity scores across all reviewer-paper pairs is approximately 0.03. This
skewed distribution of similarity scores is consistent with our intuition: for each paper, there
is only a handful of reviewers who have the aligned background and expertise. In Figure 1b,
we show the histogram of the top similarity score computed for each paper (excluding
reviewers that are also authors of the corresponding paper). We see that the mean (across
all papers) of these top scores is approximately 0.14, which is significantly higher than that
of 0.03 among all similarity scores.

0.0

0.2

0.4
0.6
Similarity Score

0.8

10

2

10

1

10

0

Mean Top Score: 0.14

0.0

(a) Scores between reviewers and papers.

0.2

0.4
0.6
0.8
Top Score of each Paper

(b) Top score of each paper.

Figure 1: The left histogram is the similarity scores between reviewers and papers of the ICLR 2018
dataset. The right histogram plots the top similarity score of each paper in the ICLR 2018 dataset.
In both plots, the vertical black line shows the mean of the distribution.

The second phase of the assignment procedure uses the similarity scores to assign reviewers
to papers. The most widely used assignment method used in practice is the Toronto Paper
Matching System or TPMS (Charlin and Zemel, 2013). This assignment method maximizes
the mean similarity score across all assigned reviewer-paper pairs.
14

On Strategyproof Conference Peer Review

In what follows, we evaluate three different methods of assigning reviewers to papers in
terms of the resulting mean similarity score across all assigned reviewer-paper pairs:
• Using TPMS assignment without any partitioning.
• The Divide-and-Rank (i.e., partitioning reviewers and papers into two sets that are
disconnected in the authorship conflict graph) with TPMS as the assignment algorithm
A.
• Partitioning the reviewers and papers into two equal groups uniformly at random, and
using TPMS with the restriction of assigning each reviewers to papers from the other
group.
We use the values µ “ 6, λ “ 3 which are typical of conferences today. We provide specific
implementation details in Appendix A.
Basic statistics and experiment results are shown in Table 3. ICLR has grown dramatically
from 2017 to 2018, with the number of papers rising from 489 to 911, and the corresponding
numbers of authors and components also almost double. The largest component now has 757
authors and 274 components, twice the size of 2017. On the other hand, the second largest
component is smaller than that of 2017, which we speculate is because the machine learning
community has grown into more refined subfields, creating more smaller clusters. Nevertheless,
we are still able to divide the authors and papers into two clusters of approximately equal
size using Partition.
Table 3: Result of experiments on ICLR 2018 data.

Description

Number

Number of submitted papers
Number of distinct authors
Number of connected components
#authors, #papers in largest connected component
#authors, #papers in second largest connected component

911
2428
465
757, 274
30, 11

Mean similarity score without partitioning
Mean similarity score with random partition (20 runs)
Mean similarity score with Partition

0.0880
0.0779 ˘ 0.0001
0.0782

The mean of similarity scores using TPMS score assignment (see Appendix A.2) without
partitioning is 0.0880 and that with partition is 0.0782, which represents a 11.4% decrease
from partitioning. On the other hand, if we randomly partition the reviewers and papers to
two sets of equal sizes, the mean (and standard deviation) of the similarity is 0.0779 (and
0.0001 respectively) from 20 runs. The result from Partition is similar to the result obtained
from random partition. So in terms of author-paper similarities, Partition does no additional
harm than shrinking the conference size (randomly) to a half. For ICLR 2018, this just
corresponds to the size of ICLR 2017.
15

Xu, Zhao, Shi, Zhang & Shah

6. Negative Theoretical Results
The positive results in the previous section focus on group unanimity, which is weaker than
the conventional notion of unanimity (the conventional notion is also known as pairwise
unanimity). Moreover, the algorithm had a disconnected review graph whereas the review
graphs of conferences today are typically connected (Shah et al., 2017). It is thus natural to
wonder about the extent to which these results can be strengthened: Can a peer-review system
with a connected reviewer graph satisfy these properties? Can a strategyproof peer-review
system be pairwise unanimous? In this section we present some negative results toward these
questions, thereby highlighting the critical impediments towards (much) stronger results.
Before stating our results, we introduce another notion of strategyproofness, which is
significantly weaker than the notion of strategyproofness (Definition 3.1), and is hence termed
as weak strategyproofness. As compared to strategyproofness which is defined with respect
to a given conflict graph, weak strategyproofness only requires the existence of a conflict
graph (with non-zero reviewer-degrees) for which the review process is strategyproof.
Definition 6.1 (Weak Strategyproofness, WSP). A review process pG, f q is called weakly
strategyproof, if for every reviewer ri , there exists some paper pj P P such that for every
pair of distinct profiles (under assignment G) π “ pπ p1q , . . . , π pi´1q , π piq , π pi`1q , . . . , π pmq q and
1
π 1 “ pπ p1q , . . . , π pi´1q , π piq , π pi`1q , . . . , π pmq q, it is guaranteed that fj pπq “ fj pπ 1 q.
In other words, weak strategyproofness requires that for each reviewer there is at least
one paper (not necessarily shares conflicts this reviewer) whose ranking cannot be influenced
by the reviewer. As the name suggests, strategyproofness is strictly stronger than weak
strategyproofness, when each reviewer has at least one paper of conflict.
We define the notion of weak strategyproofness mainly for theoretical purposes to establish
negative results, since WSP is too weak to be practical. However, even this extremely weak
requirement is impossible to satisfy in situations of practical interest.
Table 4: Summary of our negative results (first three rows of the table), and a comparison to our
positve result (fourth row).

Unanimity
Pairwise
Group
Pairwise
Group

Strategyproof
None
Weak
Weak
Yes

Requirement on G
Mild (see Corollary 6.3)
Mild (Connected G)
Complete G
None

Possible?
No
Conjecture: No
No
Yes

Reference
Theorem 6.2
Proposition 6.4
Theorem 6.5
Theorem 4.1

We summarize our results in Table 4. Recall that we show the property of group unanimity
and strategyproof for Divide-and-Rank; as the first direction of possible extension, we show in
Theorem 6.2 that the slightly stronger notion of pairwise unanimity is impossible to satisfy
under mild assumptions, even without strategyproof constraints. Then in Section 6.2 we
explore the second direction of extension, by requiring a connected G; we give conjectures and
insights that group unanimity and weak strategyproofness is impossible under this setting.
At last in Theorem 6.5 we revert to the traditional setting of social choice, where every
reviewer gives a total ranking of the set of all papers P; we show that in this setting it is
impossible for any review process to be pairwise unanimous and weakly strategyproof.
16

On Strategyproof Conference Peer Review

6.1 Impossibility of Pairwise Unanimity
We show in this section that pairwise unanimity is too strong to satisfy under mild assumptions.
These assumptions are mild in the sense that a violation of the assumptions leads to severely
limited and somewhat impractical choices of G.
In order to precisely state our result, we first introduce the notion of a review-relation
graph H. Given a paper-review assignment tPi um
i“1 , the review-relation graph H is an
undirected graph with rns as its vertices and where any two papers pj1 and pj2 are connected
iff there exists at least one reviewer who reviews both the papers. With this preliminary in
place, we are now ready to state the main result of this section:
Theorem 6.2. If H has a cycle of length 3 or more and there is no single reviewer reviews
all the papers in the cycle, then there is no review process pG, f q that is pairwise unanimous.
The proof of Theorem 6.2 is similar to a Condorcet cycle proof, and the details are in
Section 7.4. In the corollary below we give some direct implications of the condition in
Theorem 6.2 when |P1 | “ ¨ ¨ ¨ “ |Pm | “ µ, that is, when every reviewer ranks a same number
of papers.
Corollary 6.3. Suppose |P1 | “ ¨ ¨ ¨ “ |Pm | “ µ ě 2. If pG, f q is pairwise unanimous, the
following conditions hold:
(i) H does not contain any cycles of length µ ` 1 or more.
(ii) The set of papers reviewed by any pair of reviewers ri1 and ri2 must satisfy the condition
|Pi1 X Pi2 | P t0, 1, µu. In words, if a pair of reviewers review more than one common
papers, they must review exactly the same set.
(iii) The number of distinct sets in Pi , . . . , Pm is at most n´1
µ´1 .
Remarks. In modern conferences (Shah et al., 2017), each reviewer usually reviews around
3 to 6 papers. If we make the review process pairwise unanimous, by Corollary 6.3 (iii) the
number of distinct review sets is much smaller than the number of reviewers; this severely
limits the design of review sets, since many reviewers would be necessitated to review identical
sets of papers. Corollary 6.3 (ii) is a related, strong requirement, since the specialization of
reviewers might not allow for such limiting of the intersection of review sets. For instance,
there are a large number of pairs of reviewers who review more than one common paper but
none with exactly the same set of papers (Shah et al., 2017).
In summary, Theorem 6.2 and Corollary 6.3 show that it is difficult to satisfy pairwise
unanimity, even without considering strategyproofness. This justifies our choice of group
unanimity in the positive results.
6.2 Group Unanimity and Strategyproof for a Connected Review Graph
Having shown that pairwise unanimity is too strong a requirement to satisfy, we now consider
another direction for extension – conditions on the review graph G. A natural question follows:
Under what condition on the review graph G are both group unanimity and strategyproofness
possible? Although we will leave the question of finding the exact condition open, we
conjecture that if we require G to be connected, then group unanimity and strategyproofness
17

Xu, Zhao, Shi, Zhang & Shah

cannot be simultaneously satisfied. To show our insights, we analyze an extremely simplified
review setting.
Proposition 6.4. Consider any n ě 4 and suppose P “ P1 YP2 YP3 YP4 , where P1 , P2 , P3 , P4
are disjoint nonempty sets of papers. Consider a review graph G with m “ 3 reviewers, where
reviewer r1 reviews tP1 , P2 u, r2 reviews tP2 , P3 u, and r3 reviews tP3 , P4 u. Then there is no
aggregation function f that is both weakly strategyproof and group unanimous.
Proposition 6.4 thus shows that for the simple review graph considered in the statement,
group unanimity and weak strategyproofness cannot hold at the same time. Proof details
can be found in Section 7.6.
We conjecture that such a negative result may hold for more general connected review
graphs, and such a negative result may be proved by identifying a component of the general
review graph that meets the condition of Proposition 6.4. This shows that our design process
of the review graph in Section 4 is quite essential for ensuring those important properties.
6.3 Pairwise Unanimity and Strategyproof under Total Ranking
Throughout the paper so far, motivated by the application of conference peer review, we
considered a setting where every reviewer reviews a (small) subset of the papers. In contrast,
a bulk of the classical literature in social choice theory considers a setting where each reviewer
ranks all candidates or papers (Arrow, 1950; Satterthwaite, 1975). Given this long line of
literature, intellectual curiosity drives us to study the case of all reviewers reviewing all
papers for our conference peer-review setting.
We now consider our notion of pairwise unanimous and weakly strategyproof in this
section under this total-ranking setting, where P1 “ ¨ ¨ ¨ “ Pm “ P. In this case, the review
graph G is always a complete bipartite graph, and it only remains to design the aggregation
function f . Although total rankings might not be practical for large-scale conferences, it is
still helpful for smaller-sized conferences and workshops.
Under this total ranking setting, we prove a negative result showing that pairwise
unanimity and strategyproofness cannot be satisfied together, and furthermore, even the
notion of weak strategyproofness (together with PU) is impossible to achieve.
Theorem 6.5. Suppose n ě 2. If P1 “ ¨ ¨ ¨ “ Pm “ P, then there is no aggregation function
f that is both no aggregation function f that is both and pairwise unanimous.
Note that the conditions required for Theorem 6.2 are not met in the total ranking case.
To prove Theorem 6.5(details in Section 7.7.1), we use Cantor’s diagonalization argument to
generate a contradiction by assuming there exists f that is both PU and WSP.
It is interesting to note that pairwise unanimity can be easily satisfied in this setting of
total rankings, by using a simple aggregation scheme such as the Borda count. However,
Theorem 6.5 shows that surprisingly, even under the extremely mild notion of strategyproofness given by WSP, it is impossible to achieve pairwise unanimity and strategyproofness
simultaneously.

7. Proofs
In this section, we provide the detailed proofs of all the results from previous sections.
18

On Strategyproof Conference Peer Review

7.1 Proof of Proposition 3.4
Suppose pG, f q is PU, and P 1 Ă P satisfies that every reviewer ranks the papers she reviewed
from P 1 higher than those she reviewed from PzP 1 . Now for every px P P 1 and py P PzP 1
and reviewer ri such that ri reviews both px and py , ri must rank px ą py since otherwise
the assumption of P 1 is violated. Since f is PU, we know that f pπq must respect px ą py as
well. This argument holds for every px P P 1 and py P PzP 1 that have been reviewed by at
least one reviewer, and hence pG, f q is also GU.
7.2 Proof of Theorem 4.1
We assume that the condition on the partitioning of the conflict graph, as stated in the
statement of this theorem, is met. We begin with a lemma which shows that for any
aggregation algorithm B, Contract-and-Sort is group unanimous.
Lemma 7.1. For any assignment and aggregation algorithms A and B, the aggregation
procedure Contract-and-Sort is group unanimous.
We prove this lemma in Section 7.2.1. Under the assumptions on µ, λ and sizes of
RC , RCs , PC , PCs , it is easy to verify that there is a paper allocation satisfies |Pi | ď µ, @ i P rms
and each paper gets at least λ reviews. The strategyproofness of Divide-and-Rank follows from
the standard ideas in the past literature on partitioning-based methods (Alon et al., 2011):
Algorithm 1 guarantees that reviewers in RC do not review papers in PCs , and reviewers
in RCs do not review papers in PC . Hence the fact that Divide-and-Rank is strategyproof
trivially follows from the assignment procedure where each reviewer does not review the
papers that are in conflict with her, as specified by the conflict graph C. Given that all the
other reviews are fixed, the ranking of the papers in conflict with her will only be determined
by the other group of reviewers and so fixed no matter how she changes her own ranking. On
the other hand, from Lemma 7.1, since Contract-and-Sort is group unanimous, we know that
πC and πCs respect group unanimity w.r.t. π C and π C̄ , respectively. Since π “ pπ C , π C̄ q, it
follows that πC and πCs also respect group unanimity w.r.t. π. Finally, note that there is no
reviewer who has reviewed both papers from PC and PCs , the interlacing step preserves the
group unanimity, which completes our proof.
7.2.1 Proof of Lemma 7.1
r is a preference profile. Define π “ f pr
Let f pr
π q :“ Contract-and-Sortpr
π , Bq, where π
π q. Let
r πr such that each of its
k denote the number of SCCs in Gπr . Construct a directed graph G
r πr iff
vertices represents a SCC in Gπr , and there is an edge from one vertex to another in G
there exists an edge going from one SCC to the other in the original graph Gπr . Let ṽ1 , . . . , ṽk
r πr . Since ṽ1 , . . . , ṽk is a topological ordering,
be a topological ordering of the vertices in G
then edges can only go from ṽj1 to ṽj2 where j1 ă j2 . Now consider any cut pPX , PY q in Gπr
that satisfies the requirement of group unanimity, i.e., all edges in the cut direct from PX to
PY . Then there is no pair of papers px P PX and py P PY such that px and py are in the
same connected component, otherwise there will be both paths from px to py and py to px ,
contradicting that pPX , PY q forms a cut where all the edges go in one direction. This shows
that PX and PY also form a partition of all the vertices ṽ1 , . . . , ṽk . Now consider any edge
ppx , py q from PX to PY . Suppose px is in component ṽjx and py in component ṽjy . We have
19

Xu, Zhao, Shi, Zhang & Shah

jx ‰ jy , since PX and PY forms a partition of all SCCs; also it cannot happen that jx ą jy ,
otherwise ṽ1 , . . . , ṽk is not a topological ordering returned by f . So it must be jx ă jy , and
the edge ppx , py q is respected in the final ordering.
7.3 Proof of Proposition 4.2
We would first need a lemma for the location of papers:
] Y
]
)
! ´
¯ ´
¯
)
!Y
2n
n
2n
n
Lemma 7.2. Let I1 “
,
,
...,
n
,
I
“
g
,
g
,
...,
n
´
1
, where
2
|PC |
|PC |
|PCs |
|PCs |
gpxq “ rxs ´ 1 is the largest integer that is strictly smaller than x. Then I1 X I2 “ H and
I1 Y I2 “ rns.
We prove this lemma in Section 7.3.1.
Consider any paper pi , and suppose its position in π ˚ is `. Define n1 “ |PC | and
n2 “ |PC̄ |. Without loss of generality assume n1 ě n2 (the other case is symmetric) and let
n ě 4c{ log 2. We discuss the following two cases depending on whether pi P PC or pi P PC̄ .
Case I: If pi P PC . Let k be the number of papers in PC ranked strictly higher (better)
than ` according to π ˚ . Since the permutation π ˚ is uniformly random, conditioned on
this value of `, the other papers’ positions in the true ranking are uniformly at random in
positions rnszt`u. Now for any paper pj , j ‰ i, let Xj be an indicator random
variable set
ř
as 1 if position of pj is higher than ` in π ˚ , and 0 otherwise. So k “ pj PPC ztpi u Xj , and
`´1
PrpXj “ 1q “ n´1
when j ‰ i. Then using Hoeffding’s inequality without replacement, we
have
ˇ
ˆˇ
˙
ˇ k
` ´ 1 ˇˇ
ˇ
Pr ˇ
´
ě ε ď 2 expp´2pn1 ´ 1qε2 q ď 2 expp´n1 ε2 q
n1 ´ 1 n ´ 1 ˇ

for any ε ą 0. The last inequality
is due to n1 ě 2, which holds because n{n1 ď c with a
b
constant c. Now setting ε “

logp2{δq
we have the bound
n1

¸
˜ˇ
ˇ d
ˇ
ˇ k
`
´
1
logp2{δq
ˇď
Pr ˇˇ
´
ě 1 ´ δ.
n1 ´ 1 n ´ 1 ˇ
n1
Y
]
p is π
pi “ pk ` 1q ¨ nn1 .
Now note that by Algorithm 2, the position of paper pi in the ranking π
Use this relationship to substitute k in the above inequality, and notice that by assumption
maxtn{n1 , n{n2 u ď c, we have
pk ` 1q ¨

ˆ
ˆ
˙
˙
`´1
n
pn1 ´ 1q ε `
`1
n´1
n1
n1 ´ 1
n
n1 ´ 1
n
“
¨
p` ´ 1q `
¨ nε `
n1
n´1
n1
n1
n
ď
` nε ` ` ´ 1.
n1

n
ď
n1

20

On Strategyproof Conference Peer Review

On the other hand,
pk ` 1q ¨

ˆ
ˆ
˙
˙
`´1
n
pn1 ´ 1q
´ε `1
n´1
n1
n1 ´ 1
n
n
n1 ´ 1
“
¨
¨ nε `
p` ´ 1q ´
n1
n´1
n1
n1
n
n1 ´ 1
n
ě
´ nε `
¨
p` ´ 1q.
n1
n1
n´1

n
ě
n1

So
pk ` 1q ¨

n
n1 ´ 1
n
n
´`ě
´ nε `
¨
p` ´ 1q ´ `
n1
n1
n1
n´1
n
n1 ´ 1
n
pn ´ 1q ´ n
ě
´ nε `
¨
n1
n1
n´1
ě ´nε.

(1)
(2)

n
¨ n´1
ă 1, and thus RHS of (1) is minimized (as a function of `)
Here (2) is because n1n´1
1
when ` “ n. Combining the two inequalities above we have
d
a
n
logp2{δq
n
|p
πi ´ `| ď nε `
“n
`
ď 2 nc ¨ logp2{δq,
n1
n1
n1

where
the last inequality is by the assumption that n is large enough so that 2c ď
a
nc ¨ logp2{δq.
Case II: If pi P PC̄ . Again, let k be the number of papers in PC̄ ranked strictly
˚
higher
As the analysis in Case I, similarly, we have
ř (better) than ` according to π . `´1
k “ pj PPC̄ ztpi u Xj , and PrpXj “ 1q “ n´1 . With the same analysis using Hoeffding’s
inequality without replacement, with probability at least 1 ´ δ we have
ˇ d
ˇ
ˇ
ˇ k
`
´
1
logp2{δq
ˇ
ˇ
.
ˇ n2 ´ 1 ´ n ´ 1 ˇ ď
n2
´
¯
n
p in this case is π
pi “ g pk ` 1q ¨ n2 .
Now using Lemma 7.2, the position of paper pi in π
Using exactly the same analysis as Case I we have
´nε ď pk ` 1q ¨
and thus

n
n
´`ď
` nε ´ 1,
n2
n2

d
n
πi ´ `| ď nε `
|p
“n
n2

a
logp2{δq
n
`
ď 2 nc ¨ logp2{δq.
n2
n2

Combine both Case I and Case II, and notice that πi˚ is uniformly distributed in rns.
Using a union bound over i “ 1, 2, ..., n, with probability 1 ´ δ we have:
a
max |p
πi ´ πi˚ | ď 2 nc ¨ logp2n{δq.
1ďiďn

21

Xu, Zhao, Shi, Zhang & Shah

7.3.1 Proof of Lemma 7.2
]
Y
We show that for every slot q that there is no p such that p ¨ nn1 “ q, there exists one slot
´
¯
p1 for PCs such that g p1 ¨ nn2 “ q, i.e., all slots that are left empty by PC are taken by slots
of PCs . Since that the two kinds of slots have a total number of n, we show that there are no
overlap between the two kinds of slots, thus provingY the lemma.
]
Let t “ n{n1 . Suppose if there is no p such that p ¨ nn1 “ q, then there must exist some
p̂ such that
q ` 1 ď p̂t ă q ` t.

(3)

This is because there must be a multiple of t in the range rq, q ` tq, but our assumption
makes that there is no such multiply in rq, q ` 1q.
Now let u “ n{n2 . By n1 ` n2 “ n we have 1{u ` 1{t “ 1; substituting t “ u{pu ´ 1q in
(3) we have
q ă pq ´ p̂ ` 1qu ď q ` 1.
Thus there exists p1 “ gppq ´ p̂ ` 1quq P PCs . Thus we prove the lemma.
7.4 Proof of Theorem 6.2
The proof of Theorem 6.2 is a direct formulation of our intuition in Section 6.1. Without loss
of generality let pp1 , . . . , pl q be the cycle not reviewed by a single reviewer, for l ě 3. Hence
there exists a partial profile π such that for all the reviewers who have reviewed both pj
and pj`1 , pj ą pj`1 , @j P rls (define pl`1 “ p1 ). On the other hand, since for each reviewer,
at least one pair ppj , pj`1 q is not reviewed by her, the constructed partial profile is valid.
Now assume f is PU, then we must have p1 ą ¨ ¨ ¨ ą pl and pl ą p1 , which contradicts the
transitivity of the ranking.
7.5 Proof of Corollary 6.3
We prove each of the conditions in order.
Proof of part (i): If there is a cycle of size µ ` 1, then no reviewer can review all the papers
in it since it exceeds the size of review sets. So there is no such cycle.
Proof of part (ii): The statement trivially holds for µ “ 2. For µ ě 3, Suppose there are
two reviewers ri1 and ri2 such that 2 ď |Pi1 X Pi2 | ď µ ´ 1. Since Pi1 ‰ Pi2 , there exist
papers pj1 and pj2 such that pj1 P Pi1 zPi2 and pj2 P Pi2 zPi1 . Also |Pi1 X Pi2 | ě 2, and let
pj3 , pj4 P Pi1 X Pi2 . By definition it is easy to verify that ppj1 , pj3 , pj2 , pj4 q forms a cycle that
satisfies the condition in Theorem 6.2, and hence pG, f q is not pairwise unanimous.
Proof of part (iii): Define a “paper-relation graph” Gp as follows: Given a paper-review
assignment tPi um
i“1 , the paper-relation graph Gp is an undirected graph, whose nodes are the
distinct sets in tPi um
i“1 ; we connect two review sets iff they have one paper in common. Note
that by (ii), each pair of distinct sets has at most one paper in common.
22

On Strategyproof Conference Peer Review

We first show that pG, f q is pairwise unanimous, then Gp must necessarily be a forest. If
there is a cycle in Gp , then there is a corresponding cycle in the review relation graph H. To
see this, not losing generality suppose the shortest cycle in Gp is pP1 , ..., Pl q. Also, suppose
P1 X P2 “ tp1 u, P2 X P3 “ tp2 u, ..., Pl X P1 “ tpl u not losing generality. Then pp1 , ..., pl q
forms a cycle in Gp by its definition. Since each reviewer reviews exactly one set in Gp , there
is no reviewer reviewing all papers in this cycle of papers in Gp . Thus the condition in
Theorem 6.2 is satisfied, and pG, f q is not pairwise unanimous.
We now use this result to complete our proof. Consider the union of all sets of papers
that form vertices of Gp . We know that this union contains exactly n papers since each paper
is reviewed at least once. Now let kp denote the number of distinct review sets (that is,
number of vertices of Gp ), and let Pii , ..., Pikp denote the vertices of Gp . The union of three
k

p
or more sets in tPik uk“1
is empty, since otherwise there will be a cycle in Gp . Using this fact,
we apply the inclusion-exclusion principle to obtain

n“

kp
ÿ

ÿ
|Pik | ´

k“1

|Pik1 X Pik2 | “ kp µ ´ |EGp |.

1ďk1 ăk2 ďkp

Now use the inequality |EGp | ď kp ´ 1 which arises since Gp is a forest, to obtain the claimed
bound kp ď n´1
µ´1 .
7.6 Proof of Proposition 6.4
Fix some ranking of papers within each individual set P1 , P2 , P3 and P4 (e.g., according to
the natural order of their indices). In the remainder of the proof, any ranking of all papers
always considers these fixed rankings within these individual sets. With this in place, in
what follows, we refer to any ranking in terms of the rankings of the four sets of papers.
Suppose there is one such f that satisfies group unanimity and weak strategyproofness
for G, and consider the following 4 profiles:
(1) r1 : P1 ą P2 , r2 : P2 ą P3 , r3 : P3 ą P4

(2) r1 : P2 ą P1 , r2 : P3 ą P2 , r3 : P4 ą P3

(3) r1 : P2 ą P1 , r2 : P2 ą P3 , r3 : P3 ą P4

(4) r1 : P2 ą P1 , r2 : P3 ą P2 , r3 : P3 ą P4

By the property of GU, profile (1) leads to output P1 ą P2 ą P3 ą P4 , whereas (2) leads to
output P4 ą P3 ą P2 ą P1 . Now compare (1) and (3): The output of (3) must have P2 at
the top and satisfy P3 ą P4 , by the property of GU. So the output of profile (3) must be one
of i) P2 ą P1 ą P3 ą P4 , ii) P2 ą P3 ą P1 ą P4 , or iii) P2 ą P3 ą P4 ą P1 . Now note that
only reviewer r1 changes ranking across profiles (1) and (3), and hence by WSP the position
of at least one paper in the output of profile (3) must be the same as in that of profile (1).
This makes iii) infeasible, so the output of (3) must be either i) or ii). Similarly, the output
of (4) is either P3 ą P4 ą P2 ą P1 or P3 ą P2 ą P4 ą P1 . Now comparing (3) and (4): only
r2 changes ranking, but none of the four papers can be at the same position no matter how
we choose the outputs of (3) and (4). This yields a contradiction.
7.7 Proof of Theorem 6.5
We begin with a definition of an “influence graph” Gf induced by any given aggregation rule
f.
23

Xu, Zhao, Shi, Zhang & Shah

Definition 7.3 (Influence graph). For any review aggregation rule f , the influence graph
Gf induced by f is a bipartite graph with two groups of vertices R and P, and edges
as follows. A vertex ri is connected to vertex pj iff there exists a certain profile π
such that ri is able to change the output ranking of pj by changing her own preference. Formally, there exists an edge between any pair pri , pj q P EGf iff there exist profiles
π “ tπ p1q , . . . , π pi´1q , π piq , π pi`1q , . . . , π pmq u and π 1 “ tπ p1q , . . . , π pi´1q , π̃ piq , π pi`1q , . . . , π pmq u
and j P rns such that f pπqpjq ‰ f pπ 1 qpjq.
From this definition, it is thus not hard to see that f is WSP if and only if the degree of
every reviewer node in Gf is strictly smaller than n.
We prove the claim via a contradiction argument. Assume that f is both PU and
WSP. Let Gf be the corresponding influence graph. Firstly we show that degppq ą 0 for
every paper p, where the degree is for the influence graph Gf . Suppose otherwise that
degppj q “ 0 for some paper pj . This means no reviewer can affect the ranking of pj ; in
other words, the position of paper pj is fixed regardless of the profile. This contradicts
with our assumption of pairwise unanimity; to see this, pick another paper pj 1 where j 1 ‰ j
(this is possible since n ě 2). Not losing generality suppose j ă j 1 . Consider a profile π
where every reviewer ranks pj ą pj 1 ą P´pj,j 1 q , and another profile π 1 where everyone ranks
pj‘ ą pj ą P´pj,j 1 q ; here P´pj,j 1 q means the ordinal ranking of papers other than pj , pj 1 , i.e.,
p1 ą ¨ ¨ ¨ ą pj´1 ą pj`1 ą ¨ ¨ ¨ ą pj 1 ´1 ą pj 1 `1 ą ¨ ¨ ¨ ą pn . By the property of PU, when
everyone ranks the same the final result must be the same as everyone; however this means
the position of pj is different in the two profiles, and thus the position of pj is not fixed. This
makes contradiction and we prove that degppq ą 0 for every paper p.
Now for any reviewer ri P R, let epri q P P be the paper with the lowest index in P such
that pri , epri qq R EGf . Since f is WSP, epri q must exist for all i P rms. Define the set of such
papers as P 1 :“ te1 , . . . , em1 u “ tepri q : ri P Ru. Note that we must have m1 ď m and in fact
m1 can be strictly smaller than m because of the possible overlap between epri q, @i P rms.
From the definition of m1 and property of WSP, it is clear that m1 ě 1. If m1 “ 1, we have
pri , e1 q R EGf for every reviewer ri ; this contradicts with the fact that degppq ě 0 for every
paper p. So m1 ą 1.
In this proof, we slightly overload the notation of fek pπq to mean the position of paper
ek in f pπq. Based on the inverse mapping from P 1 to R, we partition all the reviewers R
into m1 groups tR11 , . . . , R1m1 u such that all reviewers in any set R1k contributed paper ek
when defining set P 1 . In particular, we have that no reviewer in R1j is connected to paper ej
in the influence graph Gf .
In the description that follows, we restrict attention to the papers in P 1 , and assume that
in any ranking all remaining pn ´ n1 q papers are positioned at the end of the preference list
of any reviewer. Now consider the following two preferences over P 1 :
π “ e1 ą e2 ą ¨ ¨ ¨ ą em1 ,

π 1 “ e2 ą e3 ą ¨ ¨ ¨ ą em1 ą e1 .

Using π and π 1 , define the following m1 ` 1 different profiles:
1
π 0 “ pπ, . . . , πq, π k “ pπ
, . . . , π 1 , looomooon
π, . . . , π q @k P rm1 s.
loooomoooon
k

24

m1 ´k

On Strategyproof Conference Peer Review

where in π k the first k preferences are π 1 and the last (m1 ´ k) preferences are π. In what
follows, we will use a diagonalization argument to generate a contradiction using the condition
that f is WSP. We first present a lemma, which we prove in Section 7.7.1.
Lemma 7.4. If f is pairwise unanimous and weakly strategyproof, fek pπ k q “ k for every
k P rm1 s, that is, under profile π k , the k-th position in the output ranking must be taken by
paper ek .
Applying Lemma 7.4 with k “ m1 , we obtain that fem1 pπ m1 q “ m1 . However, on the
other side, since π m1 “ pπ 1 , . . . , π 1 q and π 1 “ e2 ą e3 ą ¨ ¨ ¨ ą em1 ą e1 , again by the PU
property we have fem1 pπ m1 q “ 1. This leads to a contradiction, hence f cannot be both WSP
and PU.
7.7.1 Proof of Lemma 7.4
We prove by induction on k.
Base case. Since f is PU, the output ranking f pπ 0 q must be:
f pπ 0 q “ e1 ą e2 ą ¨ ¨ ¨ ą em1
Consider k “ 1. Note that π and π 1 differ only at the position of e1 , and in π 1 , only P11
changes their preference and all the other preferences are kept fixed. Then by the WSP of
f , the output ranking of e1 will not be changed because P11 are not connected to e1 in the
influence graph, so we must have:
f pπ 1 q “ e1 ą e2 ą ¨ ¨ ¨ ą em1 ,
Induction step. Suppose the claim of this lemma holds for t1, . . . , ku. Consider the case of
k ` 1.
Observe that f pπ k qpek q “ k, and in both π and π 1 we have:
ek ą ek`1 ą ¨ ¨ ¨ ą em1 .
Then since f is PU, we know that the last m1 ´ k ` 1 positions in the output ranking of
f pπ k q must be given by ek ą ek`1 ą ¨ ¨ ¨ ą em1 , i.e., fek`1 pπ k q “ k ` 1. The profiles π k and
1
1
π k`1 differ only in the preference given by Pk`1
, and no reviewer in set Pk`1
can influence
the position of paper ek`1 . It follows that fek`1 pπ k`1 q “ k ` 1, which completes our proof.

8. Discussion
In this paper we address the problem of designing strategyproof and efficient peer-review
mechanism. The setting of peer review is challenging due to the various idiosyncrasies of
the peer-review process: reviewers review only a subset of papers, each paper has multiple
authors who may be reviewers, and each reviewer may author multiple submissions. We
provide a framework and associated algorithms to impart strategyproofness to conference
peer review. Our framework, besides guaranteeing strategyproofness, is importantly very
flexible in allowing the program chairs to use the decision-making criteria of their choice.
We complement these positive results with negative results showing that it is impossible for
25

Xu, Zhao, Shi, Zhang & Shah

any algorithm to remain strategyproof and satisfy the stronger notion of pairwise unanimity.
Future work includes considering efficiency from a statistical perspective and characterizing
the precise set of conflict-of-interest graphs that permit (or not) strategyproofness.
The framework established here leads to a number of useful open problems:
• Can recruitment of a small number of reviewers with no conflicts (e.g., in case of
authorship conflicts, reviewers who have not submitted any papers) lead to significant
improvements in efficiency? Can better ways to eliminate some authors from the
reviewer pool increase applicability of partition-based algorithms?
• The results in this paper considered the social choice property of unanimity as a measure
of efficiency. While this can be regarded as a first-order notion of efficiency, it is of
interest to consider complex notions of efficiency. One useful notion of efficiency is the
statistical utility of estimation (Stelmakh et al., 2019b) of the (partial or full) ranking of
papers under a statistical model for reviewer reports. An alternative notion of efficiency
combines an assignment quality based on the similarities of assigned reviewers and
papers with group unanimity (e.g., maximizing the similarity scores in the assignment
while preserving group unanimity).

Acknowledgments
This work was supported in parts by NSF grants CRII: CIF: 1755656 and CIF: 1763734.

References
Alon, N., Fischer, F., Procaccia, A., and Tennenholtz, M. (2011). Sum of us: Strategyproof
selection from the selectors. In Proceedings of the 13th Conference on Theoretical Aspects
of Rationality and Knowledge, pages 101–110. ACM.
Anderson, M. S., Ronning, E. A., De Vries, R., and Martinson, B. C. (2007). The perverse
effects of competition on scientists’ work and relationships. Science and engineering ethics,
13(4):437–461.
Arrow, K. J. (1950). A difficulty in the concept of social welfare. Journal of political economy,
58(4):328–346.
Aziz, H., Lev, O., Mattei, N., Rosenschein, J. S., and Walsh, T. (2016). Strategyproof peer
selection: Mechanisms, analyses, and experiments. In AAAI, pages 397–403.
Aziz, H., Lev, O., Mattei, N., Rosenschein, J. S., and Walsh, T. (2019). Strategyproof peer
selection using randomization, partitioning, and apportionment. Artificial Intelligence.
Balietti, S., Goldstone, R. L., and Helbing, D. (2016). Peer review and competition in the
art exhibition game. Proceedings of the National Academy of Sciences, 113(30):8414–8419.
Barnett, W. (2003). The modern theory of consumer behavior: Ordinal or cardinal? The
Quarterly Journal of Austrian Economics, 6(1):41–65.
26

On Strategyproof Conference Peer Review

Bird, S., Loper, E., and Klein, E. (2009). Natural language processing with python o’reilly
media inc.
Bousquet, N., Norin, S., and Vetta, A. (2014). A near-optimal mechanism for impartial
selection. In International Conference on Web and Internet Economics, pages 133–146.
Springer.
Brandt, F., Conitzer, V., Endriss, U., Procaccia, A. D., and Lang, J. (2016). Handbook of
computational social choice. Cambridge University Press.
Caragiannis, I., Chatzigeorgiou, X., Krimpas, G. A., and Voudouris, A. A. (2017). Optimizing
positional scoring rules for rank aggregation. In AAAI, pages 430–436.
Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated
paper-reviewer assignment system.
Connolly, R., Miller, J., and Friedman, R. (2014). A longitudinal examination of sigite
conference submission data, 2007-2012. In Proceedings of the 15th Annual Conference on
Information technology education, pages 167–172. ACM.
De Clippel, G., Moulin, H., and Tideman, N. (2008). Impartial division of a dollar. Journal
of Economic Theory, 139(1):176–191.
Díez Peláez, J., Luaces Rodríguez, Ó., Alonso Betanzos, A., Troncoso, A., and Bahamonde Rionda, A. (2013). Peer assessment in moocs using preference learning via matrix
factorization. In NIPS Workshop on Data Driven Education.
Dörfler, F., Xiao, Y., and van der Schaar, M. (2017). Incentive design in peer review:
Rating and repeated endogenous matching. IEEE Transactions on Network Science and
Engineering.
Douceur, J. R. (2009). Paper rating vs. paper ranking. ACM SIGOPS Operating Systems
Review, 43(2):117–121.
Emerson, P. (2013). The original Borda count and partial voting. Social Choice and Welfare,
pages 1–6.
Fiez, T., Shah, N., and Ratliff, L. (2019). A SUPER* algorithm to optimize paper bidding in
peer review. In ICML workshop on Real-world Sequential Decision Making: Reinforcement
Learning And Beyond.
Fischer, F. and Klimm, M. (2015). Optimal impartial selection. SIAM Journal on Computing,
44(5):1263–1285.
Fishburn, P. C. (2015). The theory of social choice. Princeton University Press.
Fulkerson, D. and Gross, O. (1965). Incidence matrices and interval graphs. Pacific journal
of mathematics, 15(3):835–855.
Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal
matter? insights from a major nlp conference. arXiv preprint arXiv:1903.11367.
27

Xu, Zhao, Shi, Zhang & Shah

Garg, N., Kavitha, T., Kumar, A., Mehlhorn, K., and Mestre, J. (2010). Assigning papers to
referees. Algorithmica, 58(1):119–136.
Ge, H., Welling, M., and Ghahramani, Z. (2013). A Bayesian model for calibrating conference
review scores.
Gibbard, A. (1973). Manipulation of voting schemes: a general result. Econometrica: journal
of the Econometric Society, pages 587–601.
Hajek, B., Oh, S., and Xu, J. (2014). Minimax-optimal inference from partial rankings. In
Advances in Neural Information Processing Systems, pages 1475–1483.
Hartvigsen, D., Wei, J. C., and Czuchlewski, R. (1999). The conference paper-reviewer
assignment problem. Decision Sciences, 30(3):865–876.
Hazelrigg, G. (2013). Dear colleague letter: Information to principal investigators (PIs) planning to submit proposals to the Sensors and Sensing Systems (SSS) program October 1, 2013,
deadline. Deadline (NSF Website, http://www.nsf.gov/pubs/2013/nsf13096/nsf13096.jsp).
Hojat, M., Gonnella, J. S., and Caelleigh, A. S. (2003). Impartial judgment by the “gatekeepers” of science: fallibility and accountability in the peer review process. Advances in
Health Sciences Education, 8(1):75–96.
Holzman, R. and Moulin, H. (2013). Impartial nominations for a prize. Econometrica,
81(1):173–196.
Kahng, A. B., Kotturi, Y., Kulkarni, C., Kurokawa, D., and Procaccia, A. D. (2017). Ranking
wily people who rank each other. Technical Report.
Kurokawa, D., Lev, O., Morgenstern, J., and Procaccia, A. D. (2015). Impartial peer review.
In IJCAI, pages 582–588.
Langford, J. (2008). Adversarial academia. http://hunch.net/?p=499.
Lawrence, N. and Cortes, C. (2014). The NIPS Experiment. http://inverseprobability.
com/2014/12/16/the-nips-experiment. [Online; accessed 3-June-2017].
Makhorin, A. (2001). Gnu linear programming kit. Moscow Aviation Institute, Moscow,
Russia, 38.
Mathieus, C. (2008). SODA PC meetings. http://cs.brown.edu/~claire/SODAnotes.pdf
(last retrieved May 21, 2018).
Merrifield, M. R. and Saari, D. G. (2009). Telescope time without tears: a distributed
approach to peer review. Astronomy & Geophysics, 50(4):4–16.
Noothigattu, R., Shah, N., and Procaccia, A. (2018). Choosing how to choose papers. arXiv
preprint arxiv:1808.09057.
Piech, C., Huang, J., Chen, Z., Do, C., Ng, A., and Koller, D. (2013). Tuned models of peer
assessment in moocs. arXiv preprint arXiv:1307.2579.
28

On Strategyproof Conference Peer Review

Roos, M., Rothe, J., and Scheuermann, B. (2011). How to calibrate the scores of biased
reviewers by quadratic programming. In AAAI.
Salton, G., Wong, A., and Yang, C.-S. (1975). A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620.
Satterthwaite, M. A. (1975). Strategy-proofness and arrow’s conditions: Existence and
correspondence theorems for voting procedures and social welfare functions. Journal of
economic theory, 10(2):187–217.
Schütze, H., Manning, C. D., and Raghavan, P. (2008). Introduction to information retrieval.
In Proceedings of the international communication of association for computing machinery
conference, volume 4.
Shah, N. B., Balakrishnan, S., Bradley, J., Parekh, A., Ramchandran, K., and Wainwright,
M. J. (2016). Estimation from pairwise comparisons: Sharp minimax bounds with topology
dependence. The Journal of Machine Learning Research, 17(1):2049–2095.
Shah, N. B., Bradley, J. K., Parekh, A., Wainwright, M., and Ramchandran, K. (2013). A
case for ordinal peer-evaluation in moocs. In NIPS Workshop on Data Driven Education.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and von Luxburg, U. (2017). Design and
Analysis of the NIPS 2016 Review Process. arXiv preprint arXiv:1708.09794.
Stelmakh, I., Shah, N., and Singh, A. (2019a). On testing for biases in peer review. In
NeurIPS.
Stelmakh, I., Shah, N. B., and Singh, A. (2019b). PeerReview4All: Fair and accurate reviewer
assignment in peer review. In Algorithmic Learning Theory.
Stewart, N., Brown, G. D., and Chater, N. (2005). Absolute identification by relative
judgment. Psychological review, 112(4):881.
Thurner, S. and Hanel, R. (2011). Peer-review in a world with rational scientists: Toward
selection of the average. The European Physical Journal B, 84(4):707–711.
Tomkins, A., Zhang, M., and Heavlin, W. D. (2017). Reviewer bias in single-versus doubleblind peer review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.
Tsukida, K. and Gupta, M. R. (2011). How to analyze paired comparison data. Technical
report, DTIC Document.
Wang, J. and Shah, N. B. (2019). Your 2 is my 1, your 3 is my 9: Handling arbitrary
miscalibrations in ratings. In AAMAS.

Appendix A. More Details on the ICLR 2018 Experiment
In this section we describe in more detail on the similar scoring model and the optimization
formulation used in our ICLR 2018 experiment in Section 5.2.
29

Xu, Zhao, Shi, Zhang & Shah

A.1 Similarity Scoring Model
We first use text representations to model the reviewers. In particular, for each reviewer in
the pool, we scrape their (at most 10) most recent papers from arXiv8 as the corresponding
text representation. To preprocess the text of papers and reviewers, we remove the stop
words, tokenize the text, and then use the PorterStemmer (Bird et al., 2009) to obtain the
word stem of each word. After the preprocessing step, the dictionary contains d “ 290158
unique words. Based on this dictionary, we use the vector space model (Salton et al., 1975) to
represent each reviewer/paper as a vector in Rd . The ICLR 2018 data contains 911 submitted
papers and 2435 reviewers, hence there are N “ 3346 documents in total.
To compute the similarity scores between reviewers and papers, for each document
D (reviewer or paper), we compute the corresponding term frequency-inverse document
frequency (tf-idf) (Schütze et al., 2008) score as the vector representation. Specifically, for a
term w in the document, we use Nw to denote the number of times w appears in the corpus
that contains N documents. Then the inverse document frequency of the term w is given by:
idfpwq :“ log

N
.
Nw

To prevent a bias towards longer documents, e.g., raw frequency of w divided by the raw
frequency of the most occurring term in the document D, we use the following augmented
frequency as the term frequency of w in document D:
tfpw, Dq :“

fw,D
1 1
`
,
2 2 maxtfw1 ,D : w1 P Du

where we use fw,D to denote the number of times term w appearing in D. Let vD P Rd be
the vector representation of document D. Then the value of the coordinate corresponding to
term w is given by:
ˆ
˙
fw,D
N
1 1
ˆ log
vD pwq “ tfpw, Dq ˆ idfpwq “
`
.
2 2 maxtfw1 ,D : w1 P Du
Nw
We then construct the similarity matrix S P Rmˆn between reviewers and papers whose each
entry sij P r0, 1s corresponds to the similarity score between reviewer ri and paper pj . sij is
given by the cosine similarity of the corresponding tf-idf vectors:
sij “

vrTi vpj
P r0, 1s.
}vri }2 ¨ }vpj }2

A.2 The Reviewer-Paper Assignment Algorithm
Matching is the process of assigning papers to reviewers. Given the similarity score matrix
S P Rmˆn , we solve the following optimization problem, as used in the current TPMS
system, to compute the assignment. The optimization problem formulated in (4) is an integer
8. To ensure that the downloaded papers belong to the corresponding author in the general area of artificial
intelligence, we only scrape papers under the following categories: cs.LG, cs.AI, stat.ML, cs.CV, cs.NE,
cs.CL, cs.GT and cs.RO.

30

On Strategyproof Conference Peer Review

program, where the objective function corresponds to maximizing the sum of similarity scores
in the matching. Here for any reviewer-paper pair pi, jq, we have aij “ 1 iff paper pj is
assigned to reviewer ri in the matching:
ÿ ÿ
maximize
sij aij
aij

iPrms jPrns

subject to aij P t0, 1u,
ÿ
aij ď µ,

@i P rms, j P rns
@i P rms

(4)

j

ÿ

aij ě λ,

@j P rns

i

ř
The constraint j aij ď µ means that we restrict the maximum number
of papers assigned
ř
to a reviewer to be µ. Furthermore, we also use the constraint i aij ě λ to enforce that
each paper should be reviewed by at least λ reviewers. In the ICLR 2018 data the number of
optimization variables aij is more than 2 million, which is intractable to solve using existing
integer program solvers. So instead, we can relax the above integer program to the following
linear program (LP):
ÿ ÿ
maximize
sij aij
aij

iPrms jPrns

subject to 0 ď aij ď 1, @i P rms, j P rns
ÿ
aij ď µ, @i P rms

(5)

j

ÿ

aij ě λ,

@j P rns

i

In the above LP we relax the integral constraint over aij in (4) to 0 ď aij ď 1, @i P rms, j P rns.
Due to the relaxation, it is clear that the optimal value of (5)řis at least thatřof (4). On the
other hand, observe that if we reformulate the constraints j aij ď µ and i aij ě λ into
the matrix form, then the corresponding constraint matrix will be the node-edge incidence
matrix of a complete bipartite graph consisting of a set of reviewers and a set of papers. It
follows from a known sufficient condition (Fulkerson and Gross, 1965) that the incidence
matrix of a bipartite graph is totally unimodular, which implies that the solution of the LP
in (5) is guaranteed to be integral. Hence in order to obtain the optimal solution of (4), we
can use existing polynomial time solvers to solve the relaxed LP, and since the constraint
matrix in (4) is totally unimodular, this gives us a polynomial time algorithm to compute
the optimal solution of (4). In our implementation we use the GNU Linear Programming
Kit (GLPK) (Makhorin, 2001) that implements the simplex algorithm to solve the LP in (5).

31

