Under review as submission to TMLR
Settling the Communication Complexity for Distributed
Offline Reinforcement Learning
Anonymous authors
Paper under double-blind review
Abstract
Westudyanovelsettinginofflinereinforcementlearning(RL)whereanumberofdistributed
machines jointly cooperate to solve the problem but only one single round of communication
is allowed and there is a budget constraint on the total number of information (in terms of
bits) that each machine can send out. For value function prediction in contextual bandits,
and both episodic and non-episodic MDPs, we establish information-theoretic lower bounds
on the minimax risk for distributed statistical estimators; this reveals the minimum amount
of communication required by any offline RL algorithms. Specifically, for contextual ban-
dits, we show that the number of bits must scale at least as Î©(ğ´ğ¶)to match the centralised
minimax optimal rate, where ğ´is the number of actions and ğ¶is the context dimension;
meanwhile, we reach similar results in the MDP settings. Furthermore, we develop learning
algorithms based on least-squares estimates and Monte-Carlo return estimates and provide
a sharp analysis showing that they can achieve optimal risk up to logarithmic factors. Ad-
ditionally, we also show that temporal difference is unable to efficiently utilise information
from all available devices under the single-round communication setting due to the initial
bias of this method. To our best knowledge, this paper presents the first minimax lower
bounds for distributed offline RL problems.
1 Introduction
In this paper, we study a problem setting where each device can only send one message to the central server
and the number of information in terms of bits is constrained by a communication budget . After that one
transmission, no further communication is allowed; this setting models the situation where each device has
a limited connection to the central server. Examples of this problem are ubiqutous in the field of electronic
wearables and IoT devices, which are often low-powered and have limited access to internet. In such cases,
connection with global server is rare and can be effectively modelled as a one-shot. If one wants to use
Reinforcement learning approaches on such devices, each device would require access to large amounts of
past experiences to efficiently solve the task. Fortunately, if one combines the data gathered on all devices
in some region, within this collective experience we are likely to have explored enough of the environment
to achieve acceptable performance. However, to utilise the data from other devices the RL algorithm needs
to be able to communicate efficiently with other devices and learn from this collective experience in an
offline manner. It is expected that enforcing restrictions on the available communication will decrease the
overall performance for a given task. Yet, the research questions of how does the best possible performance
depend on the communication budget, and how does existing offline RL algorithms perform have never been
answered.
In this paper, we attempt to answer the above questions by studying distributed offline RL problems. Our
proof methods are based on reducing RL problems to statistical inference problems. After such reduction is
done we utilise framework developed by Zhang et al. (2013), which ties minimax risk to mutual information
of parameter and message sent. Intuitively, if a message carries little information about a certain parameter,
it will be difficult for the central server to perform accurate inference based on that message only. We thus
bound the mutual information to derive a lower bound on the risk any one-shot communication algorithm
must suffer. Under our setting, we start with the classical multi-armed bandits problem in the contextual
1Under review as submission to TMLR
Problems Algorithms Optimal Risk Bits Communicated
Parameter learning in LSE (Algorithm 1) O(ğ´ğ¶ğ‘…2
ğ‘šğ‘›ğœ†) Î˜(ğ´ğ¶logğ‘šğ‘›ğœ†
ğ‘…)
linear contextual bandits Lower Bound Î©(ğ´ğ¶ğ‘…2
ğ‘šğ‘›ğœ†) Î©(ğ´ğ¶)
Value function prediction MC LSE (Algorithm 2) O(ğ¶ğ»2ğ‘…2
ğ‘šğ‘†ğ¸ğœ†) Î˜(ğ¶logğ‘šğ‘†ğ¸ğœ†
ğ»ğ‘…)
(Episodic MDP) Lower Bound Î©(ğ¶ğ»2ğ‘…2
ğ‘†ğ¸ğ‘šğœ†) Î©(ğ¶
logğ‘š)
Value function prediction TD (Algorithm 3) O(ğœˆ
1+(1âˆ’ğ›¾)2ğ‘›) Î˜(ğ¶logğ¶
ğœˆ)
(Non-Episodic MDP) Lower Bound Î©(ğ¶ğ‘…2
(1âˆ’ğ›¾)2ğ‘šğ‘†ğ‘›ğœ‹ maxğœ†)Î©(ğ¶
logğ‘š)
Table 1: Summary of main results. ğ´denotes the cardinality action space, ğ‘†card. of state space, ğ¶is
the dimensionality of context/feature vectors, ğ‘šis the number of cooperating machines, ğ‘›is the number of
samples,ğ¸number of episodes, ğœ†is the smallest rescaled eigenvalue of the feature/context covariance matrix,
ğ»is the horizon length, ğ›¾denotes the discount factor. For the stationary distribution of states visited in
non-episodic MDP ğœ‹, we assume that ğœ‹maxâ‰¥ğœ‹(ğ‘ )â‰¥ğœ‹min. We define ğœˆ=max{ğ‘…2
ğ‘†ğœ‹minğœ†ğ‘š,âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2},
where the second argument of max operator is the initial bias of TD method.
case, where we derive a lower bound on the risk ofğ´ğ¶ğ‘…2
ğ‘šğ‘›ğœ†when the communication budget scales at least as
ğ´ğ¶, whereğ´andğ¶are number of actions and dimensions of context respectively, ğ‘…is the maximum reward,
ğ‘šandğ‘›are numbers of machines and samples for each action and ğœ†is the rescaled smallest eigenvalue
of the covariance matrix of context/feature vectors. We also present Algorithm 1 achieving optimal risk
with communication budget being optimal up to logarithmic factors. Existing analyses of communication in
multi-armed bandits problems (Wang et al. (2019), Huang et al. (2021)) define the communication cost as
the number of values transmitted rather than the number of bits, which unfortunately ignores the fact that
real communication can only occur with finite precision.
Apart from the bandits problem, we also analyse problem settings for Markov Decision Processes (MDPs)
and develop lower bounds for both episodic and non-episodic settings. To our best knowledge, we present
the first minimax lower bounds for distributed solving of MDPs. In the episodic setting, we develop a lower
bound ofğ¶ğ»2ğ‘…2
ğ‘šğ‘†ğ¸ğœ†of risk withğ¶
logğ‘šcommunication budget, where ğ»is horizon length, ğ‘†is the cardinality
of state space and ğ¸is the number of episodes we have for each state. We also prove that performing
distributed Monte-Carlo return estimates achieve optimal risk with communication budget being optimal
up to logarithmic factors. Additionally, we extend our results to the non-episodic cases and develop a lower
bound ofğ¶ğ‘…2
(1âˆ’ğ›¾)2ğ‘šğ‘†ğ‘›ğœ‹ maxğœ†with communication budgetğ¶
logğ‘š. Meanwhile, we also study the worst-case risk
of distributed temporal difference in this setting and show that with only one communication round, the
algorithm cannot efficiently utilise data stored on all machines due to its initial bias, i.e., the difference
between the average initial estimate and the true parameter value âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Î¸ğ‘–
0âˆ¥2, which does not decrease
with the number of machines.
We summarise our theoretical results in Table 1 and our contributions are listed as follows:
â€¢We present information-theoretic lower bounds for distributed offline linear contextual bandits and
linear value function prediction in Markov Decision Processes. Our lower bounds scale with the
allowed communication budget ğµ. To the best of the authorsâ€™ knowledge, these are the first lower
bounds in distributed reinforcement learning other than for bandit problems.
â€¢We prove that a distributed version of the least-square estimate achieves optimal risk for the con-
textual bandit problem and a distributed version of Monte-Carlo return estimates achieves optimal
risk in episodic MDP. We show that both of these algorithms have communication budgets optimal
up to logarithmic factors.
â€¢Weshowthattheperformanceofdistributedtemporaldifferenceinonecommunicationroundsetting
does not improve when data from more machines is used if the initial bias is large
2Under review as submission to TMLR
2 Problem Formulation
We assume there are ğ‘šprocessing centres, and the ğ‘–th centre stores gameplay history â„ğ‘–of an agent inter-
acting with a particular environment within a framework of a Markov Decision Process. The distribution
of gameplay histories ğ‘ƒ(â„)belongs to some wider family of distributions P, which is taken to be a set of
all possible distributions that might have generated gameplay given that they satisfy assumptions of the
problem. We assume there is some parameter of interest Î¸:Pâ†’Î˜embedded within that problem, which
might be a property of solely the environment or a result of the agentâ€™s actions. All centres are supposed
to cooperate to jointly obtain an estimate Ë†Î¸, closest to the true value Î¸. Based on its gameplay history â„ğ‘–,
theğ‘–th centre can send a message ğ‘Œğ‘–to the main processing centre (arbitrary chosen), according to some
communication protocol Î . There is only one communication round allowed, and the main centre cannot
send any message back to individual centres. After receiving messages from all centres ğ‘Œ1,...,ğ‘Œğ‘šthe main
centre outputs an estimate Ë†Î¸(ğ‘Œ1,...,ğ‘Œğ‘š). We define the worst-case risk of that estimate in Definition 2.1.
Definition 2.1 For a problem of estimating a parameter Î¸:Pâ†’Î˜, we define the worst-case risk of an
estimator Ë†Î¸under communication protocol Î as
ğ‘Š(Î¸,P,Ë†Î¸,Î )=sup
ğ‘ƒâˆˆPğ”¼
||Ë†Î¸(ğ‘Œ1:ğ‘š)âˆ’Î¸(ğ‘ƒ)||2
2
where the expectation is taken over possible gameplay â„ğ‘–histories generated by ğ‘ƒand messages ğ‘Œ1:ğ‘šsent by
the protocol Î based on them.
We now consider the case when each message ğ‘Œğ‘–is constrained so that its length in bits ğ¿ğ‘–is smaller than
some communication budget ğµ. We define ğ´(ğµ)to be the family of communication protocols under which
âˆ€ğ‘–âˆˆ{1,...,ğ‘š}ğ¿ğ‘–â‰¤ğµ. We define the minimax risk as the best possible worst-case risk any algorithm can have
as stated by Definition 2.2.
Definition 2.2 For a class of problems where gameplay history is generated by a distribution ğ‘âˆˆPand we
wish to estimate a parameter Î¸:Pâ†’Î˜, under a communication budget of ğµwe define the minimax risk to
be:
ğ‘€(Î¸,P,ğµ)=inf
Î âˆˆğ´(ğµ)inf
Ë†ğœƒğ‘Š(Î¸,P,Ë†Î¸,Î )=inf
Î âˆˆğ´(ğµ)inf
Ë†ğœƒsup
ğ‘ƒâˆˆPğ”¼
||Ë†Î¸(ğ‘Œ1:ğ‘š)âˆ’Î¸(ğ‘ƒ)||2
2
where the second infimum is taken over all possible estimators.
Notably, a similar definition was adopted by Zhang et al. (2013) in the setting of distributed statistical
inference. The lower bounds we formulate are given in form of a hard instance of a problem. Formally,
we show that there exists an instance such that any algorithm has a minimax risk at least equal to the
bound we derive. We use a method described in Appendix A, which gives us a lower bound on minimax
risk, provided we can obtain an upper bound of mutual information the messages sent by machines carry
about the parameter ğ¼(ğ‘Œ,ğ‘‰). To this goal, we rely on inequality stated in Appendix A.3, which requires us
to construct a set such that if samples we receive fall into it, the likelihood ratio given different values of
the parameter is bounded. If we can additionally bound the probability that the samples do not fall into
this set, which in our proofs is done through the Hoeffding inequality, we obtain an upper bound on the
information. Intuitively, if the likelihood ratio is small, it becomes hard to identify a parameter and if this
also happens with large probability, then average information ğ¼(ğ‘Œ,ğ‘‰)messages carry about the parameter
must be small. We will now introduce the communication model assumed in this paper.
2.1 Information Transmitted and Quantisation Precision
In every algorithm we propose, we assume the following transmission model; we introduce quantisation
levels separated by ğ‘ƒ, which we call the precision of quantisation. We assume the transmitted values are
always within some pre-defined range (ğ‘‰min,ğ‘‰max), hence we divide it intoğ‘‰maxâˆ’ğ‘‰min
ğ‘ƒlevels. Each value gets
quantisedintothenearestlevelsothatthedifferencebetweentheoriginalvalueandthelevelintowhichitwas
quantised is smallest. Under such a scheme, the number of bits transmitted is, therefore ğµ=log
ğ‘‰maxâˆ’ğ‘‰min
ğ‘ƒ
.
We now proceed with the analysis of the first class of the distributed offline RL problems.
3Under review as submission to TMLR
3 Parameter estimation in contextual linear bandits
In a classic multi-armed bandit problem, the agent is faced with a number of slot machines and is allowed
to pull the arm of only one of them at a given timestep. We can identify choosing each arm with choosing
an actionğ‘âˆˆA, whereğ´=|A|is the number of all arms. The reward obtained after each pull is stochastic
and is sampled from a distribution that depends on the machine. We do not specify the distribution of the
reward, but put a standard constraint on the maximum absolute value of rewards as stated by Assumption
3.1, restricting its support to [âˆ’ğ‘…max,ğ‘…max].
Assumption 3.1 The maximum absolute value of the reward an agent can receive at each timestep ğ‘¡is
bounded by some constant ğ‘…max>0, i.e.|ğ‘Ÿğ‘¡|â‰¤ğ‘…max.
Hence, the goal in such a task is to identify the arm with the highest average reward. In the contextual
version of this problem, at each time step, the agent is also given a context vector cğ‘¡âˆˆâ„ğ¶, which influences
the distribution of the reward for each arm. Consequently, an arm that is optimal under one context need
not be optimal under a different context. We explicitly assume a linear structure between the context vector
and average reward for each arm, i.e. ğ”¼[ğ‘Ÿğ‘¡|ğ‘ğ‘¡]=cğ‘‡
ğ‘¡Î¸ğ‘ğ‘¡, whereÎ¸ğ‘âˆˆâ„ğ¶is the parameter vector for the
arm associated with action ğ‘. We analyse the offline version of this problem where we have access to the
gameplay history â„={cğ‘™,ğ‘ğ‘™,ğ‘Ÿğ‘™}ğ‘
ğ‘™=1, consisting of received context vectors, the arms agent have chosen in
response to them and rewards obtained after pulling the arm. We assume that within gameplay history the
agent has chosen each arm ğ‘›times so that ğ‘=ğ´ğ‘›. We make a further assumption regarding the context
vectors.
Assumption 3.2 Context vector for each sample is normalised, i.e. âˆ¥cğ‘–âˆ¥2
2â‰¤1. If we form a matrix
ğ‘‹=(ğ‘1,...,ğ‘ğ‘›)for any number of context vectors ğ‘›, then the smallest eigenvalue of the matrix ğ‘‹ğ‘‡ğ‘‹isğœ‚min
such that 0<ğœ‚minâ‰¤1andğœ‚min=ğ‘›ğœ†minfor some constant ğœ†min.
Restricting the smallest eigenvalue is necessary as otherwise, the parameter Î¸ğ‘becomes unidentifiable, as
large changes in it will produce small changes in the average reward. Also, in practical problems, eigenvalues
of the covariance matrix naturally grow with the number of samples and the condition that ğœ‚min=ğ‘›ğœ†min
has been adopted by similar analyses (Zhang et al. (2013)). We would like to obtain an estimator for
Î¸=(Î¸1,...,Î¸ğ´), which is a concatenated vector consisting of all parameter vectors for each ğ‘âˆˆA. We
now proceed with presenting the minimax lower bound for the estimation of Î¸. We assume the context
vectors machines receive are pre-specified and the only randomness is within the reward. We now present
our first lower bound on minimax risk in Theorem 3.3 and sketch its proof, describing a hard instance of the
contextual linear bandit problem.
Theorem 3.3 In a distributed offline linear contextual MAB problem with ğ´actions and context with di-
mensionality ğ¶such thatğ¶ğ´ > 12under assumptions 3.1 and 3.2, given m processing centres each with
ğ‘›â‰¥ğ¶samples for each arm, with each centre having communication budget ğµâ‰¥1, for any independent
communication protocol, the minimax risk M is lower bounded as follows:
ğ‘€â‰¥Î© 
ğ´ğ¶ğ‘…2
max
ğ‘šğ‘›ğœ† minmin(
maxnğ´ğ¶
ğµ,1o
,ğ‘š)!
Proof 3.4 (sketch): We present a sketch of proof here and defer the full proof to Appendix B.
We construct a hard instance of the problem and use it to derive a lower bound. For each arm, at each centre
we set theğ‘˜th component of context vector in ğ‘™th sample to ğ‘ğ‘™
ğ‘˜=âˆšğœ†minğ‘›ğŸ™ğ‘˜=ğ‘™for the first ğ¶and we set context
vectors for remaining ğ‘›âˆ’ğ¶samples to zero vectors. Although this construction might seem pathological at
first glance, it satisfies Assumption 3.2 and simplifies the analysis greatly. Under our construction we choose
the distribution of the reward ğ‘Ÿğ‘™
ğ‘—forğ‘™th sample for ğ‘—th arm to be ğ‘…maxwith prob.1
2+ğ‘ğ‘™
ğ‘™ğœƒğ‘™
ğ‘—
2ğ‘…maxandâˆ’ğ‘…max
otherwise. We can observe that the expected reward is ğ”¼[ğ‘Ÿğ‘™
ğ‘—]=ğ‘ğ‘™
ğ‘™ğœƒğ‘—,ğ‘™=(cğ‘™)ğ‘‡Î¸ğ‘—, which satisfies the problem
formulation. The reward can also be rewritten as ğ‘Ÿğ‘™
ğ‘—=(2ğ‘ğ‘™
ğ‘—âˆ’1)ğ‘…max, whereğ‘ğ‘™
ğ‘—âˆ¼Bernoulli(1
2+ğ‘ğ‘™
ğ‘˜ğœƒğ‘—
2ğ‘…max). Hence
the data received can be just reduced to the Bernoulli variables {ğ‘ğ‘™
ğ‘—}ğ‘›
ğ‘™=1for each arm ğ‘—. We follow the method
4Under review as submission to TMLR
described in Appendix A. We study the likelihood ratio of ğ‘ğ‘™
ğ‘—underğ‘£ğ‘—,ğ‘˜andğ‘£â€²
ğ‘—,ğ‘˜=âˆ’ğ‘£ğ‘—,ğ‘˜and show that it is
bounded by expn
17âˆšğ‘›ğœ†minğ›¿ğ‘£ğ‘˜
8ğ‘…maxo
. We can thus satisfy the conditions of Lemma A.4, with ğ›¼=17âˆšğ‘›ğœ†minğ›¿ğ‘£ğ‘˜
8ğ‘…max. We
observe that since we havenâ€™t assumed anything about ğ‘ğ‘™
ğ‘—to derive this bound, we get that this bound holds
with probability one and hence the second and third term in the bound resulting from Lemma A.4 are zero.
For the Bernoulli distribution we can easily study the KL-divergence and together with Lemma A.2 we get
that:ğ¼(ğ‘‰,ğ‘Œ)â‰¤17
8ğ›¿2ğ‘šğ‘›ğœ† min
ğ‘…2maxminn
17
8ğµ,ğ¶ğ´
2o
. The rest of the proof consists of choosing such ğ›¿that produces
the tightest bound.
We observe that for the communication not be a bottleneck, we require a communication budget for each
machine of at least ğµ>Î©(ğ´ğ¶). We also see that above this optimal threshold, increasing the communication
budget does not decrease the lower bound because of the max operator. On the other hand, for small
communication budgets, the min operator ensures that the performance cannot be worse than as if we only
used data from one machine. Both of these results are intuitive and show that our bound is thigh with
respect to the communication. Since the optimal communication budget scales with ğ´, this lets us presume
that we can tackle this problem by performing ğ´separate least-squares estimations. Inspired by that, we
present Algorithm 1 and further show in Theorem 3.5 that it can match this lower bound up to logarithmic
factors.
Algorithm 1 (LSE) Distributed offline least-squares
Data:{cğ‘™
ğ‘,ğ‘Ÿğ‘™
ğ‘}ğ‘›
ğ‘™=1forğ‘âˆˆA
On individual machines compute:
forğ‘–âˆˆ{1,...,ğ‘š}do
forğ‘âˆˆğ´do
ğ‘‹ğ‘â†(ğ‘1
ğ‘,...,ğ‘ğ‘›
ğ‘)ğ‘‡rğ‘â†(ğ‘Ÿ1
ğ‘,...,ğ‘Ÿğ‘›
ğ‘)ğ‘‡
Ë†Î¸ğ‘–
ğ‘â†(ğ‘‹ğ‘‡
ğ‘ğ‘‹ğ‘)âˆ’1ğ‘‹ğ‘‡
ğ‘rğ‘
end
Quantise each component of Ë†Î¸ğ‘ğ‘–up to precision ğ‘ƒand send to central server.
end
At central server compute: Ë†Î¸ğ‘â†1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
ğ‘forğ‘âˆˆA
returnÎ¸ğ‘for eachğ‘âˆˆA
Theorem 3.5 Let us define Î¸=(Î¸1,...,Î¸ğ´)to be the concatenated parameter vector for all actions. For
any value ofÎ¸, Algorithm 1 using transmission with precision ğ‘ƒachieves a worst-case risk upper bounded as
follows:
ğ‘Š <O
ğ´ğ¶maxnğ‘…2
max
ğ‘šğ‘›ğœ† min,ğ‘ƒo
Proof 3.6 (sketch): We present a sketch of proof here and defer the full proof to Appendix E. We start
by assuming the transmission is lossless (i.e. the number of bits is infinite) and then study how the MSE
changes when transmission introduces quantisation error. It is a well-known fact that for the least-Squares
estimate, the bias is zero, hence using the bias-variance decomposition, we get that the bound on estimatorâ€™s
variance is also the bound on its MSE. Some algebraic manipulations allow us to show that Var (Ë†ğœƒğ‘–
ğ‘,ğ‘˜) â‰¤
Ãğ¶
ğ‘—=1Ãğ‘›
ğ‘™=1ğ‘„2
ğ‘—,ğ‘˜ğ‘‰2
ğ‘—,ğ‘™
ğœ†minğ‘›Var(ğ‘Ÿğ‘™
ğ‘). We now observe that because of Assumption 3.1, we can use Popoviciu inequality
to obtain Var(ğ‘Ÿğ‘™
ğ‘)â‰¤ğ‘…2
max. Substituting this back into the equation for MSE, we get ğ”¼[(Î¸âˆ’Ë†Î¸)2]â‰¤ğ´ğ¶ğ‘…2
max
ğ‘šğ‘›ğœ† min.
Using the fact that each component is quantised up to precision ğ‘ƒ, the max error introduced by quantisation is
ğ´ğ¶ğ‘ƒ. Combining that with the bound on MSE of lossless transmission, we get the statement of the Theorem.
A direct conclusion of this result is that if we want the quantisation to not affect the worst-case performance
of Algorithm 1, the precision ğ‘ƒneeds to scale as Î˜(ğ‘…2
max
ğ‘šğ‘›ğœ† min)and hence the number of transmitted bits ğµmust
scale asÎ˜(ğ´ğ¶logğ‘šğ‘›ğœ† min
ğ‘…2max). While enjoying the simplicity of analysis, the contextual bandit problem does not
5Under review as submission to TMLR
take into account that the current actions of the agent influence the future state of the environment. This
is an important consideration in personalised suggestions (Liao et al. (2020)), which is one of the problems
likely to be solved by a multitude of low-powered personal devices, fitting within our distributed processing
framework. Hence we proceed to study a more sophisticated model of reinforcement learning problems in
the next section, where we focus on parameter identification in full Markov Decision Processes.
4 Linear Value Function Prediction in Episodic MDP
We consider a Markov Decision Process (MDP) consisting of states ğ‘ âˆˆS, actionsğ‘âˆˆAand rewards the
agent receives that we assume follow Assumption 3.1. We assume we observe the gameplay history of an
agent taking actions according to its policy ğœ‹(ğ‘ |ğ‘):(S,A)â†’[ 0,1], hence the problem studied reduces to
a Markov Reward Process. In episodic setting, we define a return of a policy ğœ‹from stateğ‘ as the sum of
all rewards obtained while following the policy ğœ‹after having visited state ğ‘ , i.e.ğºğ‘ =ğ”¼[Ãğ»
ğ‘˜=ğ‘¡ğ‘Ÿğ‘˜|ğ‘ ğ‘¡=ğ‘ ]. To
evaluate how good a certain policy is in a given Markov Decision Process, it is common to learn its value
functionğ‘£(ğ‘ ):Sâ†’â„, which assigns the return to each state. In many cases, it is sufficient to model the
value function for each state as a linear combination of some features related to the given state, we will
denote a vector of such features as cğ‘ . We assume those features are universally known beforehand and
introduce Assumption 3.2 regarding the feature vectors, similarly as in the multi-armed bandit problem.
Assumption 4.1 Feature vector for each state is normalised, i.e. âˆ¥cğ‘—âˆ¥2
2â‰¤1. If we form a matrix ğ‘‹=
(ğ‘1,...,ğ‘ğ‘›)with feature vectors for all states, then the smallest eigenvalue of the matrix ğ‘‹ğ‘‡ğ‘‹isğœ‚minsuch
that0<ğœ‚minâ‰¤1andğœ‚min=ğ‘†ğœ†minfor some constant ğœ†min.
Although in practice, a linear model will most likely be just an approximation to the true value function,
within our problem setting we will assume that the problems we consider can be perfectly modelled in
this way, i.e. ğ‘£(ğ‘ )=cğ‘‡
ğ‘ Î¸. We will now define our assumptions for episodic and non-episodic settings. We
shall assume that the maximum number of transitions within an episode (so-called "horizon" length) is
equal toğ». We shall refer to the states that can be visited at step â„asâ„-level states and denote a set
of such states as Sâ„and we denote ğ‘†â„=|Sâ„|. Consequently the set of initial states is S0and the set of
terminal states is Sğ». We introduce a different parameter vector to model the value function at each level.
Hence for states at level â„, we have thatâˆ€ğ‘ âˆˆğ‘†â„ğºğ‘ =cğ‘‡
ğ‘ Î¸â„. and the parameter of interest is Î¸=Î¸â„. The
inference is conducted based on the gameplay history, which consists of the steps made during ğ‘episodes,
i.e.â„={{(ğ‘ 1,ğ‘1,ğ‘Ÿ1),...,(ğ‘ ğ‘‡ğ‘™,ğ‘ğ‘‡ğ‘™,ğ‘Ÿğ‘‡ğ‘™)}}ğ‘
ğ‘™=1, whereğ‘‡ğ‘™â‰¤ğ»is the length of the ğ‘™th episode. Within this
section we conduct our analysis under Assumption 4.2, regarding the number of times we visit each state at
a given level within the history.
Assumption 4.2 Within gameplay history, for every state at a given level â„we haveğ¸episodes where it
was visited.
This assumption might be treated as a strong one, however, without it, there exists a simple pathological
example, where there is a special feature equal to zero in every state except for one state that is almost never
visited. In such a case, no matter how many episodes we sample, unless we can guarantee that we have
visited this state at least some number of times, we cannot learn the parameter value for that special feature
with high certainty. We now present our lower bound on minimax risk of estimation of the parameter Î¸â„at
levelâ„in Theorem 4.3.
Theorem 4.3 In a distributed offline episodic linear value function approximation problem at level â„and
context with dimensionality ğ¶and state space size of ğ‘†â„, such thatğ‘†â„â‰¥ğ¶ >12, under assumptions 3.1, 3.2
and 4.2, given m processing centres, with communication budget ğµâ‰¥1, for any independent communication
protocol, the minimax risk M is lower bounded as follows:
ğ‘€â‰¥Î© 
ğ¶(ğ»âˆ’â„)2ğ‘…2
max
ğ‘†â„ğ¸ğ‘šğœ† minmin(
maxnğ¶
ğµlogğ‘š,1o
,ğ‘š
logğ‘š)!
6Under review as submission to TMLR
Proof 4.4 (sketch): We present a sketch of proof here and defer the full proof to Appendix C. We proceed
similarly as in the proof of Theorem 3.3 and set the ğ‘˜th component of feature vector for ğ‘—th state toğ‘ğ‘—,ğ‘˜=âˆšğœ†minğ‘†â„ğŸ™ğ‘˜=ğ‘—for the first ğ‘†â„states and we set feature vectors for remaining ğ‘›âˆ’ğ‘†â„states to zero vectors.
We consider the following Markov Decision Process: starting randomly in one of the states ğ‘—at levelâ„, the
agent chooses one of two actions. Choosing the "good" action causes the agent the receive a reward of ğ‘…max
for the remaining (ğ»âˆ’â„)steps until the episode ends, while the "bad" action causes the agent to receive
a reward ofâˆ’ğ‘…maxuntil the end. The agent draws a random variable ğ‘ğ‘—âˆ¼Bernoulli(1
2+cğ‘‡
ğ‘—Î¸
2(ğ»âˆ’â„)ğ‘…max)and
selects "good" action if ğ‘ğ‘—=1and "bad" action otherwise. We follow similar steps as in Theorem 3.3 and
bound the likelihood ratio, however, under the condition thatÃğ¸
ğ‘™=1(2ğ‘ğ‘™
ğ‘—âˆ’1)< ğ‘, whereğ‘is a quantity we
control. We then use Hoeffding inequality to bound the probability thatÃğ¸
ğ‘™=1(2ğ‘ğ‘™
ğ‘—âˆ’1)> ğ‘. We then derive
a bound on KL-divergence and use Lemma A.2 to derive a second bound on mutual information. We finish
the proof by combining two bounds and choosing such ğ›¿andğ‘that the bound is tightest.
Hence we get that for the risk not to depend on communication budget we need a number of at least
ğµ >Î©(ğ¶
logğ‘š)bits. It might be surprising that the bound decreases as the state space ğ‘†increases, however,
because of Assumption 4.2 we have that each new state effectively increases the number of available samples
for the same number of features. Note that in practice, to obtain a sensible approximation, the dimension
of features is likely to grow with the size of state space.
We now show that this class of problems can be efficiently solved by using performing distributed Monte-
Carlo return estimates. We present Algorithm 2, which uses Monte-Carlo to obtain return estimates and
then least-squared to fit the parameters to feature vectors, which are then quantised and communicated to
the main centre. We prove that it can match this lower bound with communication budget optimal up to
logarithmic factors as stated by Theorem 4.5.
Algorithm 2 (MC LSE) Distributed offline least-squares with Monte-Carlo return estimates
Data:{(ğ‘ ğ‘™
ğ‘¡,ğ‘Ÿğ‘™
ğ‘¡,ğ‘ğ‘™
ğ‘¡)ğ‘‡ğ‘™
ğ‘¡=1}ğ‘
ğ‘™=1
On individual machines compute:
forğ‘–âˆˆ{1,...,ğ‘š}do
âˆ€ğ‘ âˆˆğ‘†ğ‘ğ‘ â†0ğºğ‘ â†0
forğ‘™âˆˆ1,...,ğ‘do
forğ‘ âˆˆğ‘†do
ğ‘“ğ‘ â†step of first time visit to ğ‘ in episodeğ‘™
ğºğ‘ â†ğºğ‘ +Ãğ‘‡ğ‘™
ğ‘¡=ğ‘“ğ‘ ğ‘Ÿğ‘™
ğ‘¡ğ‘ğ‘ â†ğ‘ğ‘ +1
end
end
âˆ€ğ‘ âˆˆğ‘†ğ‘”ğ‘ â†ğºğ‘ 
ğ‘ğ‘ 
ğ‘‹â†(ğ‘1,...,ğ‘ğ‘†)ğ‘‡gâ†(ğ‘”1,...,ğ‘”ğ‘†)ğ‘‡
Ë†Î¸ğ‘–â†(ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡g
Quantise each component of Ë†Î¸ğ‘–up to precision ğ‘ƒand send to central server.
end
At central server compute: Ë†Î¸â†1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
returnÎ¸
Theorem 4.5 For any value of Î¸, Algorithm 2 using transmission with precision ğ‘ƒachieves worst-case risk
upper bounded as follows:
ğ‘Š <O
ğ¶maxn(ğ»âˆ’â„)2ğ‘…2
max
ğ‘šğ‘†â„ğ¸ğœ†min,ğ‘ƒo
Proof 4.6 We observe that the return is always within the range (âˆ’(ğ»âˆ’â„)ğ‘…max,(ğ»âˆ’â„)ğ‘…max), hence by
Popoviciu inequality we get Var (ğ‘”ğ‘™
ğ‘ )â‰¤(ğ»âˆ’â„)2ğ‘…2
maxand thus Var(ğ‘”ğ‘ )â‰¤1
ğ¸(ğ»âˆ’â„)2ğ‘…2
max. Following similar
steps as in the proof of Theorem 3.5 we get the statement of the Theorem.
7Under review as submission to TMLR
Hence, thenumberofbitstransmitted ğµmustscaleas Î˜(ğ¶logğ‘šğ‘†ğ¸ğœ† min
(ğ»âˆ’â„)ğ‘…max)forAlgorithm2toachieveminimax
optimalperformance. Whiletheepisodicsettingcanbeusedtomodelmanyproblems, inapplicationsrelated
to electronic wearables or IoT, we would usually be interested in continuous problems, as by design those
devices are meant to accompany user all the time and constantly improve their quality of life. We thus build
on the results we derived so far and conduct an analysis of the non-episodic problem setting in the next
section.
5 Linear Value Function Prediction in Non-episodic MDP
Contrary to the episodic Markov Reward Processes, in non-episodic setting we asumme all states share the
same parameter vector, which defines a linear relationship between the value function and features of a state.
In non-episodic setting we introduce a discount factor ğ›¾âˆˆ(0,1), and define the return as a discounted sum
of rewards starting from state ğ‘ and following policy ğœ‹afterwards, i.e. ğºğ‘ = ğ”¼[Ãâˆ
ğ‘˜=0ğ‘Ÿğ‘¡+ğ‘˜ğ›¾ğ‘˜|ğ‘ ğ‘¡=ğ‘ ]=cğ‘‡
ğ‘ Î¸
andÎ¸is the parameter we would like to conduct inference about. We define the received gameplay history
as i.i.d. samples of single steps made by the agent, where the initial state ğ‘ sampled from the stationary
distribution ğœ‹(ğ‘ )under its policy, i.e. â„={((ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+))}ğ‘›
ğ‘¡=1, whereğ‘ ğ‘¡âˆ¼ğœ‹(ğ‘ ). We introduce Assumption
5.1 to ensure ğœ‹(ğ‘ )exists.
Assumption 5.1 The Markov Chain describing states visited by the agentâ€™s policy is aperiodic and ir-
reducible, so that the stationary distribution ğœ‹exists. We assume that for each state ğ‘ âˆˆğ‘†we have
ğœ‹minâ‰¤ğœ‹(ğ‘ )â‰¤ğœ‹maxand there is at least one state which frequently visited, i.e. ğœ‹max>0.01.
Theorem 5.2 In a distributed linear value function approximation problem with context dimensionality of
ğ¶ > 12, state space size of ğ‘†â‰¤ğ¶and discount factor of 0< ğ›¾ < 0.99, under assumptions 3.1, 4.1 and
5.1, givenğ‘šprocessing centres each with gameplay history, with each centre having a communication budget
ğµâ‰¥1, the minimax risk ğ‘€of any algorithm is lower bounded as:
ğ‘€â‰¥Î© 
ğ¶ğ‘…2
max
(1âˆ’ğ›¾)2ğœ‹maxğ‘†ğ‘›ğ‘šğœ† minmin(
maxnğ¶logğ‘š
ğµ,1o
,ğ‘š
logğ‘š)!
Proof 5.3 (sketch): We present a sketch of proof here and defer the full proof to Appendix D.
We consider the following MDP: from each state ğ‘—, regardless of the agentâ€™s action it either states in the
same state with probability 1âˆ’ğ‘and receives a reward with a mean of Â¯ğ‘Ÿğ‘—or moves to any other states chosen
with a probability ofğ‘
ğ‘†âˆ’1and receives a reward ğ‘Ÿ0. We see that because of the consistency equation for value
functions we have:
ğ‘£(ğ‘—)=(1âˆ’ğ‘)Â¯ğ‘Ÿğ‘—+ğ‘ğ‘Ÿ0+ğ›¾âˆ‘ï¸
ğ‘ â€²âˆˆğ‘†{ğ‘—}ğ‘£(ğ‘ â€²)ğ‘
ğ‘†âˆ’1+ğ›¾ğ‘£(ğ‘—)(1âˆ’ğ‘)
We now set ğ‘Ÿ0=âˆ’ğ›¾Ã
ğ‘ â€²âˆˆğ‘†{ğ‘—}ğ‘£(ğ‘ â€²)
ğ‘†âˆ’1to get that Â¯ğ‘Ÿğ‘—=ğ‘£(ğ‘—)(1âˆ’ğ›¾+ğ‘ğ›¾)
1âˆ’ğ‘. We set feature vectors and parameters in
the same way as in the proof of Theorem 4.3. We also set Â¯ğ‘Ÿğ‘—=(2ğ‘ğ‘—âˆ’1)ğ‘…maxwhereğ‘ğ‘—âˆ¼Bernoulli(1
2+âˆšğœ†minğ‘†ğ‘£ğ‘˜ğ›¿
2ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘). We thus have a similar situation as in the proof of Theorem 4.3, where the data received
can be reduced to outcomes of Bernoulli trials. We proceed in the same way to obtain a bound that depends on
ğ‘and gets tighter as ğ‘â†’0. We now can index data generating distribution by ğ‘and obtain their supremum,
which gives the statement of the Theorem.
While we can extend the lower bound to the non-episodic case while following a similar method as in the
episodic one, we see that because Monte-Carlo estimators for return cannot be used in the non-episodic
setting, we cannot extend Algorithm 2 in the same way. In this new setting, an algorithm needs to utilise
the consistency of the value function. We thus propose Algorithm 3, which is a distributed variant of
temporal difference learning. We build on the analysis conducted by Bhandari et al. (2018) to upper bound
its worst-case performance in Theorem 5.4.
8Under review as submission to TMLR
Algorithm 3 (TD) Distributed offline temporal difference learning
Data:(cğ‘¡,ğ‘Ÿğ‘¡,ğ‘ğ‘¡,cğ‘¡+)ğ‘‡
ğ‘¡=1
On individual machines compute:
forğ‘–âˆˆ{1,...,ğ‘š}do
Ë†Î¸ğ‘–â†Î¸0
forğ‘¡âˆˆ1,...,ğ‘do
gğ‘¡â†(ğ‘Ÿğ‘¡+ğ›¾cğ‘‡
ğ‘¡+Ë†Î¸ğ‘–âˆ’cğ‘‡
ğ‘¡Ë†Î¸ğ‘–)cğ‘¡
Ë†Î¸ğ‘–â†Ë†Î¸ğ‘–+ğ›¼ğ‘¡gğ‘¡
end
Quantise each component of Ë†Î¸ğ‘–up to precision ğ‘ƒand send to central server.
end
At central server compute: Ë†Î¸â†1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
return Ë†Î¸
Theorem 5.4 The worst-case risk of Algorithm 3 run with a learning rate of ğ›¼ğ‘¡=ğ›½
Î›+ğ‘¡
ğœ”withğ›½=2
(1âˆ’ğ›¾)ğœ”
andÎ›=16
(1âˆ’ğ›¾)2ğœ”is upper bounded as follows:
ğ‘Šâ‰¤O 
max(max{ğ‘…2
max
ğ‘†ğœ‹minğœ†minğ‘š,âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2}
1+(1âˆ’ğ›¾)2ğ‘›,ğ¶ğ‘ƒ)!
Proof 5.5 (sketch): We present a sketch of proof here and defer the full proof to Appendix F.
We define Â¯Î¸ğ‘¡:=1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
ğ‘¡to be the average vector from all machines at timestep ğ‘¡. Note that this vector
is never actually created, except for the last step when we average all final results. We can see that it must
satisfy the following recursive relation:
Â¯Î¸ğ‘¡+1=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1Ë†Î¸ğ‘–
ğ‘¡+1=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1[Ë†Î¸ğ‘–
ğ‘¡+ğ›¼ğ‘¡gğ‘–
ğ‘¡]=Â¯Î¸ğ‘¡+ğ›¼ğ‘¡1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1gğ‘–
ğ‘¡
We utilise Lemmas H.1 and H.3 to show that:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]â‰¤ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2](1âˆ’2ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)+ğ›¼2
ğ‘¡)+ğ›¼2
ğ‘¡
ğ‘šğ‘…2
max
where we define ğœ”to be the smallest eigenvalue of the covariance matrix weighted by the stationary distri-
bution, i.e. smallest eigenvalue ofÃ
ğ‘ âˆˆğ‘†ğœ‹(ğ‘ )cğ‘ cğ‘‡
ğ‘ . We then finish the proof by induction and observing that
by concavity of eigenvalues we have ğ‘†ğœ‹minğœ†minâ‰¤ğœ”â‰¤ğœ†minâ‰¤1.
We see that there are essentially two terms contributing to the worst-case risk, the termğ‘…2
max
ğ‘†ğœ‹minğœ†minğ‘šresulting
from gradientâ€™s variance and the term âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2resulting from initial bias. We see that increasing
the number of machines ğ‘šwill only decrease the variance term, but will not decrease the overall worst-case
risk if the initial bias is larger. This result has a practical implication, as we essentially show that using
more devices within a network is not necessarily going to improve our estimate. It might spuriously appear
that this algorithm is contradicting the lower bound as the worst-case risk has no explicit dependence on
the dimensionality of the features. However, we can easily show (explicitly derived in Appendix G) that the
initial bias will scale at least as O(ğ‘†ğ‘…2
ğœ†min(1âˆ’ğ›¾)), which is consistent with the bound stated in Theorem 5.2, as
we assumed ğ¶â‰¤ğ‘†.
In comparison to existing distributed versions of TD learning where gradients are communicated between
machines at each step, in our version only the final estimates are communicated, hence the initial bias cannot
be cancelled by introducing data from more machines. This is, however, due to the difficulty of our problem
setting. With one communication round, it is not possible to constantly transmit information about the
gradient updates. This issue would come up in practice, whenever the devices we run our algorithm on are
9Under review as submission to TMLR
not constantly connected to the network and communication in real-time, during the algorithm runtime is
not possible.
It might come as a surprise that although we can easily propose an optimal algorithm for the episodic case,
the same is not true for the non-episodic setting. In fact, we can see that for any unbiased algorithm that
has optimal risk in a non-distributed version, its distributed version averaging results from all machines will
also have optimal risk as long as the lower bound scales as the inverse of the number of machines ğ‘š. In
the non-episodic setting, however, we cannot directly obtain estimates of the return and hence need to rely
on a method utilising the temporal structure of the problem. Although in the non-distributed version, the
temporal difference can often be superior to Monte-Carlo methods even in the episodic case due to smaller
variance, we see that in a single-round communication setting, the bias is preventing the algorithm from
efficiently utilising data available on all machines.
6 Discussions
We have studied three offline RL problems in a special case of distributed processing. We have identified a
lower bound on minimax risk in each problem and proposed algorithms that match these lower bound up to
logarithmic factors in the cases of contextual linear bandits and episodic MDP. In the case of non-episodic
MDP, we have proposed a distributed version of temporal difference learning and analysed its worst-case
risk. We have shown that its worst-case risk decreases with the number of cooperating machines only until
some point, where the risk due to initial bias starts to dominate over the risk caused by variance.
Notably, our work studies a different problem from Federated Learning (KoneÄn` y et al. (2015)) wherein
a typical setting assumes one can send parameter values to the central server while minimising the total
number of communication rounds. In our setting, we assume more restrictive conditions with regards to
communication by limiting the communication round to be only one and by further restricting the number
of information (i.e., the bits) each machine can send.
Moreover, we would like to emphasise that our bounds are developed on the risk in estimating the parameter
values rather than on the regret (Wang et al., 2019). We note that there are problems where learning the
parameter values might be of greater interest than just minimising the regret of actions. Recalling the
example of physical activity suggestion in the Introduction, we not only want to learn the optimal way of
suggesting the activities, but also analyse which factors influence how healthy the lifestyle of an individual
is. Hence, it would be more beneficial to estimate the parameters of, for example, the value function and
decrease the risk of those estimates.
In this paper, we have derived the first lower bound for distributed RL problems other bandits. Our method
relies on constructing hard instances and converting them to statistical inference problems. This approach
can be easily applied to other settings such as state-action value estimation or off-policy learning, which
constitutes one of the directions of future work. We also identified a weakness of TD learning resulting from
its initial bias. Can this bias be estimated in practice so that we do not unnecessarily utilise data from more
machines than needed? Can we correct for initial bias so that worst-case risk always decreases with more
machines? Can it match the derived lower bound? We leave these questions open to future research.
References
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Conference on learning theory , pp. 1691â€“1692. PMLR, 2018.
Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. Advances
in Neural Information Processing Systems , 34, 2021.
Jakub KoneÄn` y, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed optimization
beyond the datacenter. arXiv preprint arXiv:1511.03575 , 2015.
10Under review as submission to TMLR
Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized heartsteps: A reinforce-
ment learning algorithm for optimizing physical activity. Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies , 4(1):1â€“22, 2020.
Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: Near-optimal
regret with efficient communication. arXiv preprint arXiv:1904.06309 , 2019.
Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic lower bounds
for distributed statistical estimation with communication constraints. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing
Systems, volume 26. Curran Associates, Inc., 2013.
11Under review as submission to TMLR
A General methods for deriving lower bounds
Let us define a set V={âˆ’1,1}ğ‘‘and sample ğ‘‰uniformly fromV. Conditioned on ğ‘‰=ğ‘£we sampleğ‘‹1from
ğ‘ƒğ‘‹1(Â·|ğ‘‰=ğ‘£)so thatğœƒğ‘£=ğœƒ(ğ‘ƒğ‘¥(Â·|ğ‘£))=ğ›¿ğ‘£whereğ›¿ >0is a fixed quantity we control. Consider a Markov
chainğ‘‰â†’ğ‘‹1â†’Â·Â·Â·â†’ğ‘‹ğ‘›â†’ğ‘Œâ†’Ë†ğœƒ. Under these conditons, it was proven by Zhang et al. (2013) that for
ğ‘‘ >12we have:
ğ‘€(ğœƒ,ğ‘ƒ,ğµ)â‰¥ğ›¿2
âŒŠğ‘‘
6âŒ‹+1 
1âˆ’ğ¼(ğ‘Œ,ğ‘‰)+log 2
ğ‘‘
6!
Hence, if we can choose a ğ›¿such thatğ¼(ğ‘Œ,ğ‘‰)is upper bounded, this directly translates to a lower bound
on the minimax risk. It hence remains to upper bound the mutual information any possible message ğ‘Œcan
carry aboutv, so that we can use it to obtain a lower bound on minimax risk. To that goal, we will use
the inequalities described in subsequent subsections and combine them into one upper bound on mutual
information. After that, it remains to find such a ğ›¿that this upper bound remains constant or is at most
linear in the dimensionality of the problem.
A.1 Tensorisation of information
Lemma A.1 (Tensorisation of information property in Zhang et al. (2013)) Ifğ‘‰is a parameter
of distribution we wish to make inference about and the message ğ‘Œğ‘–send by the ğ‘–th processing centre is
constructed based only on the data ğ‘‹ğ‘–then the following is true:
ğ¼(ğ‘‰;ğ‘Œ1:ğ‘š)â‰¤ğ‘šâˆ‘ï¸
ğ‘–=1ğ¼(ğ‘‰;ğ‘Œğ‘–)
A.2 Bound on mutual information by KL-divergence
Within this section, we present another bound on mutual information, which utilises the KL-divergence of
data-generating distribution.
Lemma A.2 Letğ‘‰â†’ğ‘‹ğ‘–â†’ğ‘Œğ‘–form a Markov Chain and ğ‘Œ=(ğ‘Œ1,...,ğ‘Œğ‘š). Letğ‘‰be ağ‘‘-dimensional
vector with each component sampled uniformly from {ğ‘£ğ‘—,ğ‘£âˆ—
ğ‘—}. Let eachğ‘‹ğ‘–consist of(ğ‘…1,...,ğ‘…ğ‘‘)where every
ğ‘…1=(ğ‘…1
1,...,ğ‘…ğ‘›
1). Let every ğ‘…ğ‘™
ğ‘—be sampled from the same distribution parametrised by ğ‘£ğ‘—. Then:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤ğ‘šğ‘›
4ğ‘‘âˆ‘ï¸
ğ‘—=1 KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£),ğ‘(ğ‘…1
ğ‘—|ğ‘£âˆ—)]+KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£âˆ—),ğ‘(ğ‘…1
ğ‘—|ğ‘£)]
Proof A.3 From tensorisation of information (Lemma A.1) and data-processing inequality we get:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤ğ‘šğ¼(ğ‘Œğ‘–,ğ‘‰)â‰¤ğ‘šğ¼(ğ‘‹ğ‘–,ğ‘‰)
We now observe that ğ‘…ğ‘—are independent, hence:
ğ¼((ğ‘…1,...,ğ‘…ğ‘‘),ğ‘‰)=ğ‘‘âˆ‘ï¸
ğ‘—=1ğ¼((ğ‘…1
ğ‘—,...,ğ‘…ğ‘›
ğ‘—),ğ‘‰)=ğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1ğ¼(ğ‘…ğ‘™
ğ‘—,ğ‘‰|ğ‘…1:ğ‘™âˆ’1
ğ‘—)=
=ğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1[ğ»(ğ‘…ğ‘™
ğ‘—|ğ‘…1:ğ‘™âˆ’1
ğ‘—)âˆ’ğ»(ğ‘…ğ‘™
ğ‘—|ğ‘‰,ğ‘…1:ğ‘™âˆ’1
ğ‘—)]â‰¤ğ‘‘âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1[ğ»(ğ‘…ğ‘™
ğ‘—)âˆ’ğ»(ğ‘…ğ‘™
ğ‘—|ğ‘‰)]=ğ‘›ğ‘‘âˆ‘ï¸
ğ‘—=1ğ¼(ğ‘…1
ğ‘—,ğ‘‰)
Where the last inequality is true since conditioning can only reduce entropy and ğ‘…ğ‘–
ğ‘—are independent given ğ‘‰.
We know focus on the mutual information:
ğ¼(ğ‘…1
ğ‘—,ğ‘‰)=KL[ğ‘(ğ‘…1
ğ‘—,ğ‘‰),ğ‘(ğ‘…1
ğ‘—)ğ‘(ğ‘‰)]=KL[ğ‘(ğ‘…1
ğ‘—|ğ‘‰)ğ‘(ğ‘‰),ğ‘(ğ‘…1
ğ‘—)ğ‘(ğ‘‰)]
12Under review as submission to TMLR
=âˆ‘ï¸
ğ‘£ğ‘(ğ‘£)KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£),ğ‘(ğ‘…1
ğ‘—)]=âˆ‘ï¸
ğ‘£ğ‘(ğ‘£)KL[âˆ‘ï¸
ğ‘£â€²ğ‘(ğ‘…1
ğ‘—|ğ‘£)ğ‘(ğ‘£â€²),âˆ‘ï¸
ğ‘£â€²ğ‘(ğ‘…1
ğ‘—|ğ‘£â€²)ğ‘(ğ‘£â€²)]
Because of convexity of KL-divergence we get:
ğ¼(ğ‘…1
ğ‘—,ğ‘‰)â‰¤âˆ‘ï¸
ğ‘£,ğ‘£â€²ğ‘(ğ‘£)ğ‘(ğ‘£â€²)KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£),ğ‘(ğ‘…1
ğ‘—|ğ‘£â€²)]=
=1
4 KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£),ğ‘(ğ‘…1
ğ‘—|ğ‘£âˆ—)]+KL[ğ‘(ğ‘…1
ğ‘—|ğ‘£âˆ—),ğ‘(ğ‘…1
ğ‘—|ğ‘£)]
A.3 Bound on mutual information by set construction
We present a first bound on mutual information, which can requires constructing a set such that the samples
belong to it with some probability.
Lemma A.4 (Lemma 4 in Zhang et al. (2013)) Let V be sampled uniformly from {âˆ’1,1}ğ‘. For any
(ğ‘–,ğ‘—)assume that ğ‘‹ğ‘–
ğ‘—is independent of {ğ‘‰ğ‘—â€²:ğ‘—â€²â‰ ğ‘—}givenğ‘‰ğ‘—. Letğ‘ƒğ‘‹ğ‘—be the probability measure of ğ‘‹ğ‘—and
letğµğ‘—be a set such that for some ğ›¼:
sup
ğ‘†âˆˆğœ(ğµğ‘—)ğ‘ƒğ‘‹ğ‘—(ğ‘†|ğ‘‰=ğ‘£)
ğ‘ƒğ‘‹ğ‘—(ğ‘†|ğ‘‰=ğ‘£â€²)â‰¤exp(ğ›¼)
Define random variable ğ¸ğ‘—=1fğ‘‹ğ‘—âˆˆğµğ‘—and 0 otherwise. Then
ğ¼(ğ‘‰,ğ‘Œğ‘–)â‰¤ğ‘âˆ‘ï¸
ğ‘—=1ğ»(ğ¸ğ‘—)+ğ‘âˆ‘ï¸
ğ‘—=1ğ‘ƒ(ğ¸ğ‘—=0)+2(ğ‘’4ğ›¼âˆ’1)2ğ¼(ğ‘‹ğ‘–,ğ‘Œğ‘–)
Additionally if ğ›¼<1.2564
4, then the following is also true:
ğ¼(ğ‘‰,ğ‘Œğ‘–)â‰¤ğ‘âˆ‘ï¸
ğ‘—=1ğ»(ğ¸ğ‘—)+ğ‘âˆ‘ï¸
ğ‘—=1ğ‘ƒ(ğ¸ğ‘—=0)+128ğ›¼2ğ¼(ğ‘‹ğ‘–,ğ‘Œğ‘–)
Proof A.5 The first statement is Lemma 4 of Zhang et al. (2013).
The second statement directly follows from it, as for ğ‘¥ <1.2564, it holds that exp(ğ‘¥)âˆ’1<2ğ‘¥
B Proof of Theorem 3.3
Theorem 3.3 In a distributed offline linear contextual MAB problem with ğ´actions and context with di-
mensionality ğ¶such thatğ¶ğ´ > 12under assumptions 3.1 and 3.2, given m processing centres each with
ğ‘›â‰¥ğ¶samples for each arm, with each centre having communication budget ğµâ‰¥1, for any independent
communication protocol, the minimax risk M is lower bounded as follows:
ğ‘€â‰¥Î© 
ğ´ğ¶ğ‘…2
max
ğ‘šğ‘›ğœ† minmin(
maxnğ´ğ¶
ğµ,1o
,ğ‘š)!
Proof B.1 Where we omit indexing by the machine index ğ‘–, this means that statement holds for all machines.
Let us consider a problem where we set the parameter vector for ğ‘—th arm asÎ¸ğ‘—=ğ›¿vğ‘—whereğ›¿is a quantity
we will specify later, vğ‘—is a vector sampled uniformly from {âˆ’1,1}ğ¶and theğ‘˜th element of context vector
for theğ‘™th sample is set to ğ‘ğ‘™
ğ‘˜=âˆšğœ†minğ‘›ğŸ™ğ‘˜=ğ‘™, so that Assumption 3.2 is satisfied.
LetVbe a concatenated vector defined as V=(v1,...,vğ¶)ğ‘‡about which we would like to perform inference.
Letğ‘Ÿğ‘™
ğ‘—be the reward received after pulling ğ‘—th arm for the ğ‘™th time. We now define the underlying process for
generating the reward as follows, let ğ‘Ÿğ‘™
ğ‘—=(2ğ‘ğ‘™
ğ‘—âˆ’1)ğ‘…max, whereğ‘ğ‘™
ğ‘—=Bernoulli(1
2+ğ›¿ğ‘£ğ‘‡
ğ‘—ğ‘ğ‘™
2ğ‘…max). We see that under
such construction, the reward is always within [âˆ’ğ‘…max,ğ‘…max]and its expected value is ğ”¼[ğ‘Ÿğ‘™
ğ‘—]=ğ›¿vğ‘‡
ğ‘—cğ‘™=Î¸ğ‘—cğ‘™.
13Under review as submission to TMLR
Letğ‘Œğ‘–be the message send by the ğ‘–th processing centre and ğ‘‹ğ‘–be allğ‘Ÿğ‘™
ğ‘—for allğ‘—andğ‘™stored on the ğ‘–th
processing centre and let us define ğ‘ƒğ‘–in the same way for ğ‘ğ‘™
ğ‘—. We observe that ğ‘‰â†’ğ‘ƒğ‘–â†’ğ‘‹ğ‘–â†’ğ‘Œğ‘–forms a
Markov Chain. We see that this scenario satisfies the assumptions required to use the method from Appendix
A withğ‘‘=ğ´ğ¶. Hence we would like to find a bound on ğ¼(ğ‘‰,ğ‘Œ1:ğ‘š). First let us consider the likelihood ratio
ofğ‘ğ‘™
ğ‘—forğ‘™={1,...,ğ‘›}givenğ‘£ğ‘—,ğ‘˜andğ‘£â€²
ğ‘—,ğ‘˜=âˆ’ğ‘£ğ‘—,ğ‘˜. We observe that for our construction of cğ‘˜we essentially
havevğ‘‡
ğ‘—cğ‘˜=ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜. Hence the likelihood ratio can be expressed as:
 1
2+ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max
1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max!ğ‘ğ‘˜
ğ‘— 1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max
1
2+ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max!1âˆ’ğ‘ğ‘˜
ğ‘—
= 1
2+ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max
1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘—,ğ‘˜ğ›¿
2ğ‘…max!2ğ‘ğ‘˜
ğ‘—âˆ’1
(1)
Forğ‘¥ <1
4we have that1+ğ‘¥
1âˆ’ğ‘¥â‰¤exp{17
8ğ‘¥}, hence whenğ‘ğ‘˜
ğ‘˜ğ‘£ğ‘˜ğ›¿
ğ‘…maxâ‰¤1
4(satisfied whenğ‘£ğ‘˜ğ›¿âˆšğ¶ğœ†min
ğ‘…maxâ‰¤1
4), the ratio
above is bounded by:
â‰¤expn17ğ‘ğ‘˜
ğ‘˜ğ›¿ğ‘£ğ‘—,ğ‘˜
8ğ‘…max
2ğ‘ğ‘˜
ğ‘—âˆ’1o
=expn17âˆšğ‘›ğœ†minğ›¿ğ‘£ğ‘˜
8ğ‘…max
2ğ‘ğ‘˜
ğ‘—âˆ’1o
We observe that we always have2ğ‘ğ‘˜
ğ‘—âˆ’1â‰¤1, hence the ratio is bounded by exp{17ğ›¿âˆšğ‘›ğœ†min
8ğ‘…max}. We can thus
satisfy the conditions of Lemma A.4 with ğ›¼=17ğ›¿âˆšğ‘›ğœ†min
8ğ‘…maxif we define ğµğ‘—,ğ‘˜={ğ‘ğ‘˜
ğ‘—: 2ğ‘ğ‘˜
ğ‘—âˆ’1â‰¤1}, where we have
thatğ‘ƒ(ğ‘ğ‘˜
ğ‘—âˆˆğµğ‘—,ğ‘˜)=1. If we can set ğ›¿such that17ğ›¿âˆšğ‘›ğœ†min
8ğ‘…max<1.2564
4, then we get:
ğ¼(ğ‘‰,ğ‘Œğ‘–)â‰¤289ğœ†minğ‘›ğ›¿2
64ğ‘…2maxğ¼(ğ‘‹ğ‘–,ğ‘Œğ‘–)
Note that we use a two-dimensional index for ğ‘‹, whereas in Lemma A.4, the index is one dimensional. This
is just a matter of notation and those two types of indexing are mathematically equivalent. We see that we
can boundğ¼(ğ‘‹ğ‘–,ğ‘Œ)as follows:
ğ¼(ğ‘‹ğ‘–,ğ‘Œğ‘–)â‰¤ğ»(ğ‘Œğ‘–)â‰¤ğµ
Combining this with the tensorisation of information (Lemma A.1) we get:
ğ¼(ğ‘‰,ğ‘Œ)â‰¤289ğœ†minğ‘šğ‘›ğ›¿2
64ğ‘…2maxğµ (2)
We now develop a second inequality for ğ¼(ğ‘‰,ğ‘Œ). We observe for the distribution of ğ‘ğ‘˜
ğ‘˜we have the following:
KL[ğ‘(ğ‘ğ‘˜
ğ‘˜|ğ‘£),ğ‘(ğ‘ğ‘˜
ğ‘˜|ğ‘£âˆ—)]=1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
log 1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max!
+1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
log 1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max!
=
=1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…maxâˆ’1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
log 1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max!
=ğ‘ğ‘˜
ğ‘˜ğ›¿
ğ‘…maxlog 1
2+ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max
1
2âˆ’ğ‘ğ‘˜
ğ‘˜ğ›¿
2ğ‘…max!
Same as before we use the fact that for ğ‘¥ <1
4we have that1+ğ‘¥
1âˆ’ğ‘¥â‰¤exp{17
8ğ‘¥}, we get:
KL[ğ‘(ğ‘ğ‘˜
ğ‘˜|ğ‘£),ğ‘(ğ‘ğ‘˜
ğ‘˜|ğ‘£âˆ—)]â‰¤17
8(ğ‘ğ‘˜
ğ‘˜)2ğ›¿2
ğ‘…2max=17
8ğ‘›ğœ†minğ›¿2
ğ‘…2max(3)
Hence by Lemma A.2 with ğ‘‘=ğ´ğ¶and identifying ğ‘…ğ‘˜=(ğ‘ğ‘˜
ğ‘˜)we get:
ğ¼(ğ‘‰,ğ‘Œ)â‰¤17
8ğ‘šğ‘›ğ´ğ¶ğ›¿2ğœ†min
ğ‘…2max(4)
14Under review as submission to TMLR
Combining inequalities 2 and 4 we get:
ğ¼(ğ‘‰,ğ‘Œ)â‰¤17
8ğ›¿2ğ‘šğ‘›ğœ† min
ğ‘…2maxminn17
8ğµ,ğ¶ğ´
2o
We can now set:
ğ›¿2
ğ´â‰¤1
10ğ¶ğ´ğ‘…2
max
17
8ğ‘šğ‘›ğœ† minminn
17
8ğµ,ğ¶ğ´
2o=1
10ğ‘…2
max
17
8ğ‘šğ‘›ğœ† minminn
17
8ğµ
ğ¶ğ´,1
2o
ğ›¿2
ğµâ‰¤1
10ğ‘…2
max
17
8ğ‘›ğœ†min
ğ›¿=min{ğ›¿ğ´,ğ›¿ğµ}
We see that under such construction we haveğ›¿âˆšğ‘›ğœ†min
ğ‘…max<1
4and17ğ›¿âˆšğ‘›ğœ†min
8ğ‘…max<1.2564
4and we can also bound the
mutual information as follows:
ğ¼(ğ‘‰,ğ‘Œ)â‰¤ğ¶ğ´
10
We now remind the Assumption of the Theorem that ğ´ğ¶ > 12, which allows us to show:
 
1âˆ’ğ¼(ğ‘Œ,ğ‘‰)+log 2
ğ´ğ¶
6!
â‰¥ 
0.65âˆ’ğ¼(ğ‘Œ,ğ‘‰)
ğ´ğ¶
6!
â‰¥ 
0.65âˆ’6
10!
â‰¥0 (5)
Which together with the method from Appendix A completes the proof.
C Proof of Theorem 4.3
Theorem 4.3 In a distributed offline episodic linear value function approximation problem at level â„and
context with dimensionality ğ¶and state space size of ğ‘†â„, such thatğ‘†â„â‰¥ğ¶ >12, under assumptions 3.1, 3.2
and 4.2, given m processing centres, with communication budget ğµâ‰¥1, for any independent communication
protocol, the minimax risk M is lower bounded as follows:
ğ‘€â‰¥Î© 
ğ¶(ğ»âˆ’â„)2ğ‘…2
max
ğ‘†â„ğ¸ğ‘šğœ† minmin(
maxnğ¶
ğµlogğ‘š,1o
,ğ‘š
logğ‘š)!
Proof C.1 We consider the following Markov Decision Process: starting randomly in one of the states ğ‘–
at levelâ„, the agent chooses one of two actions. Choosing the "good" action causes the agent the receive a
reward ofğ‘…maxfor the remaining (ğ»âˆ’â„)steps until the episode ends. Choosing the "bad" action causes
the agent to receive a reward of âˆ’ğ‘…maxuntil the end of episode. The agent draws a random variable ğ‘ğ‘—âˆ¼
Bernoulli(1
2+cğ‘‡
ğ‘—Î¸
2(ğ»âˆ’â„)ğ‘…max)and selects "good" action if ğ‘ğ‘—=1and "bad" action otherwise.
We see that with such construction, the maximum absolute value of the reward can never be greater than
ğ‘…maxand the mean of the return from state ğ‘—isğ”¼[ğºğ‘—]= ğ”¼[Ãğ»
ğ‘¡=â„ğ‘Ÿğ‘—(ğ‘¡)]= ğ”¼[Ãğ»
ğ‘¡=â„ğ‘Ÿğ‘—(ğ‘¡)|ğ‘ğ‘—=1]ğ‘ƒ(ğ‘ğ‘—=
1)+ğ”¼[Ãğ»
ğ‘¡=â„ğ‘Ÿğ‘—(ğ‘¡)|ğ‘ğ‘—=0]ğ‘ƒ(ğ‘ğ‘—=0)=cğ‘‡
ğ‘—Î¸and hence the value function of state ğ‘—isğ‘£(ğ‘—)=cğ‘‡
ğ‘—Î¸.
Let us now define the parameters as Î¸=ğ›¿v, wherevis sampled uniformly from {âˆ’1,1}ğ¶. For every episode,
gameplay history contains (ğ»âˆ’â„)tuples(cğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡). Hence, we can just treat the received information as ğ¸
samples of ğ‘ğ‘—and contextcğ‘—for each state ğ‘—âˆˆğ‘†â„. We can see that this construction satisfies the problem
setting of Appendix A, with ğ‘‘=ğ¶. We select context elements as ğ‘ğ‘—,ğ‘˜=âˆšğ‘†ğœ†minğŸ™ğ‘—=ğ‘˜so that Assumption 3.2
is satisfied. Let us now consider the likelihood ratio of {ğ‘ğ‘™
ğ‘˜}ğ¸
ğ‘™=1givenğ‘£ğ‘˜=âˆ’ğ‘£â€²
ğ‘˜, definingğ‘›ğ‘™
ğ‘˜=1âˆ’ğ‘ğ‘™
ğ‘˜we have:
ğ¸Ã–
ğ‘™=1 1
2+ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)
1
2âˆ’ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)!ğ‘ğ‘™
ğ‘˜ 1
2âˆ’ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)
1
2+ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)!1âˆ’ğ‘ğ‘™
ğ‘˜
=ğ¸Ã–
ğ‘™=1 1
2+ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)
1
2âˆ’ğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
2ğ‘…max(ğ»âˆ’â„)!ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜
15Under review as submission to TMLR
Forğ‘¥ <1
4we have that1+ğ‘¥
1âˆ’ğ‘¥â‰¤exp{17
8ğ‘¥}, hence whenğ‘ğ‘—,ğ‘˜ğ‘£ğ‘˜ğ›¿
ğ‘…max(ğ»âˆ’â„)â‰¤1
4(satisfied whenğ‘£ğ‘˜ğ›¿âˆšğ‘†ğœ†min
ğ‘…max(ğ»âˆ’â„)â‰¤1
4), the
ratio above is bounded by:
â‰¤expn17ğ›¿ğ‘£ğ‘˜
8ğ‘…max(ğ»âˆ’â„)ğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘—,ğ‘˜h
ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜io
â‰¤expn17âˆšğ‘†ğœ†minğ›¿ğ‘£ğ‘˜
8ğ‘…max(ğ»âˆ’â„)ğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜o
(6)
If it holds thatÃğ¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜â‰¤ğ‘then we have that the ratio is bounded by exp{17ğ›¿âˆšğœ†min
8ğ‘…max(ğ»âˆ’â„)ğ‘}. Hence we can
satisfy the conditions of Lemma A.4 with ğ›¼=17ğ›¿âˆšğ‘†ğœ†minğ‘
8ğ‘…max(ğ»âˆ’â„)if we define ğµğ‘˜as:
ğµğ‘˜=n
(ğ‘ğ‘™
ğ‘˜,...,ğ‘ğ‘™
ğ‘˜)}ğ¸
ğ‘™=1âˆˆâ„¤ğ¸
+:ğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜<ğ‘o
To complete proof it remains to bound:
ğ‘ƒ(ğ¸ğ‘˜=0)=1
2 
ğ‘ƒğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜>ğ‘ğ‘£ğ‘˜=1
+ğ‘ƒğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜>ğ‘ğ‘£ğ‘˜=âˆ’1
+ğ‘ƒğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜<âˆ’ğ‘|ğ‘£ğ‘˜=1
+ğ‘ƒğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜<âˆ’ğ‘ğ‘£ğ‘˜=âˆ’1!
We now notice that due to symmetry, the first and fourth time are equal and so are second and third. We
also notice that the first term must be greater or equal to the second. This gives:
ğ‘ƒ(ğ¸ğ‘˜=0)â‰¤2ğ‘ƒğ¸âˆ‘ï¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜>ğ‘ğ‘£ğ‘˜=1
We notice that the mean ofÃğ¸
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜isğœ‡ğ‘˜=ğ‘£ğ‘˜ğ›¿
(ğ»âˆ’â„)ğ‘…maxÃğ¸
ğ‘™=1ğ‘ğ‘˜=ğ›¿ğ¸ğ‘£ğ‘˜âˆšğ‘†ğœ†min
(ğ»âˆ’â„)ğ‘…max. We can subtract it from
both side of inequality and divide them by ğ¸:
ğ‘ƒ(ğ¸ğ‘˜=0)â‰¤2ğ‘ƒ1
ğ¸"ğ¸âˆ‘ï¸
ğ‘™=1ğ»âˆ‘ï¸
ğ‘¡=ğ»âˆ’â„ğ‘ğ‘™
ğ‘˜(ğ‘¡)âˆ’ğ‘›ğ‘™
ğ‘˜(ğ‘¡)#
âˆ’ğœ‡ğ‘˜
ğ¸>ğ‘
ğ¸âˆ’ğœ‡ğ‘˜
ğ¸ğ‘£ğ‘˜=1
Sinceğ‘ğ‘™
ğ‘˜(ğ‘¡)are independent given ğ‘£ğ‘˜, we can use Hoeffding inequality with the number of variables equal to
ğ¸, each being confined to [0,1]. Thus ifğ‘ > ğœ‡ğ‘˜:
ğ‘ƒ(ğ¸ğ‘˜=0)â‰¤2 exp(
âˆ’2ğ¸2(ğ‘
ğ¸âˆ’ğœ‡ğ‘˜
ğ¸)2
ğ¸)
=2 exp(
âˆ’2(ğ‘âˆ’ğœ‡ğ‘˜)2
ğ¸)
=2 exp(
âˆ’2
ğ‘âˆ’ğ›¿ğ¸âˆšğ‘†ğœ†min
(ğ»âˆ’â„)ğ‘…max2
ğ¸)
(7)
Same as in Equation 3, we can bound the KL-divergence as follows:
KL[ğ‘(ğ‘ğ‘™
ğ‘˜|ğ‘£),ğ‘(ğ‘ğ‘™
ğ‘˜|ğ‘£âˆ—)]â‰¤17
8(ğ‘ğ‘˜
ğ‘˜)2ğ›¿2
(ğ»âˆ’â„)2ğ‘…2max=17
8ğœ†minğ›¿2
(ğ»âˆ’â„)2ğ‘…2max
We now use Lemma A.2 with ğ‘‘=ğ¶and identifying ğ‘…ğ‘™
ğ‘˜=ğ‘ğ‘™
ğ‘˜to obtain:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤17ğ‘šğ‘†ğ¸
16ğ¶ğœ†min
(ğ»âˆ’â„)2ğ‘…2maxğ›¿2(8)
16Under review as submission to TMLR
Combining inequalities 6, 7 and 8 we get:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤17ğ›¿2ğ‘šğœ†min
8(ğ»âˆ’â„)2ğ‘…2maxminn17
8ğ‘2ğµ,ğ‘†ğ¸ğ¶
2o
+
+ğ¶ğ‘šâ„ 
2 exp(
âˆ’2
ğ‘âˆ’ğ›¿ğ¸âˆšğ‘†ğœ†min
2(ğ»âˆ’â„)ğ‘…max2
ğ¸)!
+2ğ¶ğ‘šexp(
âˆ’2
ğ‘âˆ’ğ›¿ğ¸âˆšğ‘†ğœ†min
2(ğ»âˆ’â„)ğ‘…max2
ğ¸)
We can now set:
ğ›¿2
ğ´â‰¤1
1008ğ¶(ğ»âˆ’â„)2ğ‘…2
max
17ğ‘šğœ†minğ‘†ğ¸min{17
4ğµğ‘2
ğ¸,ğ¶
2}=1
1008ğ¶(ğ»âˆ’â„)2ğ‘…2
max
17ğ‘šğœ†minğ‘†ğ¸min{17
4ğµğ‘2
ğ¶ğ¸,1
2}
ğ›¿2
ğµâ‰¤8ğ‘…2
max(ğ»âˆ’â„)2
17ğ¸ğ‘†ğœ† minlogğ‘š
ğ›¿=min{ğ›¿ğ´,ğ›¿ğµ}
ğ‘=100âˆšï¸
ğ¸log 100ğ‘š
We see that under such choice we haveğ‘£ğ‘˜ğ›¿âˆšğ‘†ğœ†min
ğ‘…max(ğ»âˆ’â„)â‰¤1
4andğ‘ > ğœ‡ğ‘˜. We now have that:
17ğ›¿2
ğ´ğ‘šğœ†min
8(ğ»âˆ’â„)2ğ‘…2maxminn17
8ğ‘2ğµ,ğ‘†ğ¸ğ¶
2o
â‰¤0.01ğ¶
Sinceğ‘ > ğœ‡ğ‘˜we have that:
(ğ‘âˆ’ğ›¿ğµğ¸âˆšğ‘†ğœ†min
2(ğ»âˆ’â„)ğ‘…max)2â‰¥(100âˆšï¸
ğ¸log 100ğ‘šâˆ’âˆš
ğ¸
logğ‘š)2
=100ğ¸(1âˆ’1
logğ‘š)2log 100ğ‘šâ‰¥ğ¸log 100ğ‘š
We can use this to bound the third term as follows:
2ğ‘šğ¶exp(
âˆ’2
ğ‘âˆ’ğ›¿ğµâˆšğ‘†ğœ†min
2(ğ»âˆ’â„)ğ‘…max2
ğ¸)
â‰¤2ğ‘šğ¶exp{âˆ’2ğ¸log 100ğ‘š
ğ¸}=2ğ‘šğ¶exp{âˆ’2 log 100ğ‘š}â‰¤0.0002ğ¶
ğ‘š
For the second term we proceed similarly by observing that for binary entropy function we have â„(ğ‘)â‰¤6/5âˆšğ‘
forğ‘ >0, which gives:
ğ‘šğ¶â„ 
2 exp(
âˆ’2
ğ‘âˆ’ğ›¿ğµâˆšğ‘†ğœ†min
2(ğ»âˆ’â„)ğ‘…max2
ğ¸)!
â‰¤ğ‘šğ¶â„(0.0002
ğ‘š2)â‰¤6
5ğ¶âˆš
0.0002â‰¤0.02ğ¶
Hence we get that:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤0.01ğ¶+0.0002ğ¶
ğ‘š+0.02ğ¶â‰¤0.04ğ¶â‰¤0.10ğ¶
We can now obtain an inequality similar to the one in Equation 5 and proceed as in the proof of Theorem
3.3 mutatis mutandis, which completes the proof.
17Under review as submission to TMLR
D Proof of Theorem 5.2
Theorem 5.2 In a distributed linear value function approximation problem with context dimensionality of
ğ¶ > 12, state space size of ğ‘†â‰¤ğ¶and discount factor of 0< ğ›¾ < 0.99, under assumptions 3.1, 4.1 and
5.1, givenğ‘šprocessing centres each with gameplay history, with each centre having a communication budget
ğµâ‰¥1, the minimax risk ğ‘€of any algorithm is lower bounded as:
ğ‘€â‰¥Î© 
ğ¶ğ‘…2
max
(1âˆ’ğ›¾)2ğœ‹maxğ‘†ğ‘›ğ‘šğœ† minmin(
maxnğ¶logğ‘š
ğµ,1o
,ğ‘š
logğ‘š)!
Proof D.1 We consider the following MDP: from each state ğ‘–, regardless of the agentâ€™s action it either
states in the same state with probability 1âˆ’ğ‘and receives a reward ğ‘Ÿğ‘—with a mean of Â¯ğ‘Ÿğ‘—or moves to any
other states chosen with a probability ofğ‘
ğ‘†âˆ’1and receives a reward ğ‘Ÿ0. We see that because of the consistency
equation for value functions we have:
ğ‘£(ğ‘–)=(1âˆ’ğ‘)Â¯ğ‘Ÿğ‘–+ğ‘ğ‘Ÿ0+ğ›¾âˆ‘ï¸
ğ‘ â€²âˆˆğ‘†{ğ‘–}ğ‘£(ğ‘ â€²)ğ‘
ğ‘†âˆ’1+ğ›¾ğ‘£(ğ‘–)(1âˆ’ğ‘)
Similarily as in previous proofs, we set the ğ‘˜-th element of context vector for the ğ‘–th state asğ‘ğ‘–,ğ‘˜=âˆšğœ†minğ‘†ğŸ™ğ‘˜=ğ‘–
so that Assumption 3.2 is satisfied. We also set the ğ‘˜th element of parameter vector to be ğœƒğ‘˜=ğ›¿ğ‘£ğ‘˜, where
ğ‘£ğ‘˜is sampled uniformly from {âˆ’1,1}. We now set ğ‘Ÿ0=âˆ’ğ›¾Ã
ğ‘ â€²âˆˆğ‘†{ğ‘–}ğ‘£(ğ‘ â€²)
ğ‘†âˆ’1to get that:
ğ‘£(ğ‘–)=(1âˆ’ğ‘)Â¯ğ‘Ÿğ‘–+ğ›¾ğ‘£(ğ‘–)(1âˆ’ğ‘)
Â¯ğ‘Ÿğ‘–=ğ‘£(ğ‘–)(1âˆ’ğ›¾+ğ‘ğ›¾)
1âˆ’ğ‘
For statesğ‘– >ğ¶we can just deterministicaly set ğ‘Ÿğ‘–=0, for other states we get that:
Â¯ğ‘Ÿğ‘–=âˆšğœ†minğ‘†(1âˆ’ğ›¾+ğ‘ğ›¾)
1âˆ’ğ‘
We now set ğ‘Ÿğ‘–=1
99(2ğ‘ğ‘–âˆ’1)ğ‘…maxwhereğ‘ğ‘–âˆ¼Bernoulli(1
2+99âˆšğœ†minğ‘†ğ‘£ğ‘˜ğ›¿
2ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘). We see that under this
construction expected return from each state is cğ‘‡Î¸and|ğ‘Ÿğ‘–| â‰¤ğ‘…max. Since under this construction the
maximum value of the value function for any state ğ‘ isğ‘£(ğ‘ )â‰¤0.99ğ‘…max
1âˆ’ğ›¾, we also have that |ğ‘Ÿ0|â‰¤ğ›¾0.99ğ‘…max
1âˆ’ğ›¾â‰¤
ğ‘…max. We thus have a similar situation as in the proof of Theorem 4.3, where the data received can be reduced
to outcomes of Bernoulli trials. Same as in that proof, we can show that the likelihood ratio given ğ‘£ğ‘˜=âˆ’ğ‘£â€²
ğ‘˜
is bounded by expn
17âˆšğ‘†ğœ†minğ›¿ğ‘£ğ‘˜
8ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘Ãğ‘ğ‘˜
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜o
, whereğ‘ğ‘˜is the number of visits to state ğ‘˜in gameplay
history. We notice that the mean ofÃğ‘ğ‘˜
ğ‘™=1ğ‘ğ‘™
ğ‘˜âˆ’ğ‘›ğ‘™
ğ‘˜isğœ‡ğ‘˜=99ğ›¿ğœ‹(ğ‘˜)ğ‘›ğ‘£ğ‘˜âˆšğ‘†ğœ†min
ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘and we can derive a similar
Hoeffding bound as in Equation 7 with ğ‘›variables confined to [0,1], where ifğ‘ğ‘˜< ğ‘›for the last ğ‘ğ‘˜âˆ’ğ‘›
variables we set ğ‘ğ‘™
ğ‘˜=ğ‘›ğ‘™
ğ‘˜=0. This gives us the following inequality:
ğ‘ƒ(ğ¸ğ‘˜=0)â‰¤2 exp(
âˆ’2
ğ‘âˆ’ğ›¿99ğ‘›ğœ‹(ğ‘˜)âˆšğ‘†ğœ†min
ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ‘›)
We can also develop a bound on KL divergence. We denote by ğ‘ƒğ‘˜allğ‘ğ‘™
ğ‘˜forğ‘™=1,...,ğ‘ğ‘˜and note that
through convexity of KL divergence we get:
KL[ğ‘(ğ‘ƒğ‘˜|ğ‘£),ğ‘(ğ‘ƒğ‘˜|ğ‘£âˆ—)]â‰¤ğ‘›âˆ‘ï¸
ğ‘ğ‘˜=1KL[ğ‘(ğ‘ƒğ‘˜|ğ‘£,ğ‘ğ‘˜),ğ‘(ğ‘ƒğ‘˜|ğ‘£âˆ—,ğ‘ğ‘˜)]ğ‘(ğ‘ğ‘˜)
Since distribution of ğ‘ƒğ‘˜is Binomial( ğ‘›,ğœ‹(ğ‘˜)) we get:
KL[ğ‘(ğ‘ƒğ‘˜|ğ‘£),ğ‘(ğ‘ƒğ‘˜|ğ‘£âˆ—)]â‰¤ğ‘›âˆ‘ï¸
ğ‘ğ‘˜=1ğ‘(ğ‘ğ‘˜)log1
2+ğ‘
1
2âˆ’ğ‘
ğ‘ğ‘˜ğ‘
18Under review as submission to TMLR
Where we used ğ‘=99âˆšğœ†minğ‘†ğ‘£ğ‘˜ğ›¿
2ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘. Same as before we use the fact that for ğ‘¥ <1
4we have that1+ğ‘¥
1âˆ’ğ‘¥â‰¤
exp{17
8ğ‘¥}:
KL[ğ‘(ğ‘ƒğ‘˜|ğ‘£),ğ‘(ğ‘ƒğ‘˜|ğ‘£âˆ—)]â‰¤17
8ğ‘2ğ‘›âˆ‘ï¸
ğ‘ğ‘˜=1ğ‘(ğ‘ğ‘˜)ğ‘ğ‘˜=17
8ğ‘2ğ‘›ğœ‹(ğ‘˜)â‰¤17
8ğ‘2ğ‘›ğœ‹max
From Lemma A.2 with ğ‘‘=ğ¶andğ‘…ğ‘˜=ğ‘ƒğ‘˜we get:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤ğ¶ğ‘š
217
899âˆšğœ†minğ‘†ğ‘£ğ‘˜ğ›¿
2ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ‘›ğœ‹max
Proceeding as in the proof of Theorem 4.3 mutandits mutatis we get that:
ğ¼(ğ‘Œ,ğ‘‰)â‰¤17ğ›¿2ğ‘šğ‘†ğœ† min
8ğ‘…2max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
minn17
8ğ‘2ğµ,ğ¶ğ‘›ğœ‹ max
2o
+
+ğ¶ğ‘šâ„ 
2 exp(
âˆ’2
ğ‘âˆ’ğ›¿ğ‘›ğœ‹(ğ‘˜)âˆšğ‘†ğœ†min
ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ‘›)!
+ğ¶ğ‘š2 exp(
âˆ’2
ğ‘âˆ’ğ›¿ğ‘›ğœ‹(ğ‘˜)âˆšğ‘†ğœ†min
ğ‘…max1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ‘›)
Hence we can set:
ğ‘âˆâˆšï¸
ğ‘›ğœ‹maxlogğ‘š
ğ›¿2
ğ‘âˆğ¶ğ‘…2
max
ğ‘šğœ†minğ‘†min{ğµğ‘2,ğ¶ğ‘›ğœ‹ max}1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ›¿2
ğ‘âˆğ‘…2
max
ğ‘›ğ‘†ğœ‹ maxğœ†minlogğ‘š1âˆ’ğ›¾+ğ‘ğ›¾
1âˆ’ğ‘2
ğ›¿=min{ğ›¿ğ‘,ğ›¿ğ‘}
Proceeding similarly as in the proof of Theorem 4.3 we get:
ğ‘€â‰¥Î© 
ğ¶ğ‘…2
max(1âˆ’ğ‘)2
(1âˆ’ğ›¾+ğ‘ğ›¾)2ğ‘†ğœ‹maxğ‘›ğ‘šğœ† minmin(
maxnğ¶logğ‘š
ğµ,1o
,ğ‘š
logğ‘š)!
We observe that the bound gets tighter as ğ‘â†’0. We can now index distributions for ğ‘Ÿğ‘–withğ‘and observe
that since the bound is defined as a supremum over the set of considered distributions, we get the statement
of the Theorem.
E Proof of Theorem 3.5
Theorem 3.5 Let us define Î¸=(Î¸1,...,Î¸ğ´)to be the concatenated parameter vector for all actions. For
any value ofÎ¸, Algorithm 1 using transmission with precision ğ‘ƒachieves a worst-case risk upper bounded as
follows:
ğ‘Š <O
ğ´ğ¶maxnğ‘…2
max
ğ‘šğ‘›ğœ† min,ğ‘ƒo
Proof E.1 We start by assuming the transmission is lossless (i.e. the number of bits is infinite) and then
study how the MSE changes when transmission introduces quantisation error. Using the bias variance de-
composition, we get:
MSE(Ë†Î¸)=âˆ‘ï¸
ğ‘âˆˆğ´ğ¶âˆ‘ï¸
ğ‘˜=1bias(Ë†ğœƒğ‘,ğ‘˜)2+âˆ‘ï¸
ğ‘âˆˆğ´ğ¶âˆ‘ï¸
ğ‘˜=1Var(Ë†ğœƒğ‘,ğ‘˜)
It is a well-known fact that for least-squares estimate, the bias is zero, hence bias (Ë†ğœƒğ‘–
ğ‘,ğ‘˜)=0and their average
Ë†ğœƒğ‘,ğ‘˜is also unbiased. Because of the properties of the variance of average values we get:
MSE(Ë†Î¸)=1
ğ‘šâˆ‘ï¸
ğ‘âˆˆğ´ğ¶âˆ‘ï¸
ğ‘˜=1Var(Ë†ğœƒ1
ğ‘,ğ‘˜)
19Under review as submission to TMLR
Using eigendecomposition and singular value decomposition we get:
ğ‘‹ğ‘‡
ğ‘ğ‘‹ğ‘=ğ‘„ğ‘‡Î›ğ‘„
ğ‘‹ğ‘‡
ğ‘=ğ‘„ğ‘‡Î£ğ‘‰ğ‘‡
Where we choose such ğ‘„andğ‘‰such that all left and right singular vectors have a norm of 1. Since ğ‘„is
orthogonal, it follows that:
Ë†Î¸ğ‘–
ğ‘=(ğ‘„ğ‘‡Î›ğ‘„)âˆ’1ğ‘„ğ‘‡Î£ğ‘‰ğ‘‡=ğ‘„ğ‘‡Î›âˆ’1ğ‘„ğ‘„ğ‘‡Î£ğ‘‰ğ‘‡rğ‘=ğ‘„ğ‘‡Î›âˆ’1Î£ğ‘‰ğ‘‡rğ‘
Writing this equation in terms of matrix elements gives:
Ë†ğœƒğ‘–
ğ‘,ğ‘˜=ğ¶âˆ‘ï¸
ğ‘—=1ğ‘„ğ‘—,ğ‘˜âˆšï¸ğœ†ğ‘—
ğœ†ğ‘—ğ‘›âˆ‘ï¸
ğ‘™=1ğ‘Ÿğ‘™
ğ‘ğ‘‰ğ‘—,ğ‘™=ğ¶âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1ğ‘„ğ‘—,ğ‘˜1âˆšï¸ğœ†ğ‘—ğ‘Ÿğ‘™
ğ‘ğ‘‰ğ‘—,ğ‘™â‰¤ğ¶âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1ğ‘„ğ‘—,ğ‘˜1âˆšğœ†minğ‘›ğ‘Ÿğ‘™
ğ‘ğ‘‰ğ‘—,ğ‘™
Whereğœ†ğ‘–are the eigenvalues. The last inequality follows from Assumption 3.2. We can now easily bound
the variance:
Var(Ë†ğœƒğ‘–
ğ‘,ğ‘˜)â‰¤ğ¶âˆ‘ï¸
ğ‘—=1ğ‘›âˆ‘ï¸
ğ‘™=1ğ‘„2
ğ‘—,ğ‘˜ğ‘‰2
ğ‘—,ğ‘™
ğœ†minğ‘›Var(ğ‘Ÿğ‘™
ğ‘)
We now observe that due to Popoviciu inequality and Assumption 3.1 we get: Var (ğ‘Ÿğ‘™
ğ‘)â‰¤ğ‘…2
max. This gives
us:
Var(Ë†ğœƒğ‘–
ğ‘,ğ‘˜)â‰¤ğ‘…2
max
ğœ†minğ‘›ğ¶âˆ‘ï¸
ğ‘—=1ğ‘„2
ğ‘—,ğ‘˜ğ‘›âˆ‘ï¸
ğ‘™=1ğ‘‰2
ğ‘—,ğ‘™=ğ‘…2
max
ğœ†minğ‘›
Where the last equality is due to singular vectors having norm of 1. Substituting this back into the equation
for MSE, we get:
ğ‘€ğ‘†ğ¸(Ë†Î¸)â‰¤ğ´ğ¶ğ‘…2
max
ğ‘šğ‘›ğœ† min
Using the fact that each component is quantised up to precision ğ‘ƒ, the max error introduced by quantisation
isğ´ğ¶ğ‘ƒ, hence:
ğ‘€ğ‘†ğ¸(Ë†Î¸)â‰¤O
ğ´ğ¶maxnğ‘…2
max
ğ‘šğ‘›ğœ† min,ğ‘ƒo
F Proof of Theorem 5.4
Theorem 5.4 The worst-case risk of Algorithm 3 run with a learning rate of ğ›¼ğ‘¡=ğ›½
Î›+ğ‘¡
ğœ”withğ›½=2
(1âˆ’ğ›¾)ğœ”
andÎ›=16
(1âˆ’ğ›¾)2ğœ”is upper bounded as follows:
ğ‘Šâ‰¤O 
max(max{ğ‘…2
max
ğ‘†ğœ‹minğœ†minğ‘š,âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2}
1+(1âˆ’ğ›¾)2ğ‘›,ğ¶ğ‘ƒ)!
Proof F.1 We define Â¯Î¸ğ‘¡:=1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
ğ‘¡to be the average vector from all machines at timestep ğ‘¡. Note that
this vector is never actually created, except for the last step when we average all final results. We define ğœ”
to be the smallest eigenvalue of the covariance matrix weighted by the stationary distribution, i.e. smallest
eigenvalue ofÃ
ğ‘ âˆˆğ‘†ğœ‹(ğ‘ )cğ‘ cğ‘‡
ğ‘ . By convexity of eigenvalues we have ğ‘†ğœ‹minğœ†minâ‰¤ğœ”â‰¤ğœ†minâ‰¤1. We can see
that the averaged parameter vector must satisfy the following recursive relation:
Â¯Î¸ğ‘¡+1=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1Ë†Î¸ğ‘–
ğ‘¡+1=1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1[Ë†Î¸ğ‘–
ğ‘¡+ğ›¼ğ‘¡gğ‘–
ğ‘¡]=Â¯Î¸ğ‘¡+ğ›¼ğ‘¡1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1gğ‘–
ğ‘¡
Hence we can write:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]=ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2]âˆ’2ğ›¼ğ‘¡
ğ‘šğ”¼hğ‘šâˆ‘ï¸
ğ‘–=1(Î¸âˆ’Â¯Î¸ğ‘–
ğ‘¡)ğ‘‡gğ‘–
ğ‘¡i
+ğ›¼2
ğ‘¡ğ”¼[1
ğ‘š2âˆ¥ğ‘šâˆ‘ï¸
ğ‘–=1gğ‘–
ğ‘¡âˆ¥2
2]
20Under review as submission to TMLR
We now focus on the second term:
2ğ›¼ğ‘¡
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1ğ”¼h
(Î¸âˆ’Â¯Î¸ğ‘–
ğ‘¡)ğ‘‡gğ‘–
ğ‘¡i
=2ğ›¼ğ‘¡
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1ğ”¼h
(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)ğ‘‡ğ”¼h
gğ‘–
ğ‘¡Ë†Î¸ğ‘–
ğ‘¡ii
using Lemma H.1 we have that
2ğ›¼ğ‘¡
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1ğ”¼h
(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)ğ‘‡ğ”¼h
gğ‘–
ğ‘¡Ë†Î¸ğ‘–
ğ‘¡ii
â‰¥2ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)
ğ‘š2ğ”¼hğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1âˆ¥Î¸âˆ’Î¸ğ‘–
ğ‘¡âˆ¥2âˆ¥Î¸âˆ’Î¸ğ‘—
ğ‘¡âˆ¥2i
We now decompose the third term, while using the notation Ë†Î¸ğ‘¡=(Ë†Î¸1
ğ‘¡,..., Ë†Î¸ğ‘š
ğ‘¡):
ğ”¼[âˆ¥1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1gğ‘–
ğ‘¡âˆ¥2
2]=ğ”¼[ğ”¼[âˆ¥1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1gğ‘–
ğ‘¡âˆ¥2
2|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+]]
=ğ”¼[1
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1ğ”¼[gğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+]ğ‘‡ğ”¼[gğ‘—
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+]+Tr(1
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1Var(ğ‘”ğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+))]
The expected value can be upper-bounded as follows:
ğ”¼[gğ‘–
ğ‘¡]=ğ”¼[(ğ‘Ÿğ‘–
ğ‘¡+ğ›¾(cğ‘–
ğ‘¡+)ğ‘‡Î¸ğ‘–
ğ‘¡âˆ’(cğ‘–
ğ‘¡)ğ‘‡Î¸ğ‘–
ğ‘¡)cğ‘–
ğ‘¡]=
=ğ”¼[(ğ‘Ÿğ‘–
ğ‘¡+ğ›¾(cğ‘–
ğ‘¡+)ğ‘‡Î¸âˆ’cğ‘–
ğ‘¡Î¸+ğ›¾(cğ‘–
ğ‘¡+)ğ‘‡(Î¸ğ‘–
ğ‘¡âˆ’Î¸)âˆ’(cğ‘–
ğ‘¡)ğ‘‡(Î¸ğ‘–
ğ‘¡âˆ’Î¸))cğ‘–
ğ‘¡]=
=(ğ”¼[ğ‘Ÿğ‘–
ğ‘¡]+ğ›¾Î¸ğ‘‡cğ‘–
ğ‘¡+âˆ’Î¸ğ‘‡cğ‘–
ğ‘¡)cğ‘–
ğ‘¡+(ğ›¾cğ‘–
ğ‘¡+âˆ’cğ‘–
ğ‘¡)ğ‘‡(Î¸ğ‘–
ğ‘¡âˆ’Î¸)cğ‘–
ğ‘¡
By the definition of value function we have that ğ”¼[ğ‘Ÿğ‘–
ğ‘¡]=Î¸ğ‘‡cğ‘–
ğ‘¡âˆ’ğ›¾Î¸ğ‘‡cğ‘–
ğ‘¡+. Hence the expression above simplifies
to(ğ›¾cğ‘–
ğ‘¡+âˆ’cğ‘–
ğ‘¡)ğ‘‡(Î¸ğ‘–
ğ‘¡âˆ’Î¸)cğ‘–
ğ‘¡. We thus have:
ğ”¼[ğ‘”ğ‘–
ğ‘¡]ğ‘‡ğ”¼[ğ‘”ğ‘—
ğ‘¡]=(ğ›¾cğ‘–
ğ‘¡+âˆ’cğ‘–
ğ‘¡)ğ‘‡(Î¸ğ‘–
ğ‘¡âˆ’Î¸)(cğ‘–
ğ‘¡)ğ‘‡cğ‘—
ğ‘¡(ğ›¾cğ‘—
ğ‘¡+âˆ’cğ‘—
ğ‘¡)ğ‘‡(Î¸ğ‘—
ğ‘¡âˆ’Î¸)â‰¤
â‰¤âˆ¥ğ›¾cğ‘–
ğ‘¡+âˆ’cğ‘–
ğ‘¡âˆ¥2âˆ¥Î¸ğ‘–
ğ‘¡âˆ’Î¸âˆ¥2âˆ¥cğ‘–
ğ‘¡âˆ¥2âˆ¥cğ‘—
ğ‘¡âˆ¥2âˆ¥ğ›¾cğ‘—
ğ‘¡+âˆ’cğ‘—
ğ‘¡âˆ¥2âˆ¥Î¸ğ‘—
ğ‘¡âˆ’Î¸âˆ¥â‰¤âˆ¥Î¸ğ‘–
ğ‘¡âˆ’Î¸âˆ¥2âˆ¥Î¸ğ‘—
ğ‘¡âˆ’Î¸âˆ¥2
Finally, we also bound the variance:
Var(ğ‘”ğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+)=Var((ğ‘Ÿğ‘–
ğ‘¡+ğ›¾(Î¸ğ‘–
ğ‘¡)ğ‘‡cğ‘–
ğ‘¡+âˆ’(Î¸ğ‘–
ğ‘¡)ğ‘‡cğ‘–
ğ‘¡)cğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+)=Var(ğ‘Ÿğ‘–
ğ‘¡cğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+)â‰¤
â‰¤cğ‘–
ğ‘¡Var(ğ‘Ÿğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+)(cğ‘–
ğ‘¡)ğ‘‡â‰¤cğ‘–
ğ‘¡(cğ‘–
ğ‘¡)ğ‘‡ğ‘…2
max
Hence we have:
Tr(1
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1Var(ğ‘”ğ‘–
ğ‘¡|Ë†Î¸ğ‘¡,cğ‘–
ğ‘¡,cğ‘–
ğ‘¡+))â‰¤ğ‘…2
max
ğ‘š
Hence we obtain a recursive inequality:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]â‰¤ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2]âˆ’
âˆ’ 
2ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)âˆ’ğ›¼2
ğ‘¡
ğ‘š2!
ğ”¼hğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1âˆ¥Î¸âˆ’Î¸ğ‘–
ğ‘¡âˆ¥2âˆ¥Î¸âˆ’Î¸ğ‘—
ğ‘¡âˆ¥2i
+ğ›¼2
ğ‘¡
ğ‘šğ‘…2
max
We can now use a Lemma H.3 to get:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]â‰¤ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2](1âˆ’2ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)+ğ›¼2
ğ‘¡)+ğ›¼2
ğ‘¡
ğ‘šğ‘…2
max
We now set ğ›¼ğ‘¡=ğ›½
Î›+ğ‘¡
ğœ”withğ›½=2
(1âˆ’ğ›¾)ğœ”andÎ›=16
(1âˆ’ğ›¾)2ğœ”. We observe that since ğ›¼ğ‘¡â‰¤(1âˆ’ğ›¾)ğœ”we have that
âˆ’2ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)+ğ›¼2
ğ‘¡ğ‘…2
max=ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)(âˆ’2+ğ›¼ğ‘¡ğ‘…2
max
ğœ”(1âˆ’ğ›¾))â‰¤ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾)(âˆ’2+1)=âˆ’ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾). Let us now define
21Under review as submission to TMLR
ğœˆ=max{ğ›½2ğ‘…2
max
ğ‘š,Î›âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2}and observe that we have ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸0âˆ¥2
2]â‰¤ğœˆ
Î›. Proceeding by induction,
let us suppose that ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2]â‰¤ğœˆ
Ë†ğ‘¡, where Ë†ğ‘¡=ğœ”ğ‘¡+Î›. We then have:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]â‰¤ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2](1âˆ’ğ›¼ğ‘¡ğœ”(1âˆ’ğ›¾))+ğ›¼2
ğ‘¡ğ‘…2
max
ğ‘š
â‰¤ğœˆ
Ë†ğ‘¡(1âˆ’ğ›½ğœ”(1âˆ’ğ›¾)
Ë†ğ‘¡)+ğ‘…2
maxğ›½2
Ë†ğ‘¡2ğ‘š=Ë†ğ‘¡âˆ’1
Ë†ğ‘¡2ğœˆ+ğ‘…2
maxğ›½2
ğ‘šâˆ’((1âˆ’ğ›¾)ğœ”ğ›½âˆ’1)ğœˆ
Ë†ğ‘¡2
=Ë†ğ‘¡âˆ’1
Ë†ğ‘¡2ğœˆ+ğ‘…2
maxğ›½2
ğ‘šâˆ’ğœˆ
Ë†ğ‘¡2
We now observe that by definition of ğœˆwe haveğ‘…2
maxğ›½2
ğ‘šâ‰¤ğœˆand that Ë†ğ‘¡2â‰¥(Ë†ğ‘¡âˆ’1)(Ë†ğ‘¡+1). Thus we have:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡+1âˆ¥2
2]â‰¤ğœˆ
Ë†ğ‘¡+1â‰¤ğœˆ
Ë†ğ‘¡+1
ğœ”
By induction this proves that:
ğ”¼[âˆ¥Î¸âˆ’Â¯Î¸âˆ¥2
2]â‰¤O 
max(
max{ğ‘…2
max
ğœ”ğ‘š,âˆ¥Î¸âˆ’1
ğ‘šÃğ‘š
ğ‘–=1Ë†Î¸ğ‘–
0âˆ¥2
2}
1+(1âˆ’ğ›¾)2ğ‘›,ğ¶ğ‘ƒ)!
We now use the fact that ğœ”â‰¤ğ‘†ğœ‹minğœ†minto get the statement of the Theorem.
G Analysis of worst-case TD initial bias
Given the average initial parameter value Ë†Î¸0, assume we are faced with a problem where the features of states
are chosen so that Ë†Î¸0is an eigenvector of the ğ‘‹ğ‘‡ğ‘‹matrix with eigenvalue ğœ†min. We see that if we form
vectoruconsisting of value function for each state we get âˆ¥ğ‘¢âˆ¥2
2â‰¤O(ğ‘†ğ‘…2
max
(1âˆ’ğ›¾)2). We choose the true parameter
vectorÎ¸to be parallel to Ë†Î¸0, henceâˆ¥ğ‘‹ğ‘‡Î¸âˆ¥2
2â‰¤ğœ†minâˆ¥Î¸âˆ¥2
2â‰¤O(ğ‘†ğ‘…2
max
(1âˆ’ğ›¾)2). We thus haveâˆ¥Î¸âˆ¥2
2â‰¤O(ğ‘†ğ‘…2
max
(1âˆ’ğ›¾)2ğœ†min).
We can now set Î¸=âˆ’Ë†Î¸0and observe that we have âˆ¥Î¸âˆ’Ë†Î¸0âˆ¥2
2=4âˆ¥Î¸âˆ¥2
2â‰¤O(ğ‘†ğ‘…2
max
(1âˆ’ğ›¾)2).
H Lemmas for TD learning
Within this appendix, we propose Lemma H.1, which is a more general version of Lemma 1 and 3 from
Bhandari et al. (2018). We also propose Lemma H.3, which is a simple consequence of the triangle inequality.
Both of these Lemmas consistute very useful tools for our analysis of distributed TD learning.
Lemma H.1 For TD value function approximation problem in a MDP with discount factor ğ›¾, withğœ”being
the smallest eigenvalue of the feature matrix weighted by the stationary distribution, for ğ‘”ğ‘–
ğ‘¡=(ğ‘Ÿğ‘¡+ğ›¾cğ‘‡
ğ‘¡+Ë†Î¸ğ‘–âˆ’
cğ‘‡
ğ‘¡Ë†Î¸ğ‘–)cğ‘¡being the gradient step, we have that:
(Î¸âˆ’Î¸ğ‘–
ğ‘¡)ğ‘‡ğ”¼[gğ‘—
ğ‘¡|Ë†Î¸ğ‘—
ğ‘¡]â‰¥ğœ”(1âˆ’ğ›¾)âˆ¥Î¸âˆ’Î¸ğ‘–
ğ‘¡âˆ¥2âˆ¥Î¸âˆ’Î¸ğ‘—
ğ‘¡âˆ¥2
Proof H.2 Let us define ğœ‰ğ‘–=(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)cğ‘–
ğ‘¡,ğœ‰â€²
ğ‘–=(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)cğ‘–
ğ‘¡+andğœ‰ğ‘–,ğ‘—=(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)cğ‘—
ğ‘¡. Observing that the expected
gradient is zero at the optimal value of Î¸we get:
ğ”¼[gğ‘—
ğ‘¡|Ë†Î¸ğ‘—
ğ‘¡]=ğ”¼[gğ‘—
ğ‘¡|Ë†Î¸ğ‘—
ğ‘¡]âˆ’ğ”¼[(ğ‘Ÿğ‘—
ğ‘¡+ğ›¾Î¸ğ‘‡cğ‘—
ğ‘¡+âˆ’Î¸ğ‘‡cğ‘—
ğ‘¡)cğ‘¡|Ë†Î¸ğ‘—
ğ‘¡]=ğ”¼[cğ‘—
ğ‘¡(ğ›¾cğ‘—
ğ‘¡+âˆ’cğ‘—
ğ‘¡)(Ë†Î¸ğ‘—
ğ‘¡âˆ’Î¸)|Ë†Î¸ğ‘—
ğ‘¡]=
=ğ”¼[cğ‘—
ğ‘¡(cğ‘—
ğ‘¡âˆ’ğ›¾cğ‘—
ğ‘¡+)|Ë†Î¸ğ‘—
ğ‘¡]
Hence we get:
(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)ğ‘‡ğ”¼[gğ‘—
ğ‘¡|Ë†Î¸ğ‘—
ğ‘¡]=ğ”¼[ğœ‰ğ‘–,ğ‘—(ğœ‰ğ‘—âˆ’ğ›¾ğœ‰â€²
ğ‘—)|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]=ğ”¼[ğœ‰ğ‘–,ğ‘—ğœ‰ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]âˆ’ğ›¾ğ”¼[ğœ‰ğ‘–,ğ‘—ğœ‰â€²
ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]
22Under review as submission to TMLR
Sincecğ‘—
ğ‘¡andcğ‘–
ğ‘¡are independent we have that given estimate of parameter vectors ğœ‰ğ‘–,ğ‘—andğœ‰ğ‘—are also
independent. Moreover by Cauchy-Schwarz we have that
ğ”¼[ğœ‰ğ‘–,ğ‘—ğœ‰â€²
ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]â‰¤âˆšï¸ƒ
ğ”¼[ğœ‰2
ğ‘–,ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]ğ”¼[(ğœ‰â€²
ğ‘—)2|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]
. We now observe that:
ğ”¼[ğœ‰ğ‘–,ğ‘—ğœ‰ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]=âˆ‘ï¸
ğ‘ âˆˆğ‘†ğœ‹(ğ‘ )(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)ğ‘‡cğ‘–
ğ‘¡(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)ğ‘‡cğ‘–
ğ‘¡
=(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)ğ‘‡Î£(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)â‰¤(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)ğ‘‡(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)ğœ”â‰¤ğœ”âˆ¥Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡âˆ¥2âˆ¥Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡âˆ¥2
Moreover by similar argument we can show that: ğ”¼[ğœ‰2
ğ‘–,ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]â‰¤ğœ”âˆ¥Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡âˆ¥2
2andğ”¼[ğœ‰2
ğ‘—|Ë†Î¸ğ‘—
ğ‘¡,Ë†Î¸ğ‘–
ğ‘¡]â‰¤ğœ”âˆ¥Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡âˆ¥2
2
which complete the proof.
Lemma H.3âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2â‰¤1
ğ‘š2Ãğ‘š
ğ‘–=1âˆ¥Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡âˆ¥2Ãğ‘š
ğ‘—=1âˆ¥Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡âˆ¥2
Proof H.4
âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2=âˆ¥Î¸âˆ’1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1Ë†Î¸ğ‘–
ğ‘¡âˆ¥2
2=
âˆ¥1
ğ‘šğ‘šâˆ‘ï¸
ğ‘–=1(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)âˆ¥22
=1
ğ‘š2
âˆ¥ğ‘šâˆ‘ï¸
ğ‘–=1(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)âˆ¥22
By triangle inequality we have:
âˆ¥Î¸âˆ’Â¯Î¸ğ‘¡âˆ¥2
2â‰¤1
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1âˆ¥(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)âˆ¥22
=1
ğ‘š2ğ‘šâˆ‘ï¸
ğ‘–=1ğ‘šâˆ‘ï¸
ğ‘—=1âˆ¥(Î¸âˆ’Ë†Î¸ğ‘–
ğ‘¡)âˆ¥2âˆ¥(Î¸âˆ’Ë†Î¸ğ‘—
ğ‘¡)âˆ¥2
23