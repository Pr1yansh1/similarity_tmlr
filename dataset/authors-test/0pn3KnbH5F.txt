Under review as submission to TMLR
Learning-to-defer for sequential medical decision-making un-
der uncertainty
Anonymous authors
Paper under double-blind review
Abstract
Learning-to-defer is a framework to automatically defer decision-making to a human expert
when ML-based decisions are deemed unreliable. Existing learning-to-defer frameworks are
not designed for sequential settings. That is, they defer at every instance independently,
based on immediate predictions, while ignoring the potential long-term impact of these
interventions. As a result, existing frameworks are myopic. Further, they do not defer
adaptively, which is crucial when human interventions are costly. In this work, we propose
Sequential Learning-to-Defer (SLTD), a framework for learning-to-defer to a domain expert
in sequential decision-making settings. Contrary to existing literature, we pose the problem
of learning-to-defer as model-based reinforcement learning (RL) to i) account for long-term
consequences of ML-based actions using RL and ii) adaptively defer based on the dynamics
(model-based). Our proposed framework determines whether to defer (at each time step) by
quantifying whether a deferral now will improve the value compared to delaying deferral to
the next time step. To quantify the improvement, we account for potential future deferrals.
As a result, we learn a pre-emptive deferral policy (i.e. a policy that defers early if using the
ML-based policy could worsen long-term outcomes). Our deferral policy is adaptive to the
non-stationarity in the dynamics. We demonstrate that adaptive deferral via SLTD provides
an improved trade-oï¬€ between long-term outcomes and deferral frequency on synthetic,
semi-synthetic, and real-world data with non-stationary dynamics. Finally, we interpret
the deferral decision by decomposing the propagated (long-term) uncertainty around the
outcome, to justify the deferral decision.
1 Introduction
Machine learning (ML) has the potential to be deployed for decision-making in complex domains such as
healthcare, lending, and legal systems. In many cases, ML-based policy may not generalize to situations not
encountered during training. In practice, it may be safer to defer to a human expert when using the ML
policy may not improve outcomes or cause active harm. Automatically deferring to a human expert is called
â€˜Learning-to-defer.â€™ Earlier works have considered the problem of learning-to-defer in non-sequential settings
(Mozannar and Sontag, 2020; Madras et al., 2017)..
In situations such as managing health, however, two key challenges remain. First, deferral decisions can
signiï¬cantly alter long-term outcomes. Thus modeling the long-term outcome is critical to decide whento
defer to an expert. Deferring too late may lead to unintended and irreversible harm. Deferring too early
may increase the burden on the human expert. Second, when human interventions (after deferral) are costly,
learning-to-defer adaptively and only when critical is crucial. To defer adaptively, we need a well-characterized
model of the environment, a challenging estimation issue, especially under non-stationarity, i.e., when the
dynamics of the environment change over time.
Existing learning-to-defer methods defer based on immediate outcomes e.g. Mozannar and Sontag (2020);
Madras et al. (2017); Gennatas et al. (2020), and are therefore myopic. Further, the objective to defer is
to improve the performance of some prediction tasks (such as the ability to predict a patient outcome).
These frameworks either defer based on the probability of correct short-term prediction or characterizing
the trade-oï¬€ of paying a cost (to defer). Instead, interventions based on an ML system can have long-term
consequences that are crucial to the model. Further, in many cases, merely deferring to optimize for decision/
1Under review as submission to TMLR
prediction accuracy in a supervised learning setting does not suï¬ƒce to improve long-term outcomes. Existing
approaches also do not leverage the potential of modeling the environment to defer adaptively , especially
beneï¬cial if the environment is non-stationary.
HealthyMonitorAdverseOutcomeğ‘¡Pre-emptive deferLate deferEarly deferMyopic deferral to expertPre-emptively defer to expert (SLTD)
<latexit sha1_base64="KeG1r/5YO4NiVY8xR0EEDYBVIdo=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0lE1GPRi8cK9gOaEDbbTbt0swm7k2IJ+SdePCji1X/izX/jts1BWx8MPN6bYWZemAquwXG+rcra+sbmVnW7trO7t39gHx51dJIpyto0EYnqhUQzwSVrAwfBeqliJA4F64bju5nfnTCleSIfYZoyPyZDySNOCRgpsG0v5UHuAXuCHIgqisCuOw1nDrxK3JLUUYlWYH95g4RmMZNABdG67zop+DlRwKlgRc3LNEsJHZMh6xsqScy0n88vL/CZUQY4SpQpCXiu/p7ISaz1NA5NZ0xgpJe9mfif188guvFzLtMMmKSLRVEmMCR4FgMecMUoiKkhhCpubsV0RBShYMKqmRDc5ZdXSeei4V41Lh8u683bMo4qOkGn6By56Bo10T1qoTaiaIKe0St6s3LrxXq3PhatFaucOUZ/YH3+AIXhlEA=</latexit>â‡¡tar
No deferralsuboptimal compared to <latexit sha1_base64="gwIZZJZyJBKphdCvme7BKbIW9PY=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOJsmQ2dlhplcISz7CiwdFvPo93vwbJ8keNLGgoajqprsr0lJY9P1vr7C2vrG5Vdwu7ezu7R+UD4+aNkkN4w2WyMS0I2q5FIo3UKDkbW04jSPJW9H4bua3nrixIlGPONE8jOlQiYFgFJ3U6mrRy/xpr1zxq/4cZJUEOalAjnqv/NXtJyyNuUImqbWdwNcYZtSgYJJPS93Uck3ZmA55x1FFY27DbH7ulJw5pU8GiXGlkMzV3xMZja2dxJHrjCmO7LI3E//zOikObsJMKJ0iV2yxaJBKggmZ/U76wnCGcuIIZUa4WwkbUUMZuoRKLoRg+eVV0ryoBlfVy4fLSu02j6MIJ3AK5xDANdTgHurQAAZjeIZXePO09+K9ex+L1oKXzxzDH3ifPz43j4U=</latexit>â‡¡0
01234567
Figure 1: A conceptual overview of deferral strategies in a medical setting. We show potential patient
outcome trajectories over time (x-axis). The outcome is shown on the y-axis. A higher outcome is better.
A discretized reward is available to us: â€˜Healthyâ€™, â€˜Continue Monitoringâ€™, and â€˜Adverseâ€™. The white regions
indicate regions where Ï€tarwill provide similar recommendations to Ï€0(speciï¬c actions are not shown). The
shaded gray region indicates that at these times, the patient is in states (not shown) where Ï€tarprovides
recommendations that do not improve long-term outcomes compared to Ï€0. The patient follows the gray
trajectory from time t= 0tot= 2whenÏ€tarâ€™s recommendations are used. The red trajectory shows the
ï¬nal outcome if we continue to use Ï€tartill the end of the horizon. One potential deferral strategy is to defer
when the observed outcome deteriorates to â€˜Adverseâ€™ (orange trajectory). This deferral is late/myopic and
the expert policy is unable to signiï¬cantly improve the patientâ€™s outcome in a single time step. This baseline
is akin to a model where the expert takes over from Ï€tarwhen they see that the patient is deteriorating and
is not an automated deferral. A better-automated deferral strategy is to defer when the predicted outcome
usingÏ€taris â€˜Adverseâ€™ (yellow trajectory). This deferral strategy corresponds to supervised learning-to-defer
methods such as those of Mozannar et. al. and Madras et. al. and is also late/myopic as i) it is based
on immediate predicted outcomes, and ii) does not characterize the regions of the state-space where Ï€tar
is worse than Ï€0(gray region) in terms of the long-term patient outcome. On the other hand, the green
trajectories indicate long-term outcomes if we defer in the gray region (corresponding to what SLTD and
SLTD-Stationary will do), where immediate predicted outcomes are not adverse, but long-term consequences
of continuing to use Ï€tarsigniï¬cantly deteriorate the patient. Note that here we do not show the distinction
between behaviors due to the non-stationarity of the dynamics. Thus, ideally, a pre-emptive deferral policy
should characterize all regions of the state space where long-term outcomes using Ï€tarare worse than Ï€0,
and defer accordingly. Also note that here, both green trajectories are pre-emptive according to our setup
though SLTD will defer earlier at t= 3as well as at t= 4and then rely on Ï€tarfortâ‰¥5. In regions marked
â€œEarly deferâ€, we would like to avoid deferrals since as Ï€taris as good as or better than Ï€0and deferring is
unnecessarily costly. SLTD can avoid such premature deferrals by incorporating a cost for deferrals.
Algorithmic Motivation. To address these challenges, we deviate signiï¬cantly from existing learning-
to-defer methods, which use a supervised learning framework. Instead, we model the learning-to-defer
problem for sequential settings as oï¬„ine model-based reinforcement learning (RL). SLTD is the ï¬rst RL-based
learning-to-defer framework. We focus on settings where online experimentation is prohibitive for safety
reasons, such as healthcare.
We assume access to batch data collected by human experts (such as clinicians) using a behavior policy. Our
goal is to learn a deferral policy with respect to a ï¬xed ML-based policy (called the target policy). SLTD
decides whether or not to defer (to an expert policy) at each instance by modeling the impact of delaying
deferral(by one time step) on the long-term outcomes. SLTD defers if delaying deferral does not improve
outcomes compared to deferring in the current instance. To quantify long-term outcomes, we also account for
all future deferrals. In doing so, SLTD precisely identiï¬es the regions of the state space where the target
ML-based policy will not improve outcomes. As a result, our method is pre-emptive, i.e., it defers in all
2Under review as submission to TMLR
regions where the ML policy is unlikely to improve long-term outcomes . See Figure 1 for a conceptual overview
of SLTD.
Human expert interventions are often costly. Hence deferring too often is not desirable. To defer adaptively ,
we propose to leverage an estimate of the environment dynamics and the associated uncertainty. Modeling
the dynamics allows us to reliably quantify the impact of delaying deferral on long-term outcomes, which is
particularly beneï¬cial in non-stationary settings. We show that modeling the non-stationarity provides a
better trade-oï¬€ of improving outcomes versus the frequency of deferrals. In a myopic environment (i.e., when
the eï¬€ect of interventions are observed in the near future), it may seem unnecessary to model the dynamics.
However, we demonstrate that deferral methods that defer myopically, based on immediate outcomes still
beneï¬t from modeling the dynamics, and consequently the impact of potential future myopic deferrals. When
SLTD defers, human experts can beneï¬t from an additional justiï¬cation of the deferral decision to determine
potential interventions. Hence, we also interpret SLTDâ€™s decision to defer at any given time by quantifying
the long-term uncertainty in the outcome and decomposing the sources of uncertainty. We justify how the
decomposition can guide experts to potential interventions.
Clinical Motivation. We are motivated by clinical settings where a target policy is learned from batch data
to work well across multiple institutions. Such a policy may perform well on average, but when deployed to a
new environment, encounter a diï¬€erent or an evolving patient population. An evolving patient physiology
can often result in non-stationary dynamics on which the target policy is not uniformly better, necessitating
deferral. Clinicians might also follow a slightly diï¬€erent treatment protocol than this learned policy, which
could be an auxiliary reason to defer. Most importantly, regulatory constraints may prevent signiï¬cantly
adapting our target policy completely to the new site. In this case, it is safer to leverage batch data from the
new site to quantify when it is reliable to deploy the learned target policy. In situations where the policy
does not improve outcomes compared to the human expert, exacerbated by challenges like non-stationarity, it
is safer to defer to human experts. We further propose a model-based method motivated by the fact that in
many clinical settings, mechanistic models of patient physiology are available. Beyond healthcare, our work
is applicable in many safety-focused, data-scarce, non-stationary settings where online policy improvement is
not allowed due to ethical or practical constraints.
2 Related Work & Background
Mixture-of-Experts (MoE). Many methods focus on deciding to deploy two or more policies. For
example, Jacobs et al. (1991); Jordan and Jacobs (1994) switch between diï¬€erent policies in decision-making
by partitioning the input space into regions assigned to diï¬€erent specialized sub-models. Variants of this
framework enforce an explicit preference for a speciï¬c expert, e.g., a human expert, and train other experts to
complement the human expert (Pradier et al., 2021). In sequential settings, Parbhoo et al. (2017); Gottesman
et al. (2019); Parbhoo et al. (2018) combine parametric and non-parametric experts to learn more accurate
estimates of the value function. On the other hand, we focus on deferral to human experts when future
outcomes using the current ML-based policy are potentially undesirable . Further, we defer based on explicitly
quantifying the impact of delayed deferral to decide whento defer.
Policy Improvement with Expert Supervision. Sonabend et al. (2020) use hypothesis testing to assess
whether, at each state, a policy from a human expert would improve value estimates over a target policy
during training to improve the target policy. In contrast, our work identiï¬es the value of delaying deferral to
a human expert at test time . Improvements using expert supervision are unlikely to be always feasible due
to safety and regulatory constraints. Learning-to-defer with respect to a ï¬xed target policy is crucial as a
safeguard. Some works focus on safe policy improvement in a non-stationary MDP setting (Chandak et al.,
2020b;a). Chandak et al. (2020a) assume that the non-stationarity is governed by an exogenous process, and
so past actions do not impact the underlying non-stationarity. Our work diï¬€ers in two ways: ï¬rst, we argue
that model misspeciï¬cation, speciï¬cally ignoring non-stationarity induced by (deferral) actions, aï¬€ects the
likelihood of future deferrals. Accounting for this non-stationarity is crucial to avoid costly deferrals. Second,
we incorporate human expertise by explicitly measuring the impact of delaying deferral.
Learning-to-defer to Human Expertise. Madras et al. (2017); Mozannar and Sontag (2020) propose
supervised models to defer to the expert. Here, the classiï¬ers are trained on the samples of an expertâ€™s
decisions. Madras et al. (2017) train a separate rejection and prediction function, while Mozannar and Sontag
3Under review as submission to TMLR
(2020) learn a joint predictor for all targets and deferral. Madras et al. (2017) is conceptually closer to our
work but in a non-sequential setting. Other approaches such as Raghu et al. (2019); Wilder et al. (2020)
ï¬rst train a standard classiï¬er on the data and then compute uncertainty estimates for this classiï¬er and the
human expert. The models defer to the expert if the model is highly uncertain or can signiï¬cantly beneï¬t
from deferral. Liu et al. (2021) incorporate uncertainty in Learning-to-Defer algorithms for classiï¬cation
tasks. Instead, we focus on learning-to-defer in non-stationary, sequential settings. Our work highlights the
role that the dynamics of the environment can have on our ability to defer preemptively.
Learning-to-defer as Causal OPPE. We pose our Learning-to-defer problem as an oï¬„ine model-based
reinforcement learning problem (OPPE). Besides learning, evaluating the utility of a policy (on data collected
from a behavior policy) in an oï¬„ine manner is called Oï¬€-policy Evaluation (OPE) (Precup, 2000). OPE is
a challenging problem as the utility of a policy needs to be determined without exploration. Literature on
OPE is extensive, summarized in a seminal review of (Uehara et al., 2022). Two major classes of solutions i)
Importance Sampling (IS), and ii) Q-learning are used for OPE tasks. Q-learning requires making parametric
assumptions of the Action-value functions. We strongly believe this is less grounded for healthcare settings.
On the other hand, mechanistic models of disease and physiology are often available in clinical settings,
motivating our model-based approach to oï¬„ine learning. For evaluating the quality of our proposed method,
we rely on either the knowledge of the true dynamics or IS in this work. IS is asymptotically unbiased but
can suï¬€er from variance challenges in ï¬nite-sample settings. For real-world data, we use the self-normalized
variant of importance sampling from Uehara et al. (2022); Precup (2000); Robins et al. (2007) as one of the
evaluation metrics.
Implicit in the framework of OPE, including our work, are assumptions about no hidden confounding, which
are surfaced by a causal framing of the OPE problem (Uehara et al., 2022; Gottesman et al., 2018). This
view poses OPE as a causal inference problem given observations from a causal system. The no-hidden
confounding assumption is primarily because most OPE solutions assume that the data is generated from an
MDP which does not allow for potential latent factors to drive decisions. This assumption can have profound
consequences on the quality of OPE estimates. In our setup, we also assume that our oï¬„ine data is generated
from an MDP, thus making the no unobserved/hidden confounding assumption.
Eï¬€orts to relax this assumption allow for the presence of latent factors that inï¬‚uence decision-making in
oï¬€-line data and provide OPE estimates that are robust to the variability of this inï¬‚uence (Kallus and
Zhou, 2018; Tennenholtz et al., 2020; Oberst and Sontag, 2019). Often these works focus on parametric
assumptions of how much the propensity or the probability of a particular treatment is allowed to deviate,
known as the Marginal Structural Model (MSM) assumption (Robins et al., 2000) to provide conservative
OPE estimates under worst-case deviations under the MSM assumption. The utility of MSM assumptions
has been signiï¬cant in epidemiological settings, though relevance in chronic condition management, which is
the focus of our clinical setting, is less clear.
Decomposing Uncertainty for Interpreting Policies. Uncertainty, if well calibrated can help decision-
makers understand the failure modes of a model (Bhatt et al., 2020; Tomsett et al., 2020; Zhang et al., 2020).
Several methods estimate predictive uncertainty in ML (Gal and Ghahramani, 2016; Guo et al., 2017). Here,
we focus on capturing the propagated uncertainty in sequential settings to interpret deferral decisions. We
interpret the (diï¬€erent) sources of propagated uncertainty when SLTD defers to the expert. Decomposing
the sources of uncertainty into modeling and irreducible uncertainty over predictions has been explored
in classiï¬cation and prediction settings (Yao et al., 2019; Depeweg et al., 2018) but remains signiï¬cantly
under-explored for sequential settings.
Background and Notation. We consider our environment to be a ï¬nite horizon MDP deï¬ned by
Mâ‰¡ (S,A,P,r,p 0)whereSindicates the state-space, Aindicates the action-space, Pthe transition
dynamics,r:sÃ—aâ†’R+the reward function, p0the initial state distribution. The action-space is assumed
to be discrete, while the state space can be discrete or continuous. Any intervention policy (usually stochastic
in our case) is given by Ï€=:SÃ—Aâ†’ [0,1]. We consider a non-stationary environment such that the
dynamics at any time tare governed by a speciï¬c MDP Mt. Thus the environment is a sequence of MDPs.
We assume the existence of a true set of non-stationary dynamics governing all episodes and denote it by
Mâˆ—:={Mâˆ—
t}t. In the rest of the draft, M:={Mt}tdenotes an estimate of the true dynamics Mâˆ—. LetT
4Under review as submission to TMLR
be the episode-length. The value of a policy Ï€attis given by VM
Ï€,t(s) =EM[/summationtextT
j=trj(s,a)|st=s,Ï€]. The
action value is given by QM
Ï€,t(s,a) =r(s,a) +/summationtext
s/primeâˆˆSP(s/prime|s,a)VM
Ï€,t(s/prime).
3 Sequential Learning-to-Defer
Problem Setup. Assume we are given a policy Ï€tarthat may be learned from batch data from one or more
environments. Ï€taris intended to be deployed in a new environment. We have access to batch data, denoted
byDâˆ—={si,0,ai,0,ri,0,Â·Â·Â·,si,T,ai,T,ri,T}N
i=1collected in the new non-stationary environment Mâˆ—={Mâˆ—
t}t,
from some (potentially non-stationary) expert policy Ï€0. Note that we assume that Ï€0is given. For example,
it may be the behavior policy from which we have data samples in the target environment. Here Ndenotes
the number of episodes. Our goal is to learn a deferral policy gÏ€tar(s,t) :SÃ—Tâ†’{0,1}(where 1corresponds
to defer orâŠ¥) with respect to Ï€tarto defer to the expert policy Ï€0. In practice, experts may deviate from Ï€0
in some cases. In our experiments, we account for this by using an /epsilon1-greedy version of the behavior policy
asÏ€0, which serves as a proxy model for such deviation. In addition, a clinician may override a treatment
recommendation even when a model does not defer. We account for this using /epsilon1-greedy version of Ï€tar. For
ease of exposition, we still refer to them as Ï€0andÏ€tar.
Deferral to the expert is denoted by the action âŠ¥. That is, we will augment the action space of existing
MDPMâˆ—to include a new deferral action AâŠ¥:=AâˆªâŠ¥. At every step, the agent decides whether or not
to defer. If the agent defers, Ï€0will be deployed for that time step. We describe the formulation assuming
strict adherence to Ï€0at deferral to emphasize other aspects of our contribution such as the impact of
non-stationarity and how to account for relevant sources of uncertainty to compare outcomes. SLTD can
easily account for the uncertainty of expert actions in the framework.
In practice, the target policy Ï€tarmay not uniformly improve over Ï€0for all states. That is guaranteeing that
VMâˆ—
Ï€tar,0(s)â‰¥VMâˆ—
Ï€0,0(s)for allsâˆˆS, is challenging. Even when ï¬ne-tuning is allowed, it is challenging to ensure
that the target policy is indeed better than Ï€0in all regions of the state space. Hence, we would like to get
the best of both worlds. We can deploy Ï€tar, to reduce the costs of relying on human expertise, and learn
to automatically defer to the costlier policy Ï€0(i.e. human expert) when relying on Ï€tardoes not improve
outcomes. In regions of the state-space where the value of Ï€taris lower than Ï€0, it is better to defer to the
human as a â€œsafety protocolâ€. Formally, we only assume that VÏ€tar(s)>VÏ€0(s)for some states sâˆˆS.
Type of non-stationarity: Assuming that the non-stationary dynamics are represented by a sequence of MDPs
allows the SLTD framework to be general and not restricted to speciï¬c forms of non-stationary environments.
The main diï¬€erence between each component in the MDP sequence is that they can be arbitrarily diï¬€erent
state-transition dynamics within the same family of distributions (e.g. gaussian or multinomial distributions).
As a result, this sequence will not share the optimal policy, and hence the optimal is a non-stationary
deterministic policy. When mechanistic models on the speciï¬c typeof non-stationarity are available, they can
be incorporated into our framework to provide less conservative deferral policies.
SLTD. To determine whether to defer at each time step, we quantify whether deferring (relying on Ï€0) or
not deferring (using Ï€tar) at the current time step improves the long-term outcome. Long-term outcomes are
aï¬€ected by potential future deferrals. Thus comparing the consequences of deferring versus relying on the
ML policy at the current instance is equivalent to comparing the impact of deferring now versus delaying
deferral by one time step.
Future deferrals imply that some unknown mixture of Ï€tarandÏ€0is used in the future. We denote such a
mixture policy as Ï€mix. To minimize cumbersome notation, we denote a policy where Ï€taris deployed at
instancetandÏ€mixin the future as: Ï€tar(t),mix(t+). Similarly, if we defer now, then the policy that is deployed
at timetisÏ€0, andÏ€mixin the future. We denote this mixture as Ï€0(t),mix(t+). Thus, at any instance t, we
want to defer if VM
Ï€tar(t),mix(t+)(s)<VM
Ï€0(t),mix(t+)(s). Note that we consider deferral to Ï€0as a costly one. This
is accounted through a constant cost c >0in terms of the value. That is, deferral incurs cost cand the
resulting value is: VM
Ï€0(t),mix(t+)(s)âˆ’c. We can now formalize our stochastic deferral policy:
5Under review as submission to TMLR
Deï¬nition 1. LetÏ€tar,tbe such that there exists stâŠ†Sâˆ€tâˆˆ{0,1,Â·Â·Â·,T}whereP(VM
Ï€tar(t),mix (t+)(s)<
VM
Ï€0(t),mix (t+)(s)âˆ’c)>Ï„for constant cost of deferral c>0and threshold Ï„ >0,âˆ€sâˆˆst. Then the deferral
policygÏ€tar(s,t),1[P(VM
Ï€tar(t),mix (t+)(s)<VM
Ï€0(t),mix (t+)(s)âˆ’c)>Ï„],1[ËœgÏ€tar(s,t)>Ï„].
Corollary 1. By Deï¬nition 1, gÏ€tar(s,t), includes the earliest time in the episode where ËœgÏ€tar(s,t),
P(VM
Ï€tar(t),mix(t+)<VM
Ï€0(t),mix(t+)âˆ’c)>Ï„. Thus,gÏ€tar(s,t)is a pre-emptive deferral policy.
The costcdetermines how conservative SLTD is and trades-oï¬€ frequency of deferral to the value attained.This
parameter should be tuned by domain experts aware of the trade-oï¬€ and risks involved. For instance, in a
critical care setting, we may be more conservative and use a smaller cthan in a chronic care situation. Ï„is a
safety threshold on the probability of worse outcome beyond which we deem that deferral is necessary.
Deï¬nition 1 indicates that to reliably learn the deferral policy, we need to estimate ËœgÏ€tar(s,t),
P(VM
Ï€tar(t),mix (t+)(s)< VM
Ï€0(t),mix (t+)(s)âˆ’c). To estimate this probability, we should model all sources of
uncertainty in the system, including the non-stationary dynamics, and the uncertainty associated with our
modeling assumptions. We use a Bayesian RL approach to account for all sources of uncertainty. We motivate
this by ï¬rst describing our dynamic programming approach to learn-to-defer.
Our dynamic programming procedure maintains an estimate of the deferral probability ËœgÏ€tar(s,t)and reï¬nes
it as we train on the batch data. Given an estimate of ËœgÏ€tar(s,t), we outline the procedure to i) estimate the
value under mixture policies corresponding to deferral (and delayed deferral), ii) modeling the probability of
improvement under various sources of uncertainty, and ï¬nally iii) obtaining a new estimate of the deferral
probability ËœgÏ€tar(s,t)âˆ€sâˆˆSat the given time tusing i) and ii). We then bootstrap this procedure over our
batch data to reï¬ne our deferral probabilities. We describe the procedure for estimating the dynamics in the
discrete setting.
Algorithm 1 Sequential Learning to Defer
Input:Dâˆ—, expert policy Ï€0, target policy Ï€tar.
Estimate Posterior Distributions {Mt,pt(Â·|Dâˆ—)}T
t=0(posteriors over rewards not shown here)
Initialization: Deferral function gÏ€tar(s,t) = 0for allsâˆˆSandtâˆˆ{1,2,Â·Â·Â·,T}.
fornâˆˆBOOTSTRAPS (Dâˆ—)do
SampleMk,{Mk,tâˆ¼pt(Ë™|Dâˆ—)}âˆ€tâˆˆ{1,2,Â·Â·Â·,T},âˆ€kâˆˆ{1,2,Â·Â·Â·,K}
fortâˆˆ{T,Tâˆ’1,Â·Â·Â·,1}do
forsâˆˆSdo
ComputeVM
Ï€tar(t),mix(t+),VM
Ï€0(t),mix(t+)âˆ’câˆ€M
ËœgÏ€tar(s,t)â†â‰ˆ1
K/summationtext
Mkâˆ¼{pt/prime(Â·|D)}T
t/prime=t[1(VMkÏ€tar(t),mix(t+)<VMkÏ€0(t),mix(t+)âˆ’c)]
end for
end for
end for
returngÏ€tar(s,t) =1(ËœgÏ€tar(s,t)>Ï„)âˆ€s,tâˆˆSÃ—{ 1,2,Â·Â·Â·,T}
Estimating Value function. At any instance we defer based on current estimates of gÏ€tar(s,t)(or
equivalently ËœgÏ€tar(s,t)). We sample actions from Ï€tarifgÏ€tar(s,t) = 0andÏ€0otherwise (equivalent to âŠ¥).
Note that the current estimate of gÏ€tar(s,t)determines the future mixture policy as well. We now estimate
the value of the mixture policies using the Bellman Equation of the state and action value functions. For the
mixture policy Ï€m,Ï€tar(t),mix (t+)(corresponding to no deferral at t), the Q-function is:
QM
Ï€m,t(s,a) =r(s,a) +/summationdisplay
s/primeâˆˆSPM(s/prime|s,a)VM
Ï€mix(t+),t+1(s/prime) (1)
and the Value function is:
VM
Ï€m,t(s) =/summationdisplay
aâˆˆAÏ€tar(t)(a|s)QM
Ï€m,t(s,a) (2)
Similarly for the mixture policy if we defer at t.
6Under review as submission to TMLR
Estimating the probability of improving outcomes by delaying deferral. At each instance t, for
all statess, we can estimate the indicator function 1[VMâˆ—
Ï€tar(t),mix(t+)(s)<VMâˆ—
Ï€0(t),mix(t+)(s)âˆ’c]given an estimate
ofËœgÏ€taras described above. However, we do not have access to the true dynamics Mâˆ—. In batch settings, such
as ours, we often estimate the dynamics using maximum-likelihood estimation. Such methods make speciï¬c
assumptions about the distribution governing the dynamics. Our assumptions about the dynamics may be
incorrect resulting in potential misspeciï¬cation of our dynamics model. This increases the uncertainty in the
outcome and potentially over-estimates the probability that relying on the model may improve outcomes. To
account for this additional source of uncertainty, we use a Bayesian RL approach. We describe the procedure
for the dynamics. The procedure for rewards follows an analogous process.
Suppose the parameters of the distributions governing the dynamics are denoted by Î¸tâˆ€tâˆˆ{0,Â·Â·Â·,T}. We
denote the full set of parameters by Î¸={Î¸t}t. We assume a prior distribution over the parameters of the
distribution governing the dynamics PM
Î¸(s/prime|s,a)and the rewards r(s,a). Given batch samples Dâˆ—, we can
estimate the posterior distribution over the non-stationary MDPs and rewards using Bayesian inference:
p(Î¸|Dâˆ—)âˆp(Dâˆ—|Î¸)p(Î¸)
More speciï¬cally, we assume conjugate priors for our parameters Î¸. By relying on conjugate priors in our
inference, the parameters of posterior distributions over the dynamics and rewards are obtained in closed
form. For discrete state dynamics (and rewards), we assume a Dirichlet prior distribution and model the
observations p(Dâˆ—|Î¸)using a Multinomial distribution. For continuous states, p(Dâˆ—|Î¸)is assumed to be
normally distributed with Î¸being the mean and variance parameters. The prior distributions over the mean
and precision (inverse of the variance) is the Normal-gamma prior. This is a domain-dependent choice and
SLTD is agnostic so long as we can sample from the posterior distributions of the learned model dynamics. A
detailed derivation of how the data is leveraged to estimate the posterior distributions over the dynamics are
provided in Appendix A.1. By allowing ï¬‚exibility of modeling the dynamics via Bayesian RL, we can account
for uncertainty over our modeling assumptions.
Finally, based on our assumption that the non-stationary environment is governed by a sequence of MDPs,
we estimate the MDP for each time step independently from batch data. This allows us to make fewer
assumptions about the typeof non-stationarity. Any additional domain knowledge about the nature of
non-stationarity can be leveraged for data eï¬ƒciency. We can now estimate the impact of delayed deferral by
sampling non-stationary MDPs from our posterior distributions and averaging to obtain our ï¬nal probability:
ËœgÏ€tar(s,t),P(VMâˆ—
Ï€tar(t),mix(t+)(s)<VMâˆ—
Ï€0(t),mix(t+)(s)âˆ’c)
=EMâˆ¼p(Â·|Dâˆ—)[1[VM
Ï€tar(t),mix(t+)(s)<VM
Ï€0(t),mix(t+)(s)âˆ’c]]
â‰ˆ1
K/summationdisplay
Mkâˆ¼{pt/prime(Â·|Dâˆ—)}T
t/prime=t1[VMkÏ€tar(t),mix(t+)(s)<VMkÏ€0(t),mix(t+)(s)âˆ’c](3)
where the second line comes from the deï¬nition due to the randomness over the dynamics, and the last
term comes from approximating the expectation using Ksamples from the posterior distribution of the
dynamicsp(Â·|Dâˆ—). Thus, for every instant t, in a given state s, our deferral policy gÏ€tar(s,t)is given by,
gÏ€tar(s,t) :=1[ËœgÏ€tar(s,t)>Ï„].
Dynamic Programming to estimate gÏ€tar(s,t).Our dynamic programming procedure is summarized
in Algorithm 1. We initialize gÏ€tar(s,t) = 0for allsâˆˆS. We estimate VM
Ï€tar(t),mix(t+)(s),VM
Ï€0(t),mix(t+)(s)for a
giventusing Bellman Equations 1 and 2. Following that, we can update our estimate of gÏ€tar(s,t)using our
posterior MDPs, i.e. Equation 3. We repeat (over t) using the updatedestimates of gÏ€tar(s,t). Note further
thatÏ€0is stochastic. Thus, we do not make explicit assumptions on the speciï¬c actions an expert will take
in the futher. More speciï¬cally, SLTD accounts for the added uncertainty in humanâ€™s actions by explicitly
taking expectations over actions aâˆ¼Ï€0. In our experiments, we use an /epsilon1-greedy versions of both Ï€0andÏ€tar,
which will allow for further deviations from the expert policy or allowing for overrides, to reï¬‚ect realistic
settings.
Optimality of Learned Deferral Policy. Note that the optimal policy in our environment is a deter-
ministic non-stationary policy. More speciï¬cally, the optimal policy is a sequence of policies where each
7Under review as submission to TMLR
component in the sequence is optimal with respect to the speciï¬c dynamics at the corresponding time instance.
It may not be possible to always reach such an optimal via deferral. Our goal is thus to signiï¬cantly improve
over our target policy Ï€tarby deferring to the expert policy. This could be envisioned as a setup where are
restricted to a policy class of oï¬„ine RL where we are only allowed to learn from families that defer to the
expert policy and use Ï€tarotherwise.
4 Decomposing the uncertainty at deferral
SLTD defers at time tbecause the probability that relying on Ï€tarimproves the outcome is below our safety
threshold, i.e. SLTD is uncertain of an improved outcome. Conveying this uncertainty can help the domain
expert take over decision-making. We interpret this deferral decision in terms of the total and decomposed
uncertainty on long-term outcomes. We convey two diï¬€erent sources of uncertainty at deferral. First, we
consider epistemic/modeling uncertainty , which captures whether our model speciï¬cation has resulted in high
uncertainty and the aleatoric uncertainty which mainly results from the stochasticity of the environment
itself. A high relative value of the former suggests that adding more data to train SLTD can improve the
conï¬dence of the model. High aleatoric uncertainty suggests that the environment itself is highly variable
leading to the lack of conï¬dence in relying on Ï€tar.
Concretely, let tdbe a time when SLTD defers. The agent is in state std. We are interested in the reward
(and uncertainty over the reward) at time Tdue to deferral at td, i.e., E[rT|std,Âµtd,Ï€0(td),mix (td+)]. We
denote the posterior MDP samples for any state-action pair by Âµt. The variability in these samples captures
modeling uncertainty. The dynamics parameters are denoted by Î¸t(s,a)for each state-action pair. First,
we sample the parameters of the dynamics from posterior distribution p(Î¸t/prime|Dâˆ—), followed by sampling the
MDPsÂµt/primeâˆ¼p(Âµt/prime|Î¸/prime
t(st/prime,at/prime)). Once we defer, we sample actions from Ï€0at timet/prime=tdandÏ€mixfort/prime>td
where the mixture probability is determined by the learned gÏ€tarfor future deferrals. The expected long-term
outcome is given by:
E[rT|std,Âµtd] =/integraldisplaysT
std+1/integraldisplayaT
atd/integraldisplayÂµT
Âµtd+1/integraldisplayT
Î¸tdr(sT,aT)Ã—T/productdisplay
t/prime=td+1pt/prime(st/prime|Âµt/prime)pt/prime(Âµt/prime|Î¸/prime
t(st/prime,at/prime))Ï€t/prime(at/prime|st/prime)pt/prime(Î¸t/prime|D)dsdadÂµdÎ¸
Integrands are written in short-hand: s={std+1,std+2,Â·Â·Â·,sT}(analogously for other quantities). We
maintain one estimate of parameter Î¸t/primeand sample KMDPsÂµt/primefrom this distribution. Thus, the epistemic
uncertainty we capture is due to the uncertainty over dynamics under ï¬xed parameters. The total uncertainty
can now be decomposed using the law of total variance:
Var(rT|std,D)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Total Uncertainty=EÂµtdâˆ¼p(Âµtd|D)[Var(rT|Âµtd,std,D)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Irreducible/ Aleatoric Uncertainty+VarÂµtdâˆ¼p(Âµtd|D)(E[rT|Âµtd,std,D])
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Epistemic/Modeling Uncertainty
The second term is the variance conditioned on knowledge of the model Âµtd. This is the propagated uncertainty
due to modeling uncertainty at tdand can be reduced by data collection. The ï¬rst term averages over the
variance due to Âµtdand captures propagated uncertainty due to aleatoric uncertainty at td, which conveys
stochasticity of the environment itself. This uncertainty can only be reduced by careful interventions at td. We
estimate these using Monte-Carlo sampling. Additional details on the derivation are provided in Appendix A.2.
As suggested before, a high propagated epistemic uncertainty conveys that the current uncertainty of model
prediction (of the dynamics) is high but could be improved if additional data could be collected. High
propagated aleatoric uncertainty indicates high variability in the dynamics that can only be reduced with
careful interventions and is otherwise not manageable.
5 Experiments
We evaluate SLTDâ€™s ability to defer adaptively in sequential settings with respect to a known and ï¬xed Ï€tarto
the expert policy Ï€0. We test the utility of i) deferring based on long-term outcomes, ii) adaptively deferring
by quantifying the impact of delaying deferral, i.e., in regions where delayed deferral can worsen outcomes,
iii) modeling the non-stationarity on deferral frequency, iv) quantifying multiple sources of uncertainty to
estimate the probability of diï¬€erent outcomes under delayed deferral. We test our method on synthetic data,
a non-stationary diabetes simulator1modiï¬ed from Chandak et al. (2020b), and real-world HIV data.
1Jinyu Xie. Simglucose v0.2.1 (2018) [Online]. Available: https://github.com/jxx123/simglucose . Accessed on: 07-24-2021.
8Under review as submission to TMLR
Synthetic Data. In this synthetic simulation, the region of deferral is known apriori by careful design of
Ï€tar. This environment has 8discrete states and binary actions {a0,a1}. All samples start at state 0and
progress toward a sink state 7. The episode length is 15. State 6has a low reward ( âˆ’5) while all other states
have a reward of +1. The initial dynamics are set up such that action a0reduces the probability of landing
in stage 6, and action a1increases the probability of reaching state 6.Ï€tarincreases the chances to reach
state 6unfavourably by taking action a1in states 2,3,4whent<5ort>12. We expect to defer in states
2,3,4even though rewards are favorable if a method is pre-emptive. When 5â‰¤tâ‰¤12, the dynamics ï¬‚ip such
thata0becomes an unfavorable action that increases the probability of landing in 6, whilea1reduces this
probability. Here, Ï€taragain increases the chances of landing in 6, by taking a0more often in states 2,3,4.
By ï¬‚ipping the better action to a0in this region, it becomes crucial to estimate the dynamics over predicting
the best action. The dynamics are non-stationary and the probability of landing in state 6progressively
increases when 5â‰¤tâ‰¤12. Fortâ‰¥13, the dynamics reset to noise levels at t<5adding non-stationarity to
the dynamics. Note that the optimal policy is Ï€(s,t) = 1âˆ€sâˆˆS,5â‰¤tâ‰¤12and0otherwise. The expert
policy is such that:
Ï€0(s,t) :=p(a1) =ï£±
ï£´ï£²
ï£´ï£³0.9if5â‰¤tâ‰¤12,sâˆˆ{2,3,4}
0.7if5â‰¤tâ‰¤12,s /âˆˆ{2,3,4}
0.1t<5ort>12,âˆ€sâˆˆS
for all states sâˆˆS. In this case, pre-emptive deferral will allow us to reach close to the optimal by deferring
in states{2,3,4}.
Real-world simulator: Diabetes Data. We use an open-source implementation of the FDA-approved
Type-1 Diabetes Mellitus simulator (T1DMS) for modeling the treatment of Type-1 diabetes. We sample
10adolescent patient trajectories (episodes) over 24hours (aggregated at 15minute intervals). Glucose
levels are discretized into 13states. Combination interventions of insulin and bolus are discretized to
generate a total of 25actions. We introduce non-stationarity in each episode by increasingly changing the
adolescent patientâ€™s properties to an alternative patient. We enable this by smoothly varying the weighting
of the patient parameters over the horizon. While this does not reï¬‚ect a realistic patient scenario but will
nonetheless evaluate the utility of all methods for a smoothly transitioning non-stationary environment. The
non-stationarity signiï¬cantly aï¬€ects the utility of the initial target policy which is learned on the dynamics of
the original patient, thus necessitating deferral as the patient properties change over time. The non-stationary
target policy Ï€tarfor this task is estimated using Q-learning. An /epsilon1-greedy version of this Q-learned policy
is used in our experiments. We defer to a clinician policy, here simulated by learning an /epsilon1-greedy version
of a policy learned using Q-learning under (estimated) non-stationary dynamics on the target data. For
evaluation of value post learning, we estimate the dynamics on N= 1000patients to remove estimation bias
for evaluation purposes. We further provide IS estimates as we discuss in the metrics below.
Real-world: HIV Data. We identiï¬ed individuals between 18-72 years of age from the EuResist database
(Zazzi et al., 2012) comprising of genotype, phenotype, and clinical information of over 65,000 individuals in
response to antiretroviral therapy administered between 1983-2018. We focus on a subset of 32,960patientsâ€™
genotype, treatment response, CD 4+, and viral load measurements, gender, age, risk group, number of past
treatments collected over on average 14years (aggregated at 4-6 month intervals). Our action space consists
of the 25most frequently occurring drug combinations, while our state space consists of 100continuous
states of cell counts and viral loads. Since the virus evolves in response to drug pressure, the problem is
inherently non-stationary. Our data is collected using a standard ï¬rst-line therapy provided by clinicians.
For our ï¬rst case study (Case-I), we use a candidate clinician-provided policy as Ï€tar. Given data from the
ï¬rst-line therapy, we investigate whether deferring to second-line therapy ( Ï€0), as proposed by standard
medical guidelines in response to potential drug resistance(Saag et al., 2020), improves long-term outcomes.
The non-stationary behavior policy is the ï¬rst line therapy estimated using Q-learning. For our second case
study (Case-II), data is collected from a non-stationary behavior policy which corresponds to a ï¬rst-line
therapy typically used for treating patients of subtype C. Using the same candidate Ï€taras in Case I, we then
examine whether deferring to ï¬rst-line therapy, given by clinical collaborators, for patients of subtype M (due
to potential drug resistance) improves long-term outcomes.
Baselines. We compare to the following baselines.
9Under review as submission to TMLR
0 50.00.51.0
SLTD
0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5
0 50.00.51.0
SLTD-Stat.
0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5
0 50.00.51.0
SLTD-One Step
0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 5
Figure 2: Learned deferral probabilities for SLTD (top row), SLTD stationary (second row), SLTD one-step (third row),
and Augmented-MDP (red dotted line) for c= 0. Each row is a method (Row 1 is SLTD, Row 2 is SLTD-Stationary,
and Row 3 is SLTD-One Step). We are plotting ËœgÏ€tar(s,t)which is a function of states sand timet. Since this is
a function of two dimensions, we represent the variation along tin a row and then for each ï¬xed t, we show the
diï¬€erent values this function can take for diï¬€erent states s. According to Synthetic data design, target policy always
takes suboptimal actions in the yellow region. That is, the optimal deferral policy is to defer for all states 2,3,4for
all0â‰¤tâ‰¤15. That is,gÏ€tar(s,t) = 1âˆ€sâˆˆ{2,3,4}and0â‰¤tâ‰¤15and0otherwise. Further, the dynamics change
over time so that the optimal action ï¬‚ips, as well as the noise increases when 5â‰¤tâ‰¤12requires deferral more
often. Thus, shaded yellow regions are the region of pre-emptive deferral. SLTD, which models non-stationarity defers
adaptively and early in the shaded yellow region (top row) and increases the deferral probability when dynamics
change. SLTD-stationary does not learn calibrated probabilities in the yellow region over time and only defers when
theaveragedynamics of the environment require deferral. SLTD-one-step and Augmented-MDP (dotted red line) only
defer in state 6when the reward is negative and is not pre-emptive.
Mozannar et. al. (Mozannar and Sontag, 2020): This is a supervised method using a consistent loss
function to learn-to-defer. It learns an augmented regressor to defer or recommend treatment myopically
(independently at every time step). When the model defers, the clinicianâ€™s treatment recommendation is
used.
Madras et. al. (Madras et al., 2017): This is an alternative supervised learning-to-defer method. This
baseline learns separate regressors to defer and recommend treatments. We modify it to use Ï€tarto recommend
and learn the rejection function to defer to Ï€0. Note that both the supervised learning-to-defer methods are
trained to predict action targets.
Augmented-MDP : A conceptual contribution of SLTD is to defer by comparing outcomes by delaying
deferral with some knowledge of the expert policy. The deferral action itself is considered to augment the
MDP action space. We explore a baseline that uses Value Iteration in this augmented MDP. Comparing with
this baseline helps evaluate the utility of deferring based on outcomes of delayed versus immediate deferral.
This baseline will defer permanently to the expert, and knowledge of an expert policy is not assumed. This
augmented MDP has action-space is AâˆªâŠ¥, an augmented state-space Sâˆªsdefer(sdeferis the deferred state),
and defers based on the cost c. This baseline models non-stationary dynamics, and is designed for sequential
settings. However since this method defers permanently to the expert, it incurs a larger deferral cost. In our
experiments, all values are plotted withoutthe cost to reï¬‚ect actual environment outcomes.
SLTD-Stationary : To assess the impact of misspecifying the non-stationarity, we compare to a variant of
SLTD that assumes the dynamics (and rewards) are stationary while allowing the method the ï¬‚exibility of
learning a non-stationary deferral policy.
SLTD-One Step : We compare to a myopic version of SLTD that defers based on the immediate reward.
The key diï¬€erence with the myopic Madras et. al., Mozannar et. al. baselines is that SLTD-One Step models
the dynamics and the uncertainty on the immediate reward. Thus, this baseline accounts for future deferrals
while deferring myopically.
Ablations for Uncertainty Modeling : For all SLTD variants, we evaluate the utility of accounting for
diï¬€erent sources of uncertainty, more speciï¬cally the modeling uncertainty to estimate the probability of
improving outcomes via delayed deferral. In SLTD, modeling uncertainty is accounted for by sampling
multiple (K) MDPs (Equation 3) from the posterior dynamics distribution, over which our outcomes are
averaged. Higher variability across Kindicates higher modeling uncertainty. Hence, in Equation 3, using
10Under review as submission to TMLR
K= 1assumes a perfect estimate of the dynamics model and only accounts for the irreducible stochasticity
of the environment. A larger Kaccounts for potential variability in estimation (original SLTD formulation).
If our modeling uncertainty in the environment is indeed large, we anticipate the choice of Kto have a larger
eï¬€ect on SLTDâ€™s performance. Modeling uncertainty can be large when there is insuï¬ƒcient data to ï¬t the
target function class of the dynamics.
HIV - Case Study I
(Value)HIV - Case Study II
(Value)
MethodThresholded Defer
(MLE Dynamics)
(meanÂ±2 s.e.)Stochastic Defer
(MLE Dynamics)
(meanÂ±2 s.e.)Self-Normalized IS
(meanÂ±2 s.e.)Thresholded Defer
(MLE Dynamics)
(meanÂ±2 s.e.)Stochastic Defer
(MLE Dynamics)
(meanÂ±2 s.e.)Self-Normalized IS
(meanÂ±2 s.e.)
SLTD 14.792Â±0.267 14.872 Â±0.159 14.159 Â±0.326 8.754Â±0.125 8.951 Â±0.023 8.861Â±0.041
SLTD-Stat. 11.020Â±0.230 11.148 Â±0.17214.126Â±0.027 4.291Â±0.218 4.874 Â±0.156 5.120Â±0.016
SLTD-One Step 9.671Â±0.129 8.671 Â±0.149 7.159Â±0.133 4.588Â±0.178 4.129 Â±0.158 5.223Â±0.085
SLTD
(K=1)9.311Â±0.162 9.425 Â±0.216 13.851Â±0.159 6.492Â±0.388 6.517 Â±0.218 7.759Â±0.128
SLTD-Stat.
(K=1)8.659Â±0.027 7.153 Â±0.128 9.756Â±0.277 3.662Â±0.059 3.917 Â±0.124 5.179Â±0.059
SLTD-One Step
(K=1)8.640Â±0.104 6.174 Â±0.157 6.191Â±0.123 3.959Â±0.130 4.125 Â±0.022 3.898Â±0.035
Augmented-MDP N/A N/A N/A N/A N/A N/A
Mozannar et. al. N/A 3.820 Â±0.231 -3.159Â±0.457 N/A 4.726 Â±0.295 4.618Â±0.330
Madras et. al. N/A 3.330 Â±0.481 -2.159Â±0.318 N/A 4.972 Â±0.125 4.916Â±0.174
Ï€tar N/A 5.837 Â±0.171 6.054Â±0.138 N/A 3.292 Â±0.274 4.174Â±0.218
Ï€0 N/A 14.124 Â±0.592 14.127Â±0.389 N/A 5.629 Â±0.159 5.128Â±0.065
Table 1: Expected Value for HIV Case I and II using i) MLE Estimate of dynamics when we use a deterministic
thresholded deferral policy (ï¬rst column), ii) MLE Estimate of dynamics using the stochastic deferral policy,
i.e. without thresholding and sampling to defer (second column), iii) Self-normalized Importance Sampling
estimate (third column). Note that only SLTD baselines will behave diï¬€erently based on stochastic versus
thresholded deferral. All other baselines are implemented to be consistent with their original implementation
and hence stochastic vs threshold diï¬€erence is not applicable. These baselines that defer stochastically and
hence appear in the â€˜Stochastic deferâ€™ column. As can be seen, SLTD improves value based on all value
estimates.
5.1 Evaluation Metrics.
We use the following evaluation metrics for assessing our proposed models.
Value Estimation. Once a deferral policy is learned, evaluating its utility using oï¬„ine data is the problem
of oï¬€-policy evaluation (OPE). We use the following estimation procedure for evaluating the Value of a
deferral policy:
1.Rollout with true dynamics: For Synthetic data and Diabetes , we have access to the true dynamics ,
although Diabetes data may be prone to discretization error. Here we can roll out the trajectories
under the true dynamics using deferral and collect the true value estimates. In our experiments, we
collect 10000trajectories for all datasets. We use two variants of the deferral policy learned by each
method: i) Thresholded deterministic policy, as proposed in our setup, and ii) Stochastic version
of the learned deferral policy. The latter allows for better comparison with our third method for
estimating the value, speciï¬cally Importance Sampling.
2.Rollout with estimated dynamics: For HIV data, we do not have access to the true dynamics.
Hence we use the same procedure above but with estimated dynamics, which are estimated using
Maximum-Likelihood. This evaluation can be used when there is suï¬ƒcient domain knowledge to rely
on the utility of MLE estimates of dynamics. In our HIV data, the MLE estimates are considered
state-of-the-art in clinical knowledge of HIV treatment Parbhoo et al. (2017). However, in general,
relying on an MLE estimate is prone to a biased value estimate.
3.Self-normalized Importance Sampling (IS): For real-world data where true dynamics are unavailable
and relying on MLE estimates of the dynamics may lead to bias, we provide an IS estimate of the
11Under review as submission to TMLR
value. This metric will likely be used in most real-world scenarios while choosing the best deferral
policy. Note that IS is only asymptotically unbiased and suï¬€ers from large variance issues. Hence
we use a Self-normalized IS estimate. Further, IS can only be used when the assumption of no
unobserved confounding holds. In cases where true dynamics are known, such as Synthetic data,
comparing the value estimate from the rollout of the stochastic version of the deferral policy (under
true dynamics) and the Self-normalized IS version can provide a good sense of the amount of bias
in the IS estimate. In our results, we provide IS estimates for stochastic versions of SLTD and its
variants for nuanced comparison.
Deferral Frequency. In order to assess the trade-oï¬€ of deferral frequency and value attained, we use the
true dynamics for Synthetic data and Diabetes data to roll out trajectories and estimate deferral frequency.
For HIV data, we use the Maximum-Likelihood dynamics to roll out trajectories to estimate deferral frequency
based on the consistency of value estimates of Self-normalized IS and MLE dynamics, which then allows us
to use the MLE dynamics to roll out trajectories. Please see further justiï¬cation of this choice in Section 6.
Synthetic data Diabetes
MethodThresholded Defer
(True Dynamics)
(meanÂ±2 s.e.)Stochastic Defer
(True Dynamics)
(meanÂ±2 s.e.)Self-Normalized IS
(meanÂ±2 s.e.)Thresholded Defer
(True Dynamics)
(meanÂ±2 s.e.)Stochastic Defer
(True Dynamics)
(meanÂ±2 s.e.)Self-Normalized IS
(meanÂ±2 s.e.)
SLTD 8.029Â±0.039 4.919Â±0.064 5.455Â±0.043 36.931Â±0.166 28.257Â±0.093 -0.417Â±0.000
SLTD-Stat. 5.588Â±0.048 4.329 Â±0.057 5.291Â±0.000 34.326Â±0.172 23.119 Â±0.166 -0.418Â±0.000
SLTD-One Step 5.578Â±0.050 1.391 Â±0.047 5.367Â±0.022 36.819Â±0.23 28.829Â±0.325 -0.417Â±0.000
SLTD
(K=1)8.011Â±0.025 4.959 Â±0.124 5.399Â±0.066 36.678Â±0.289 28.478 Â±0.351 -0.417Â±0.000
SLTD-Stat.
(K=1)5.575Â±0.036 4.343 Â±0.089 5.291Â±0.000 34.320Â±0.175 22.840 Â±0.280 -0.418Â±0.000
SLTD-One Step
(K=1)5.595Â±0.038 1.399 Â±0.110 5.400Â±0.068 36.482Â±0.284 28.943 Â±0.406 -0.417Â±0.001
Augmented-MDP 3.044Â±0.023 N/A 5.854Â±0.000 13.273Â±0.126 N/A -0.404 Â±0.000
Mozannar et. al. N/A 6.369Â±2.226 4.469Â±0.289 N/A -15.155 Â±0.675 -0.418Â±0.000
Madras et. al. N/A 5.731 Â±0.223 5.291Â±0.000 N/A 35.388Â±0.475 -0.419Â±0.000
Ï€tar N/A -1.80 Â±0.071 5.291Â±0.000 N/A 13.202 Â±0.162 -0.419Â±0.000
Ï€0 N/A 5.485 Â±0.035 11.276Â±0.000 N/A 34.241 Â±0.151 -8.085Â±0.000
Table 2: Expected Value of SLTD compared with baselines for Synthetic data and Diabetes data. i) True
dynamics when we use a deterministic thresholded deferral policy (ï¬rst column), ii) True dynamics using the
stochastic deferral policy, i.e. sampling to defer (second column), iii) Self-normalized Importance Sampling
estimate (third column). Note that only SLTD baselines will behave diï¬€erently based on stochastic versus
thresholded deferral. All other baselines are implemented to be consistent with their original implementation
and hence stochastic vs threshold diï¬€erence is not applicable. These baselines that defer stochastically and
hence appear in the â€˜Stochastic deferâ€™ column. Also note that Augmented-MDP is a deterministic policy.
For Synthetic data, IS estimate is biased relative to the True dynamics that uses a stochastic deferral policy,
suggesting IS estimates may not be reliable due to ï¬nite samples. For Diabetes , we see that IS is unable to
distinguish between diï¬€erent deferral policies due to support diï¬€erences between the target policy and the
behavior policy. We thus rely on values estimated from the true dynamics for our analysis.
6 Results
Optimizing for long-term outcomes learns qualitatively diï¬€erent deferral policies. Our deferral
policy is a non-stationary stochastic function ËœgÏ€tar(s,t)which we threshold. Visualizing ËœgÏ€tar(s,t)enables us
to understand the utility of various modeling choices of SLTD. Figure 2 shows the histograms of ËœgÏ€tarfor
SLTD and its Stationary and One-Step variant when the cost c= 0for Synthetic data. Visualizing without
deferral cost allows us to see how adaptive SLTD is without a penalty. Each row corresponds to a method;
the x-axis corresponds to time over the horizon T. Each box in a row corresponds to a single time point. For
a ï¬xedt,ËœgÏ€taris a stochastic function of the states, shown as a histogram.
12Under review as submission to TMLR
The yellow shaded region indicates the state space where Ï€tartakes unfavorable actions. Over time, the
dynamics change so that the favorable action ï¬‚ips 5â‰¤tâ‰¤12and the stochasticity in the dynamics
increases requiring more frequent deferrals. Deferring in the yellow region is desirable to pre-emptively avoid
landing in a state of 6. SLTD is highly adaptive, and pre-emptively defers in the yellow region. As the
stochasticity increases, the probability of deferrals appropriately increases. The stationary variant signiï¬cantly
underestimates the need to defer in states 2,3,4whent<5. It is only able to pre-emptively defer in regions
where the average stochasticity of the estimated dynamics aligns with the environment. The One-Step variant
defers only in state 6and is therefore not preemptive. Augmented-MDP (red vertical line) deterministically
defers in state 6. Thus deferring based on the probability of improved outcomes of immediate and delayed
deferrals is desirable over alternatives (see also Figures 4, 5, and 6 in Appendix for Diabetes and HIV
data).
It is important to note that SLTD lowers deferral frequency for tâ‰¥12. The reason for this is twofold. First,
it is an artifact of the synthetic data that we use to evaluate SLTD. More speciï¬cally, there is an additional
form of non-stationarity that kicks in at tâ‰¥12where we stop adding noise to our dynamics, which resets
the dynamics to where they were at t<5. As a result, the deferral frequency is expected to be lower than
at5â‰¤tâ‰¤12. Second, since SLTD is designed in an oï¬„ine learning environment, the support of the data
matters (for any oï¬„ine learning algorithm). For t>12, the oï¬„ine data collected does not have signiï¬cant
support over the â€œdeferralâ€ states [2,3,4], which induces an estimation issue. While we expect this to be
compensated by uncertainty modeling that we incorporate, lack of data support is a much more fundamental
problem in oï¬„ine learning, and we anticipate requiring better prior knowledge of the dynamics to completely
overcome the bias that is introduced in the deferral frequency. This bias further lowers deferral frequencies
even more compared to t<5. Nonetheless, the learned deferral policy reaches optimal as can be seen from
Table 9 and noting that the value estimate we have from the optimal policy for Synthetic data was estimated
to beâ‰ˆ7.9.
Method
SLTD SLTD-Stationary SLTD-One Step Augmented-MDP Mozannar et al. Madras et al. Clinician Behavior
0.0 0.2 0.4 0.6 0.8 1.0
Defer Frequency1234567ValueSynthetic
0.0 0.2 0.4 0.6 0.8 1.0
Defer Frequency10
0102030ValueDiabetes
Figure 3: The trade-oï¬€ of deferral frequency and value attained (Synthetic data and Diabetes use true
dynamics, while HIV uses Maximum-Likelihood estimates for value estimates). The plot shows the expected
value (higher is better) for a sweep over deferral cost (and any other method-speciï¬c hyperparameters) to
the deferral frequency (lower is better). SLTD achieves the best trade-oï¬€ and better value by deferring
pre-emptively. The stationary variant defers signiï¬cantly more to achieve better outcomes. The One-Step
variant cannot improve over Ï€tarin MDPs where the eï¬€ect of interventions is not myopic (Synthetic and HIV)
and achieve good performance by modeling the dynamics in Diabetes, compared to existing learning-to-defer
baselines Mozannar et. al. and Madras et. al. (which are myopic). Augmented-MDP is unable to achieve
good performance even at up to 50% deferral and unable to improve value in Diabetes indicating the beneï¬ts
of deferring by explicitly quantifying the impact of delaying deferral as in SLTD over Augmented-MDPâ€™s
Value Iteration method.
SLTD improves long-term outcomes. Tables 1 and 2 show the value (higher is better) for all baselines
using the diï¬€erent value estimation methods outlined above. We have shown the highest value for each value
estimate independently. For HIV data, we do not see signiï¬cant diï¬€erences between MLE dynamics and IS
evaluation metrics. SLTD unanimously outperforms all baselines. Table 2 shows signiï¬cant biases in the IS
estimate compared to the true dynamics suggesting it is not reliable for Synthetic data and Diabetes data.
Our original formulation of thresholding the deferral policy works better under the true dynamics than the
stochastic version as evidenced by results corresponding to the Stochastic defer variant. We nonetheless
13Under review as submission to TMLR
include this method to assess biases in IS estimates, as IS uses the stochastic deferral policy for value
estimation. Based on IS estimates, Augmented-MDP performs the best for Synthetic data. Table 10 in
Appendix shows the frequency of deferrals and the corresponding cost hyperparameter, if IS value estimates
are used to select the best deferral methods. This performance is obtained when there is no cost to deferring
(c= 0). Forc>0, we see a drop in value to 3.57Â±0.029for Augmented-MDP, with about 50%deferral
frequency. This suggests that SLTD is still eï¬ƒcient and able to reach a high value for fewer deferrals in
Synthetic data data. Nonetheless, we would like to highlight that for Synthetic data we the IS estimates
appear to have a large bias to be entirely reliable.
For Diabetes data, support diï¬€erences between the behavior and target policy result in unreliable IS estimates.
This is also evidenced by the fact that the clinician policyâ€™s IS estimate is â‰ˆâˆ’8.0, which is not possible
since the behavior policy is learned on the target data using Q-learning (the average value estimate from
Q-learning on the true dynamics was found to be 18.027). For the rest of this analysis, we thus focus on the
value estimates from the true dynamics for Synthetic data and Diabetes data, and we rely on the consistency
of all value evaluation methods for HIV to draw our conclusions.
Based on the true dynamics (and threshold policy), Augmented-MDP baseline is not preemptive despite
modeling the non-stationary dynamics. Hence, deferring by comparing outcomes of delayed deferral is a better
alternative, speciï¬cally Value Iteration on the Augmented MDP. Additional beneï¬ts of modeling the dynamics
are clear from the improved performance of all SLTD variants compared to the myopic baselines.
Mis-speciï¬cation of dynamics (SLTD-Stat.) results in worse performance. SLTD-Stat. defers more often to
achieve comparable performance. Figure 3 demonstrates this trade-oï¬€ is general, for all choices of deferral
costs and other parameters. In Figure 3, the x-axis corresponds to deferral frequency (lower is better) and
y-axis the value attained (higher is better). For Synthetic data and Diabetes , value estimates using the true
dynamics are shown. For HIV data, since we do not have access to the true dynamics, the deferral frequency
is estimated using an MLE estimate of the dynamics. From Table 1, we can see that all value estimates are
comparable. SLTD achieves the best trade-oï¬€.
Further, pre-emptive deferral allows SLTD to reach close to optimal (value average obtained is â‰ˆ7.9).
SLTD-One Step only relies on immediate rewards failing to improve long-term outcomes for synthetic data
and HIV. However, as long as we model the dynamics appropriately, even myopic deferral using SLTD One
Step is beneï¬cial for Diabetes compared to Mozannar et. al., Madras et. al.. This is possible when the eï¬€ect
of interventions is observed myopically, as is the case in Diabetes data since modeling the dynamics and
impact of future deferral is beneï¬cial to characterize. Madras et. al., Mozannar et. al. baselines are unable
to maximize long-term rewards. Madras et. al. performs well on Diabetes data suggesting optimal actions
donâ€™t signiï¬cantly deviate in target data and that its design of training a rejection function worked better
than the loss function design of Mozannar et. al.
Defer Time
tdTotal
UncertaintyModeling
UncertaintyMean
Outcome
Synthetic data 3 26.190 0.233 3.42
Diabetes 3 3418.17 34.160 73.669
Table 3: Interpreting ï¬rst time of deferral for a sample trajectory. Modeling uncertainty remains low in all
cases whereas in comparison, total variance is high. This indicates irreducible stochasticity of the dynamics is
the primary source of uncertainty. Additional results are in Appendix.
Ablations for uncertainty modeling. We study the utility of accounting for modeling uncertainty in
our framework. As described in Section 4, multiple sources of propagated uncertainty contribute to variability
in estimated outcomes. Modeling uncertainty is crucial to account for in a model-based framework. Here we
evaluate the impact of not accounting for this uncertainty on SLTDâ€™s performance.
If modeling uncertainty is high, variability of the sampled MDPs used to estimate Equation 3 will be higher.
Evaluating for K= 1, will evaluate the impact of ignoring this uncertainty. In Table 9 (see also Figure 8
in Appendix), we demonstrate the results with K= 1for all SLTD variants. We do not observe signiï¬cant
diï¬€erences for Synthetic data and Diabetes indicating that our modeling uncertainty is low in these data.
14Under review as submission to TMLR
The diï¬€erence is higher in HIV suggesting the importance of accounting for this uncertainty for real-world
HIV data. Such analysis is crucial to understanding whether our modeling assumptions are reasonable.
Decomposing uncertainty in SLTD can help interpret deferral. Conveying the type of uncertainty
to a domain expert can help identify the dominant source of uncertainty that resulted in a deferral to their
standard practice (expert policy). Table 3 shows this decomposition for one timepoint for discrete data. In
each case, the modeling uncertainty is a small fraction of the total uncertainty. This suggests that systematic
non-stationarity is the dominant source of uncertainty which generally cannot be reduced by collecting
data and may require careful interventions beyond the standard policy. Knowledge of the amount of model
uncertainty can enable users to further improve decision-making through data collection or improving model
assumptions.
7 Discussion
We proposed SLTD, a learning-to-defer framework for sequential settings using oï¬„ine model-based RL. We
learn a deferral policy by quantifying the impact of delaying deferral to the future. SLTD can defer based
on long-term outcomes and learns a pre-emptive deferral policy. Further, we emphasize a model-based
RL method that captures the dynamics of the environment, particularly non-stationarity. Modeling the
non-stationarity of the environment allows deferring adaptively. Misspecifying non-stationarity leads to
signiï¬cantly more deferrals to improve long-term outcomes. We demonstrate that existing learning-to-defer
frameworks are myopic. That is, these methods do not learn a pre-emptive policy even in sequential settings
as they focus on the immediate consequences of actions. We further demonstrate the utility of accounting for
all potential sources of stochasticity to quantify the impact of delayed deferral. Explicit characterization of the
probability of improving outcomes is beneï¬cial to prevent over-estimation of the beneï¬ts of delaying deferral.
We further interpret deferral decisions of SLTD by decomposing the long-term propagated uncertainty.
Limitations and Future Work. While quantifying the uncertainty is useful, especially to compensate for
the fundamental challenge of data support in oï¬„ine reinforcement learning, modeling uncertainty through non-
stationarity is costly. Developing a model-free framework is an important aspect of future work. Uncertainty
quantiï¬cation may not be able to compensate for cases of severe data support issues, which may result in biases
in the learned SLTD policy. In this case, better prior knowledge of the dynamics is necessary. SLTD assumes
no hidden/unobserved confounding by relying on an MDP data-generating assumption. Thus our current
uncertainty quantiï¬cation does not account for the added epistemic uncertainty due to potential hidden
confounding. Incorporating clinically motivated sensitivity models to account for unobserved confounding is
an active area of our future work. SLTD can account for some deviations from the expert policy, as well as
the potential for its recommendations to be overridden, though signiï¬cant deviations should be modeled as
an online human-in-the-loop system. In this case, SLTD can serve as a reliable warm-start policy that could
be further improved using (online) human input. We also posit that our current /epsilon1-greedy version will prove
to be more conservative compared to such an online framework, as we expect real human decisions to be
more informed (modulo their own biases) than a noise model that is /epsilon1-greedy. Nonetheless, rigorously testing
this human-in-the-loop learning-to-defer framework is left to future work. Finally, note that if the type of
non-stationarity we would like to defer against is not observed in the oï¬„ine data, the deferral policy may
over/understimate the need to defer.
Ethical considerations. SLTD is a technical proof-of-concept to defer to an expert by accounting for
long-term eï¬€ects, assuming that the expert is better at increasing value over the current policy in certain
regions. In practice, an expert policy may not be bias free. Thus, deferral may result in biased decisions if the
expert is biased. Such bias may be exacerbated due to potential sources of hidden confounding (Gottesman
et al., 2018). While we are not focused on addressing bias, exposing uncertainties may encourage expert
introspection. Nonetheless, deferring is better when an automated decision may be harmful.
References
Umang Bhatt, Javier AntorÃ¡n, Yunfeng Zhang, Q Vera Liao, Prasanna Sattigeri, Riccardo Fogliato,
Gabrielle Gauthier MelanÃ§on, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, et al. Uncertainty as a
form of transparency: Measuring, communicating, and using uncertainty. arXiv preprint arXiv:2011.07586 ,
2020.
15Under review as submission to TMLR
Yash Chandak, Scott M Jordan, Georgios Theocharous, Martha White, and Philip S Thomas. Towards safe
policy improvement for non-stationary mdps. arXiv preprint arXiv:2010.12645 , 2020a.
Yash Chandak, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar Mahadevan, and Philip Thomas.
Optimizing for the future in non-stationary mdps. In International Conference on Machine Learning , pages
1414â€“1425. PMLR, 2020b.
Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steï¬€en Udluft. Decomposition of
uncertainty in bayesian deep learning for eï¬ƒcient and risk-sensitive learning. In International Conference
on Machine Learning , pages 1184â€“1193. PMLR, 2018.
Daniel Fink. A compendium of conjugate priors. See http://www. people. cornell.
edu/pages/df36/CONJINTRnew% 20TEX. pdf , 46, 1997.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd
International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research ,
pages 1050â€“1059, New York, New York, USA, 20â€“22 Jun 2016. PMLR. URL http://proceedings.mlr.press/
v48/gal16.html .
Efstathios D. Gennatas, Jerome H. Friedman, Lyle H. Ungar, Romain Pirracchio, Eric Eaton, Lara G.
Reichmann, Yannet Interian, JosÃ© Marcio Luna, Charles B. Simone, Andrew Auerbach, Elier Delgado,
Mark J. van der Laan, Timothy D. Solberg, and Gilmer Valdes. Expert-augmented machine learning.
Proceedings of the National Academy of Sciences , 117(9):4571â€“4577, 2020. ISSN 0027-8424. doi: 10.1073/
pnas.1906831117. URL https://www.pnas.org/content/117/9/4571 .
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan, Linying
Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning algorithms in
observational health settings. arXiv preprint arXiv:1805.12298 , 2018.
Omer Gottesman, Yao Liu, Scott Sussex, Emma Brunskill, and Finale Doshi-Velez. Combining parametric
and nonparametric models for oï¬€-policy evaluation. In International Conference on Machine Learning ,
pages 2366â€“2375. PMLR, 2019.
Chuan Guo, Geoï¬€ Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning , pages 1321â€“1330. PMLR, 2017.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoï¬€rey E Hinton. Adaptive mixtures of local
experts. Neural computation , 3(1):79â€“87, 1991.
Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
computation , 6(2):181â€“214, 1994.
Nathan Kallus and Angela Zhou. Confounding-robust policy improvement. Advances in neural information
processing systems , 31, 2018.
Jessie Liu, Blanca Gallego, and Sebastiano Barbieri. Incorporating uncertainty in learning to defer algorithms
for safe computer-aided diagnosis. arXiv preprint arXiv:2108.07392 , 2021.
David Madras, Toniann Pitassi, and Richard Zemel. Predict responsibly: improving fairness and accuracy by
learning to defer. arXiv preprint arXiv:1711.06664 , 2017.
Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. arXiv preprint
arXiv:2006.01862 , 2020.
Michael Oberst and David Sontag. Counterfactual oï¬€-policy evaluation with gumbel-max structural causal
models. In International Conference on Machine Learning , pages 4881â€“4890. PMLR, 2019.
16Under review as submission to TMLR
Sonali Parbhoo, Jasmina Bogojeska, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Combining kernel
and model based learning for hiv therapy selection. AMIA Summits on Translational Science Proceedings ,
2017:239, 2017.
Sonali Parbhoo, Omer Gottesman, Andrew Slavin Ross, Matthieu Komorowski, Aldo Faisal, Isabella Bon,
Volker Roth, and Finale Doshi-Velez. Improving counterfactual reasoning with kernelised dynamic mixing
models.PloS one , 13(11):e0205839, 2018.
Melanie F Pradier, Javier Zazo, Sonali Parbhoo, Roy H Perlis, Maurizio Zazzi, and Finale Doshi-Velez.
Preferential mixture-of-experts: Interpretable models that rely on human expertise as much as possible.
arXiv preprint arXiv:2101.05360 , 2021.
Doina Precup. Eligibility traces for oï¬€-policy policy evaluation. Computer Science Department Faculty
Publication Series , page 80, 2000.
Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mullainathan. The
algorithmic automation problem: Prediction, triage, and human eï¬€ort. arXiv preprint arXiv:1903.12220 ,
2019.
James Robins, Mariela Sued, Quanhong Lei-Gomez, and Andrea Rotnitzky. Comment: Performance of
double-robust estimators when" inverse probability" weights are highly variable. Statistical Science , 22(4):
544â€“559, 2007.
James M Robins, Miguel Angel Hernan, and Babette Brumback. Marginal structural models and causal
inference in epidemiology. Epidemiology , pages 550â€“560, 2000.
Michael S Saag, Rajesh T Gandhi, Jennifer F Hoy, Raphael J Landovitz, Melanie A Thompson, Paul E Sax,
Davey M Smith, Constance A Benson, Susan P Buchbinder, Carlos Del Rio, et al. Antiretroviral drugs for
treatment and prevention of hiv infection in adults: 2020 recommendations of the international antiviral
societyâ€“usa panel. Jama, 324(16):1651â€“1669, 2020.
Aaron Sonabend, Junwei Lu, Leo Anthony Celi, Tianxi Cai, and Peter Szolovits. Expert-supervised
reinforcement learning for oï¬„ine policy learning and evaluation. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,
pages 18967â€“18977. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
daf642455364613e2120c636b5a1f9c7-Paper.pdf .
Guy Tennenholtz, Uri Shalit, and Shie Mannor. Oï¬€-policy evaluation in partially observable environments.
InAAAI, pages 10276â€“10283, 2020.
Richard Tomsett, Alun Preece, Dave Braines, Federico Cerutti, Supriyo Chakraborty, Mani Srivastava,
Gavin Pearson, and Lance Kaplan. Rapid trust calibration through interpretable and uncertainty-aware ai.
Patterns, 1(4):100049, 2020.
Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of oï¬€-policy evaluation in reinforcement
learning. arXiv preprint arXiv:2212.06355 , 2022.
Bryan Wilder, Eric Horvitz, and Ece Kamar. Learning to complement humans. arXiv preprint
arXiv:2005.00582 , 2020.
Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quantiï¬cation for
bayesian neural network inference. arXiv preprint arXiv:1906.09686 , 2019.
Maurizio Zazzi, Francesca Incardona, Michal Rosen-Zvi, Mattia Prosperi, Thomas Lengauer, Andre Altmann,
Anders Sonnerborg, Tamar Lavee, Eugen SchÃ¼lter, and Rolf Kaiser. Predicting response to antiretroviral
treatment by machine learning: the euresist project. Intervirology , 55(2):123â€“127, 2012.
Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. Eï¬€ect of conï¬dence and explanation on accuracy
and trust calibration in ai-assisted decision making. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency , pages 295â€“305, 2020.
17