Under review as submission to TMLR
ANCER: Anisotropic Certiﬁcation via Sample-wise Volume
Maximization
Anonymous authors
Paper under double-blind review
Abstract
Randomized smoothing has recently emerged as an eﬀective tool that enables certiﬁcation of
deep neural network classiﬁers at scale. All prior art on randomized smoothing has focused
on isotropic /lscriptpcertiﬁcation, which has the advantage of yielding certiﬁcates that can be easily
compared among isotropic methods via /lscriptp-norm radius. However, isotropic certiﬁcation limits
the region that can be certiﬁed around an input to worst-case adversaries, i.e.it cannot
reason about other “close”, potentially large, constant prediction safe regions. To alleviate
this issue, ( i) we theoretically extend the isotropic randomized smoothing /lscript1and/lscript2certiﬁcates
to their generalized anisotropic counterparts following a simpliﬁed analysis. Moreover, ( ii) we
propose evaluation metrics allowing for the comparison of general certiﬁcates – a certiﬁcate is
superior to another if it certiﬁes a superset region – with the quantiﬁcation of each certiﬁcate
through the volume of the certiﬁed region. We introduce AnCer, a framework for obtaining
anisotropic certiﬁcates for a given test set sample via volume maximization. We achieve
it by generalizing memory-based certiﬁcation of data-dependent classiﬁers. Our empirical
results demonstrate that AnCer achieves state-of-the-art /lscript1and/lscript2certiﬁed accuracy on
CIFAR-10 and ImageNet in the data-dependence setting, while certifying larger regions in
terms of volume, highlighting the beneﬁts of moving away from isotropic analysis.
1 Introduction
The well-studied fact that Deep Neural Networks (DNNs) are vulnerable to additive imperceptible noise
perturbations has led to a growing interest in developing robust classiﬁers (Goodfellow et al., 2015; Szegedy
et al., 2014). A recent promising approach to achieve state-of-the-art provable robustness ( i.e.a theoretical
bound on the output around every input) at the scale of ImageNet (Deng et al., 2009) is randomized smoothing
(Lecuyer et al., 2019; Cohen et al., 2019). Given an input xand a network f, randomized smoothing constructs
g(x) =E/epsilon1∼D[f(x+/epsilon1)]such thatg(x) =g(x+δ)∀δ∈R, where the certiﬁcation region Ris characterized by
x,f, and the smoothing distribution D. For instance, Cohen et al. (2019) showed that if D=N(0,σ2I),
thenRis an/lscript2-ball whose radius is determined by x,fandσ. Since then, there has been signiﬁcant progress
towards the design of Dleading to the largest Rfor all inputs x. The interplay between Rcharacterized by
/lscript1,/lscript2and/lscript∞-balls, and a notion of optimal distribution Dhas been previously studied (Yang et al., 2020).
Despite this progress, current randomized smoothing approaches provide certiﬁcation regions that are isotropic
in nature, limiting their capacity to certifying smaller and worst-case regions. We provide an intuitive example
of this behavior in Figure 1. The isotropic nature of Rin prior art is due to the common assumption that
the smoothing distribution Dis identically distributed (Yang et al., 2020; Kumar et al., 2020; Levine & Feizi,
2021). Moreover, comparisons between various randomized smoothing approaches were limited to methods
that produce the same /lscriptpcertiﬁcate, with no clear metrics for comparing with other certiﬁcates. In this
paper, we address both concerns and present new state-of-the-art certiﬁed accuracy results on both CIFAR-10
and ImageNet datasets.
Our contributions are threefold. (i)We provide a general and simpler analysis compared to prior art (Cohen
et al., 2019; Yang et al., 2020) that paves the way for the certiﬁcation of anisotropic regions characterized by
any norm, holding prior art as special cases. We then specialize our result to regions that, for a positive deﬁnite
1Under review as submission to TMLR
Anisotropic
Isotropic
(a)
Anisotropic
Isotropic (b)
Anisotropic
Isotropic (c)
Anisotropic
Isotropic (d)
Figure 1: Illustration of the landscape of fy(blue corresponds to a higher conﬁdence in y, the true label) for
a region around an input in a toy, 2-dimensional radially separable dataset. For two dataset examples, in (a)
and (b) we show the boundaries of the optimal /lscript1isotropic and anisotropic certiﬁcates, while (c) and (d) are
the boundaries of the optimal /lscript2isotropic and anisotropic certiﬁcates. A thorough discussion of this ﬁgure is
presented in Section 3.
A, are ellipsoids, i.e./bardblAδ/bardbl2≤c,c> 0, and generalized cross-polytopes, i.e./bardblAδ/bardbl1≤c, generalizing both /lscript2
(Cohen et al., 2019) and /lscript1(Lecuyer et al., 2019; Yang et al., 2020) certiﬁcation (Section 4). (ii)We introduce
a new evaluation framework to compare methods that certify general (isotropic or anisotropic) regions. We
compare two general certiﬁcates by deﬁning that a method certifying R1is superior to another certifying R2,
ifR1is a strict superset to R2. Further, we deﬁne a standalone quantitative metric as the volume of the
certiﬁed region, and specialize it for the cases of ellipsoids and generalized cross-polytopes (Section 5). (iii)
We propose AnCer, an anisotropic certiﬁcation method that performs sample-wise ( i.e.per sample in the
test set) region volume maximization (Section 6), generalizing the data-dependent, memory-based solution
from Alfarra et al. (2022). Through experiments on CIFAR-10 (Krizhevsky, 2009) and ImageNet (Deng et al.,
2009), we show that restricting AnCer’s certiﬁcation region to /lscript1and/lscript2-balls outperforms state-of-the-art /lscript1
and/lscript2results from previous works (Yang et al., 2020; Alfarra et al., 2022). Further, we show that the volume
of the certiﬁed regions are signiﬁcantly larger than all existing methods, thus setting a new state-of-the-art
in certiﬁed accuracy. We highlight that while we eﬀectively achieve state-of-the-art performance, it comes
at a high cost given the data-dependency requirements. A discussion of the limitations of the solution is
presented in Section 6.
Notation. We consider a base classiﬁer f:Rn→P (K), whereP(K)is a probability simplex over K
classes,i.e.fi≥0and1/latticetopf= 1, fori∈{1,...,K}. Further, we use (x,y)to be a sample input xand its
corresponding true label ydrawn from a test set Dt, andfyto be the output of fat the correct class. We
use/lscriptpto be the typically deﬁned /bardbl·/bardblpnorm (p≥1), and/lscriptA
por/bardbl·/bardblA,pforp={1,2}to be a composite
norm deﬁned with respect to a positive deﬁnite matrix Aas/bardblA−1/pv/bardblp.
2 Related Work
Veriﬁed Defenses. Since the discovery that DNNs are vulnerable against input perturbations (Goodfellow
et al., 2015; Szegedy et al., 2014), a range of methods have been proposed to build classiﬁers that are veriﬁably
robust (Huang et al., 2017; Gowal et al., 2019; Bunel et al., 2018; Salman et al., 2019b). Despite this progress,
these methods do not yet scale to the networks the community is interested in certifying (Tjeng et al., 2019;
Weng et al., 2018).
Randomized Smoothing. The ﬁrst works on randomized smoothing used Laplacian (Lecuyer et al.,
2019; Li et al., 2019) and Gaussian Cohen et al. (2019) distributions to obtain /lscript1and/lscript2-ball certiﬁcates,
respectively. Several subsequent works improved the performance of smooth classiﬁers by training the base
classiﬁer using adversarial augmentation (Salman et al., 2019a), regularization (Zhai et al., 2019), or general
adjustments to training routines (Jeong & Shin, 2020). Recent work derived /lscriptp-norm certiﬁcates for other
isotropic smoothing distributions (Yang et al., 2020; Levine & Feizi, 2020; Zhang et al., 2019). Concurrently,
Dvijotham et al. (2020) developed a framework to handle arbitrary smoothing measures in any /lscriptp-norm;
2Under review as submission to TMLR
x
 x+
Figure 2: Visualization of a CIFAR-10 image xand an example x+δof an imperceptible change that is not
inside the optimal isotropic certiﬁed region, but iscovered by the anisotropic certiﬁcate.
however, the certiﬁcation process requires signiﬁcant hyperparameter tuning. Similarly, Mohapatra et al.
(2020) introduces larger certiﬁcates that require higher-order information, yet do not provide a closed-form
solution. This was followed by a complementary data-dependent smoothing approach, where the parameters
of the smoothing distribution were optimized per test set sampleto maximize the certiﬁed radius at an
individual input (Alfarra et al., 2022). All prior works considered smoothing with isotropic distributions and
hence certiﬁed isotropic /lscriptp-ball regions. In this paper, we extend randomized smoothing to certify anisotropic
regions, by pairing it with a generalization of the data-dependent framework (Alfarra et al., 2022) to maximize
the certiﬁed region at each input point.
3 Motivating Anisotropic Certiﬁcates
Certiﬁcation approaches aim to ﬁnd the saferegionR, where arg maxifi(x) =arg maxifi(x+δ)∀δ∈R.
Recent randomized smoothing techniques perform this certiﬁcation by explicitly optimizing the isotropic /lscriptp
certiﬁed region around each input (Alfarra et al., 2022), obtaining state-of-the-art performance as a result.
Despite this /lscriptpoptimality, we note that any /lscriptp-norm certiﬁcate is worst-case from the perspective of that
norm, as it avoids adversary regions by limiting its certiﬁcate to the /lscriptp-closest adversary. This means that
it can only enjoy a radius that is at most equal to the distance to the closest decision boundary. However,
decision boundaries of general classiﬁers are complex, non-linear, and non-radially distributed with respect to
a generic input sample (Karimi et al., 2019). This is evidenced by the fact that, within a reasonably small
/lscriptp-ball around an input, there are often only a small set of adversary directions (Tramèr et al., 2017; 2018)
(e.g.see the decision boundaries in Figure 1). As such, while /lscriptp-norm certiﬁcates are useful to reason about
worst-case performance and are simple to obtain given previous works (Cohen et al., 2019; Yang et al., 2020;
Lee et al., 2019), they are otherwise uninformative in terms of the shape of decision boundaries, i.e.which
regions around the input are safe.
To visualize these concepts, we illustrate the decision boundaries of a base classiﬁer ftrained on a toy
2-dimensional, radially separable (with respect to the origin) binary classiﬁcation dataset, and consider two
diﬀerent input test samples (see Figure 1). We compare the optimalisotropic and anisotropic certiﬁed regions
of diﬀerent shapes at these points. In Figures 1a and 1b, we compare an isotropic cross-polytope (of the
form/bardblδ/bardbl1≤r) with an anisotropic generalized cross-polytope (of the form /bardblAδ/bardbl1≤r), while in Figures 1c
and 1d we compare an isotropic /lscript2ball (of the form /bardblδ/bardbl2≤r) with an anisotropic ellipsoid (of the form
/bardblAδ/bardbl2≤r). Notice that in Figures 1a and 1c, due to the curvature of the classiﬁcation boundary (shown in
white), the optimal certiﬁcation region is isotropic in nature, which is evidenced by the similarities of the
optimal isotropic and anisotropic certiﬁcates. On the other hand, in Figures 1b and 1d, the location of the
decision boundary allows for the anisotropic certiﬁed regions to be considerably larger than their isotropic
counterparts, as they are not as constrained by the closest decision boundary, i.e.theworst-case performance.
We note that these diﬀerences are further highlighted in higher dimensions, and we study them for a single
CIFAR-10 test set sample in Appendix A.1.
As shown, anisotropic certiﬁcation reasons more closely about the shape of the decision boundaries, allowing
for further insights into constant prediction (safe) directions. In Figure 2, we present a series of test set
imagesx, as well as practically indistinguishable x+δimages which are not inside the optimal certiﬁed
3Under review as submission to TMLR
isotropic/lscript2-balls for each input sample, yet are within the anisotropic certiﬁed regions. This showcases the
merits of using anisotropic certiﬁcation for characterizing larger safe regions.
4 Anisotropic Certiﬁcation
One of the main obstacles in enabling anisotropic certiﬁcation is the complexity of the analysis required. To
alleviate this, we follow a Lipschitz argument ﬁrst observed by Salman et al. (2019a) and Jordan & Dimakis
(2020) and propose a simple and general certiﬁcation analysis. We start with the following two observations.
All proofs are in Appendix B.
Proposition 1. Consider a diﬀerentiable function g:Rn→R. Ifsupx/bardbl∇g(x)/bardbl∗≤Lwhere/bardbl·/bardbl∗has a dual
norm/bardblz/bardbl=maxxz/latticetopxs.t./bardblx/bardbl∗≤1, thengisL-Lipschitz under norm /bardbl·/bardbl∗, that is|g(x)−g(y)|≤L/bardblx−y/bardbl.
Given the previous proposition, we formalize /bardbl·/bardblcertiﬁcation as follows:
Theorem 1. Letg:Rn→RK,gibeL-Lipschitz continuous under norm /bardbl·/bardbl∗∀i∈{1,...,K}, and
cA=argmaxigi(x). Then, we have arg maxigi(x+δ) =cAfor allδsatisfying:
/bardblδ/bardbl≤1
2L/parenleftBig
gcA(x)−max
cgc/negationslash=cA(x)/parenrightBig
.
Theorem 1 provides an /bardbl·/bardblnorm robustness certiﬁcate for any L-Lipschitz classiﬁer gunder/bardbl·/bardbl∗. The
certiﬁcate is only informative when one can attain a tight non-trivial estimate of L, ideally supx/bardbl∇g(x)/bardbl∗,
which is generally diﬃcult when gis an arbitrary neural network.
Framework Recipe. In light of Theorem 1, randomized smoothing can be viewed diﬀerently as an instance
of Theorem 1 with the favorable property that the constructed smooth classiﬁer genjoys an analytical form
forL=supx/bardbl∇g(x)/bardbl∗by design. As such, to obtain an informative /bardbl·/bardblcertiﬁcate, one must, for an arbitrary
choice of a smoothing distribution, compute the analytic Lipschitz constant Lunder/bardbl·/bardbl∗forg. While
there can exist a notion of “optimal” smoothing distribution for a given choice of /bardbl·/bardblcertiﬁcate, as in
part addressed earlier for the isotropic /lscript1,/lscript2and/lscript∞certiﬁcates (Yang et al., 2020), this is not the focus of
this paper. The choice of the smoothing distribution in later sections is inspired by previous work for the
purpose of granting anisotropic certiﬁcates. This recipe complements randomized smoothing works based on
Neyman-Pearson’s lemma (Cohen et al., 2019) or the Level-Set and Diﬀerential Method (Yang et al., 2020).
We will deploy this framework recipe to show two specializations for anisotropic certiﬁcation, namely ellipsoids
(Section 4.1) and generalized cross-polytopes (Section 4.2).1.
4.1 Certifying Ellipsoids
In this section, we consider the certiﬁcation under /lscriptΣ
2norm, or/bardblδ/bardblΣ,2=√
δ/latticetopΣ−1δ, that has a dual norm
/bardblδ/bardblΣ−1,2. Note that both /bardblδ/bardblΣ,2≤rand/bardblδ/bardblΣ−1,2≤rdeﬁne an ellipsoid. Despite that the following results
hold for any positive deﬁnite Σ, we assume for eﬃciency reasons that Σis diagonal throughout. First,
we consider the anisotropic Gaussian smoothing distribution N(0,Σ)with the smooth classiﬁer deﬁned as
gΣ(x) =E/epsilon1∼N(0,Σ)[f(x+/epsilon1)]. Considering the classiﬁer Φ−1(gΣ(x)), where Φis the standard Gaussian CDF,
and following Theorem 1 to grant an /lscriptΣ
2certiﬁcate for Φ−1(gΣ(x)), we derive the Lipschitz constant Lunder
/bardbl·/bardbl Σ−1,2, in the following proposition.
Proposition 2. Φ−1(gΣ(x))is1-Lipschitz (i.e. L= 1) under the/bardbl·/bardbl Σ−1,2norm.
Since Φ−1is a strictly increasing function, by combining Proposition 2 with Theorem 1, we have:
Corollary 1. LetcA= arg maxigi
Σ(x), then arg maxigi
Σ(x+δ) =cAfor allδsatisfying:
/bardblδ/bardblΣ,2≤1
2/parenleftBig
Φ−1(gcA
Σ(x))−Φ−1/parenleftBig
max
cgc/negationslash=cA
Σ(x)/parenrightBig/parenrightBig
.
Corollary 1 holds the /lscript2certiﬁcation from Zhai et al. (2019) as a special case for when Σ =σ2I.2
1Our analysis also grants a certiﬁcate for a mixture of Gaussians smoothing distribution (see Appendix B.1).
2A similar result was derived in the appendix of Fischer et al. (2020); Li et al. (2020) with a more involved analysis by
extending Neyman-Pearson’s lemma.
4Under review as submission to TMLR
4.2 Certifying Generalized Cross-Polytopes
Hereweconsidercertiﬁcationunderthe /lscriptΛ
1normdeﬁningageneralizedcross-polytope, i.e.theset{δ:/bardblδ/bardblΛ,1=
/bardblΛ−1δ/bardbl1≤r}, as opposed to the /lscript1-bounded set that deﬁnes a cross-polytope, i.e.{δ:/bardblδ/bardbl1≤r}. As with the
ellipsoid case and despite that the following results hold for any positive deﬁnite Λ, for the sake of eﬃciency, we
assume Λto be diagonal throughout. For generalized cross-polytope certiﬁcation, we consider an anisotropic
Uniform smoothing distribution U, which deﬁnes the smooth classiﬁer gΛ(x) =E/epsilon1∼U[−1,1]n[f(x+ Λ/epsilon1)].
Following Theorem 1 and to certify under the /lscriptΛ
1norm, we compute the Lipschitz constant of gΛunder the
/bardblΛx/bardbl∞norm, which is the dual norm of /bardbl·/bardbl Λ,1(see Appendix B), in the next proposition.
Proposition 3. The classiﬁer gΛis1/2-Lipschitz (i.e. L=1/2) under the/bardblΛx/bardbl∞norm.
Similar to Corollary 1, by combining Proposition 3 with Theorem 1, we have that:
Corollary 2. LetcA= arg maxigi
Λ(x), then arg maxigi
Λ(x+δ) =cAfor allδsatisfying:
/bardblδ/bardblΛ,1=/bardblΛ−1δ/bardbl1≤/parenleftBig
gcA
Λ(x)−max
cgc/negationslash=cA
Λ(x)/parenrightBig
.
Corollary 2 holds the /lscript1certiﬁcation from Yang et al. (2020) as a special case for when Λ =λI.
5 Evaluating Anisotropic Certiﬁcates
With the anisotropic certiﬁcation framework presented in the previous section, the question arises: “Given
two general (isotropic or anisotropic) certiﬁcation regions R1andR2, how can one eﬀectively compare them?”.
We propose the following deﬁnition to address this issue.
Deﬁnition 1. For a given input point x, consider the two certiﬁcation regions R1andR2obtained for
two classiﬁers f1andf2, i.e.A1={δ:arg maxcfc
1(x) =arg maxcfc
1(x+δ),∀δ∈R 1}andA2={δ:
arg maxcfc
2(x) =arg maxcfc
2(x+δ),∀δ∈R 2}where arg maxcfc
1(x) =arg maxcfc
2(x). We sayA1is a
"superior certiﬁcate" to A2(i.e.A1/followsA 2), if and only if,A1⊃A 2.
This deﬁnition is a natural extension from the radius-based comparison of /lscriptp-ball certiﬁcates, providing a
basis for evaluating anisotropic certiﬁcation. To compare an anisotropic to an isotropic region of certiﬁcation,
it is not immediately clear how to (i)check that an anisotropic region is a superset to the isotropic region,
and(ii)if it were a superset, how to quantify the improvement of the anisotropic region over the isotropic
counterpart. In Sections 5.1 and 5.2, we tackle these issues for the particular cases of ellipsoid and generalized
cross-polytope certiﬁcates.
5.1 Evaluating Ellipsoid Certiﬁcates
Comparing /lscript2−Balls to/lscriptΣ
2−Ellipsoids (Specialization of Deﬁnition 1). Recall that if Σ =σ2I, our
ellipsoid certiﬁcation in Corollary 1 recovers as a special case the isotropic /lscript2-ball certiﬁcation of Cohen et al.
(2019); Salman et al. (2019a); Zhai et al. (2019). Consider the certiﬁed regions R1={δ:/bardblδ/bardbl2≤˜σr1}and
R2={δ:/bardblδ/bardblΣ,2=√
δ/latticetopΣ−1δ≤r2}for givenr1,r2>0. Since we take Σ =diag({σ2
i}n
i=1), the maximum
enclosed/lscript2-ball for the ellipsoid R2is given by the set R3={δ:/bardblδ/bardbl2≤miniσir2}, and thusR2⊇R 3.
Therefore, it suﬃces that R3⊇R 1(i.e.miniσir2≥˜σr1), to say thatR2is a superior certiﬁcate to the
isotropicR1as per Deﬁnition 1.
Quantifying /lscriptΣ
2Certiﬁcates. The aforementioned specialization is only concerned with whether our
ellipsoid certiﬁed region R2is “superior” to the isotropic /lscript2-ball without quantifying it. A natural solution
is to directly compare the volumes of the certiﬁed regions. Since the volume of an ellipsoid given by R2is
V(R2) =rn
2√
πn/Γ(n/2+1)/producttextn
i=1σi(Kendall, 2004), we directly compare the proxy radius ˜Rdeﬁned forR2as
˜R=r2n/radicalbig/producttextn
iσi, since larger ˜Rcorrespond to certiﬁed regions with larger volumes. Note that ˜R, which is the
nthroot of the volume up to a constant factor, can be seen as a generalization to the certiﬁed radius in the
case whenσi=σ∀i.
5Under review as submission to TMLR
5.2 Evaluating Generalized Cross-Polytope Certiﬁcates
Comparing /lscript1−Balls to/lscriptΛ
1−Generalized Cross-Polytopes (Specialization of Deﬁnition 1). Con-
sider the certiﬁcates S1={δ:/bardblδ/bardbl1≤˜λr1},S2={δ:/bardblδ/bardblΛ,1=/bardblΛ−1δ/bardbl1≤r2}, andS3={δ:/bardblδ/bardbl1≤
miniλir2}, where we take Λ =diag({λi}n
i=1). Note that since S2⊇S3, then as per Deﬁnition 1, it suﬃces
thatS3⊇S1(i.e.miniλir2≥˜λr1) to say that the anisotropic generalized cross-polytope S2is superior to
the isotropic /lscript1-ballS1.
Quantifying /lscriptΛ
1Certiﬁcates. Following the approach proposed in the /lscriptΣ
2case, we quantitatively compare
the generalized cross-polytope certiﬁcation of Corollary 2 to the /lscript1certiﬁcate through the volumes of the two
regions. We ﬁrst present the volume of the generalized cross-polytope.
Proposition 4. V/parenleftbig
{δ:/bardblΛ−1δ/bardbl1≤r}/parenrightbig
=(2r)n
n!/producttext
iλi.
Following this deﬁnition, we deﬁne the proxy radius forS2in this case to be ˜R=r2n/radicalbig/producttextn
i=1λi. As with the
/lscript2case, larger ˜Rcorrespond certiﬁed regions with larger volumes. As in the ellipsoid case, ˜Rcan be seen as a
generalization to the certiﬁed radius when λi=λ∀i.
6 AnCer: Sample-wise Volume Maximization for Anisotropic Certiﬁcation
Given the results from the previous sections, we are now equipped to certify anisotropic regions, in particular
ellipsoids and generalized cross-polytopes. As mentioned in Section 4, these regions are generally deﬁned as
R={δ:/bardblδ/bardblΘ,p≤rp}for a given parameter of the smoothing distribution Θ =diag({θi}n
i=1), an/lscriptp-norm
(p∈{1,2}), and a gapvalue ofrp∈R+. At this point, one could simply take an anisotropic distribution
with arbitrarily chosen parameters Θand certify a trained network at any input point x, in the style of what
was done in the previous randomized smoothing literature with isotropic distributions. However, the choice
ofΘis more complex in the anisotropic case. A ﬁxed choice of anisotropic Θcould severely underperform the
isotropic case – take, for example, the anisotropic distribution of Figure 1d applied to the input of Figure 1c.
Instead of taking a ﬁxed Θ, we generalize the framework introduced by Alfarra et al. (2022), where parameters
of the smoothing distribution are optimized per input test point ( i.e.in asample-wise fashion) so as to
maximize the resulting certiﬁcate. The goal of the optimization in (Alfarra et al., 2022) is, at a point x,
to maximize the isotropic /lscript2region described in Section 4.1 ( i.e.{δ:/bardblδ/bardbl2≤σxrp(x,σx))}), whererpis
the gap and a function of xandσx∈R+. In the isotropic /lscriptpcase, this generalizes to maximizing the
region{δ:/bardblδ/bardblp≤θxrp(x,θx)}, which can be achieved by maximizing radius θxrp(x,θx)throughθx∈R+,
obtainingr∗
iso(Alfarra et al., 2022).
For the general anisotropic case, we propose AnCer, whose objective is to maximize the volume of the
certiﬁed region through the proxy radius , while satisfying the superset condition with respect to the maximum
isotropic/lscript2radius,r∗
iso. In the case of the ellipsoids and generalized cross-polytopes as presented in Sections 5.1
and 5.2, respectively, AnCer’s optimization problem can be written as:
arg max
Θxrp(x,Θx)n/radicalBigg/productdisplay
iθx
is.t. min
iθx
irp(x,Θx)≥r∗
iso (1)
whererp(x,Θx)is the gap value under the anisotropic smoothing distribution. We iteratively solve a relaxed
version of Equation equation 1 , with further details presented in Appendix C.
Memory-based Anisotropic Certiﬁcation. While each of the classiﬁers induced by the parameter Θx, i.e.
gΘx, is robust by deﬁnition as presented in Section 4, the certiﬁcation of the overall data-dependent classiﬁer
is not necessarily sound due to the optimization procedure for each x. This is a known issue in certifying
data-dependent classiﬁers, and is addressed by Alfarra et al. (2022) through the use of a memory-based
procedure. In Appendix D, we present an adapted version of this algorithm to AnCer. All subsequent
results are obtained following this procedure.
Limitations of AnCer. Given AnCer uses a memorization procedure similar to the one presented
in Alfarra et al. (2022), it incurs limitations on memory and runtime complexity. The main limitations of the
6Under review as submission to TMLR
memory-based certiﬁcation are outlined in Appendix E of Alfarra et al. (2022). The anisotropic case increases
on the complexity of the isotropic framework by the increased runtime of speciﬁc functions presented in
Appendix D. Certiﬁcation runtime comparisons are in Section 7.4.
Further, note that in memory-based data-dependent certiﬁcation there is a single procedure for both
certiﬁcation and inference in contrast with the ﬁxed σsetting from Cohen et al. (2019). While the linear
runtime dependency on memory size might appear daunting for the deployment of such a system, there are a
few factors that could mitigate the cost. Firstly, in practice the models deployed get regularly updated in
deployment, and the memory should be reset in those situations. Secondly, there are possible solutions which
might attain sublinear runtime for the post-certiﬁcation stage, such as the application of k-d trees to reduce
the space of comparisons and speed-up the process. As such, we believe AnCerto be suited to applications
in oﬄine scenarios, where improved robustness is desired and inference time is not a critical issue.
A further limitation of the memorization procedure has to do with the impact of the order in which inputs
are certiﬁed on the overall statistics obtained. Within a memory-based framework, certifying x2withx1
in memory can be diﬀerent from certifying x1withx2in memory if they intersect. In practice, given the
low number of intersections observed with the original certiﬁed regions, this eﬀect was almost negligible in
the results presented in Section 7. For fairness of comparison with non-memory based methods, we report
"worst-case" results for AnCerin which we abstain from deciding whenever an intersection of two certiﬁed
regions occurs.
7 Experiments
We now study the empirical performance of AnCer to obtain/lscriptΣ
2,/lscriptΛ
1,/lscript2and/lscript1certiﬁcates on networks
trained using randomized smoothing methods found in the literature. In this section, we show that AnCeris
able to achieve (i)improved performance on those networks in terms of /lscript2and/lscript1certiﬁcation when compared
to certiﬁcation baselines that smooth using a ﬁxed isotropic σ(Fixedσ) (Cohen et al., 2019; Yang et al.,
2020; Salman et al., 2019a; Zhai et al., 2019) or a data-dependent and memory-based isotropic one (Isotropic
DD) (Alfarra et al., 2022); and (ii)a signiﬁcant improvement in terms of the /lscriptΣ
2and/lscriptΛ
1-norm certiﬁed
region obtained by the same methods – compared by computing the proxy radius of the certiﬁed regions –
thus generally satisfying the conditions of a superior certiﬁcate proposed in Deﬁnition 1. Note that both
data-dependent approaches (Isotropic DD and AnCer) use memory-based procedures. As such, the gains
described in this section constitute a trade-oﬀ given the limitations of the method described in Section 6.
We follow an evaluation procedure as similar as possible to the ones described in Cohen et al. (2019); Yang
et al. (2020); Salman et al. (2019a); Zhai et al. (2019) by using code and pre-trained networks whenever
available and by performing experiments on CIFAR-10 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009),
certifying the entire CIFAR-10 test set and a subset of 500 examples from the ImageNet test set. For the
implementation of AnCer, we solve Equation equation 1 with Adam for 100 iterations, where the certiﬁcation
gaprp(x,Θx)is estimated at each iteration using 100noise samples per test point (see Appendix C) and
Θxin Equation equation 1 is initialized with the Isotropic DD solution from Alfarra et al. (2022). Further
details of the setup can be found in Appendix E.
As in previous works, /lscriptpcertiﬁed accuracy at radiusRis deﬁned as the portion of the test set Dtfor which
the smooth classiﬁer correctly classiﬁes with an /lscriptpcertiﬁcation radius of at least R. In a similar fashion, we
deﬁne the anisotropic /lscriptΣ
2//lscriptΛ
1certiﬁed accuracy at a proxy radius of ˜R(as deﬁned in Section 5) to be the portion
ofDtin which the smooth classiﬁer classiﬁes correctly with an /lscriptΣ
2//lscriptΛ
1-norm certiﬁcate of an nthroot volume of
atleast ˜R. Wealsoreport averagecertiﬁedradius (ACR)deﬁnedas Ex,y∼Dt[Rx 1(g(x) =y)](Alfarraetal.,
2022;Zhai etal.,2019) aswell as averagecertiﬁedproxyradius (AC˜R)deﬁnedas Ex,y∼Dt[˜Rx 1(g(x) =y)],
whereRxand ˜Rxdenote the radius and proxy radius at xwith a true label yfor a smooth classiﬁer g. Recall
that in the isotropic case, the proxy radius is, by deﬁnition, the same as the radius for a given /lscriptp-norm. For
each classiﬁer, we ran experiments on the σvalues reported in the original work (with the exception of Yang
et al. (2020), see Section 7.2). For the sake of brevity, we report in this section the top-1 certiﬁed accuracy
plots,ACRandAC˜Rper radius across σ, as in Salman et al. (2019a); Zhai et al. (2019); Alfarra et al. (2022).
The performance of each method per σis presented in Appendix G.
7Under review as submission to TMLR
0 1 2 3 4 5
2 radius
0.00.20.40.60.8Certified accuracyCIFAR-10 - Cohen
Fixed
Isotropic DD
ANCER
0 1 2 3 4 5
2 radius
0.00.20.40.60.8Certified accuracyCIFAR-10 - SmoothAdv
Fixed
Isotropic DD
ANCER
0 1 2 3 4 5
2 radius
0.00.20.40.60.8Certified accuracyCIFAR-10 - MACER
Fixed
Isotropic DD
ANCER
0 1 2 3 4
2 radius
0.00.20.40.6Certified accuracyImageNet - Cohen
Fixed
Isotropic DD
ANCER
0 1 2 3 4
2 radius
0.00.20.40.6Certified accuracyImageNet - SmoothAdv
Fixed
Isotropic DD
ANCER
0 1 2 3 4 5
2 proxy volume
0.00.20.40.60.8Certified accuracyFixed
Isotropic DD
ANCER
0 1 2 3 4 5
2 proxy volume
0.00.20.40.60.8Certified accuracyFixed
Isotropic DD
ANCER
0 1 2 3 4 5
2 proxy volume
0.00.20.40.60.8Certified accuracyFixed
Isotropic DD
ANCER
0 1 2 3 4
2 proxy radius
0.00.20.40.6Certified AccuracyFixed
Isotropic DD
ANCER
0 1 2 3 4
2 proxy radius
0.00.20.40.6Certified AccuracyFixed
Isotropic DD
ANCER
Figure 3: Distribution of top-1 certiﬁed accuracy as a function of /lscript2radius (top) and /lscriptΣ
2-norm proxy radius
(bottom) obtained by diﬀerent certiﬁcation methods on CIFAR-10 and ImageNet.
7.1 Ellipsoid certiﬁcation ( /lscript2and/lscriptΣ
2-norm certiﬁcates)
We perform the comparison of /lscript2-ball vs./lscriptΣ
2-ellipsoid certiﬁcates via Gaussian smoothing using networks
trained following the procedures deﬁned in Cohen et al. (2019), Salman et al. (2019a), and Zhai et al. (2019).
For each of these, we report results on ResNet18 trained using σ∈{0.12,0.25,0.5,1.0}for CIFAR-10, and
ResNet50 using σ∈{0.25,0.5,1.0}for ImageNet. For details of the training procedures, see Appendix E.1.
Figure 3 plots top-1 certiﬁed accuracy as a function of the /lscript2radius (top) and of the /lscriptΣ
2-norm proxy radius
(bottom) per trained network and dataset, while Table 1 presents an overview of the certiﬁed accuracy at
various/lscript2radii, as well as /lscript2ACRand/lscriptΣ
2-normAC˜R. Recall that, following the considerations in Section 5.1,
the/lscript2certiﬁcate obtained through AnCeris the maximum enclosed isotropic /lscript2-ball in the /lscriptΣ
2ellipsoid.
0.0 0.5 1.0 1.5 2.0
1 radius
0.00.20.40.60.8Certified accuracyCIFAR-10 - RS4A
Fixed
Isotropic DD
ANCER
0.00 0.25 0.50 0.75 1.00
1 radius
0.00.20.40.60.8Certified accuracyImageNet - RS4A
Fixed
Isotropic DD
ANCER
0 1 2
1 proxy radius
0.00.20.40.60.8Certified accuracyFixed
Isotropic DD
ANCER
0 1 2 3
1 proxy volume
0.00.20.40.60.8Certified accuracyFixed
Isotropic DD
ANCER
Figure 4: Distribution of top-1 certiﬁed ac-
curacy as a function of /lscript1radius (top) and
/lscriptΛ
1-norm proxy radius (bottom) obtained by
diﬀerent certiﬁcation methods on CIFAR-10
and ImageNet.First, wenotethatsample-wisecertiﬁcation(IsotropicDDand
AnCer) achieves higher certiﬁed accuracy than ﬁxed σacross
the board. This mirrors the ﬁndings in Alfarra et al. (2022),
since certifying with a ﬁxed σfor all samples struggles with
the robustness/accuracy trade-oﬀ ﬁrst mentioned in Cohen
et al. (2019), whereas the data-dependent solutions explicitly
optimizeσper sample to avoid it. More importantly, AnCer
achieves new state-of-the-art /lscript2certiﬁed accuracy at most
radii in Table 1, e.g.at radius 0.5 AnCer brings certiﬁed
accuracy to 77% (from 66%) and 70% (from 62%) on CIFAR-
10 and ImageNet, respectively, yielding relative percentage
improvements in ACRbetween 13% and 47% when compared
to Isotropic DD. While the results are signiﬁcant, it might
not be immediately clear why maximizing the volume of an
ellipsoid with AnCerresults in a larger maximum enclosed
/lscript2-ball certiﬁcate in /lscriptΣ
2ellipsoid when compared to optimizing
the/lscript2-ball with Isotropic DD. We explore this phenomenon
in Appendix 7.3.
As expected, AnCer substantially improves /lscriptΣ
2AC˜Rcom-
paredtoIsotropicDDinallcases–withrelativeimprovements
inAC˜Rbetween 38% and 63% over both datasets. The joint
results, certiﬁcation with /lscript2and/lscriptΣ
2, establish that AnCercertiﬁes the /lscript2-ball region obtained by previous
approaches, in addition to a much larger region captured by the /lscriptΣ
2certiﬁed accuracy and AC˜R, and therefore
is, according to Deﬁnition 1, generally superior to the Isotropic DD one.
8Under review as submission to TMLR
Table 1: Comparison of top-1 certiﬁed accuracy at diﬀerent /lscript2radii,/lscript2average certiﬁed radius ( ACR) and
/lscriptΣ
2average certiﬁed proxy radius ( AC˜R) obtained by using the isotropic σused for training the networks
(Fixedσ); the isotropic data-dependent (Isotropic DD) optimization scheme from Alfarra et al. (2022); and
AnCer’s data-dependent anisotropic optimization.
CIFAR-10 CertiﬁcationAccuracy @ /lscript2radius (%)/lscript2ACR/lscriptΣ
2AC˜R0.0 0.25 0.5 1.0 1.5 2.0 2.5
Cohen
Cohen et al. (2019)Fixedσ 86 71 51 27 14 6 2 0.722 0.722
Isotropic DD 82 76 62 39 24 14 8 1.117 1.117
AnCer 86 85 77 53 31 17 10 1.449 1.772
SmoothAdv
Salman et al. (2019a)Fixedσ 82 72 55 32 19 9 5 0.834 0.834
Isotropic DD 82 75 63 40 25 15 7 1.011 1.011
AnCer 83 81 73 48 30 17 8 1.224 1.573
MACER
Zhai et al. (2019)Fixedσ 87 76 59 37 24 14 9 0.970 0.970
Isotropic DD 88 80 66 40 17 9 6 1.007 1.007
AnCer 84 80 67 34 15 11 9 1.136 1.481
ImageNet CertiﬁcationAccuracy @ /lscript2radius (%)/lscript2ACR/lscriptΣ
2AC˜R0.0 0.5 1.0 1.5 2.0 2.5 3.0
Cohen
Cohen et al. (2019)Fixedσ 70 56 41 31 19 14 12 1.098 1.098
Isotropic DD 71 59 46 36 24 19 15 1.234 1.234
AnCer 70 70 62 61 42 36 29 1.810 1.981
SmoothAdv
Salman et al. (2019a)Fixedσ 65 59 44 38 26 20 18 1.287 1.287
Isotropic DD 66 62 53 41 32 24 20 1.428 1.428
AnCer 66 66 62 58 44 37 32 1.807 1.965
7.2 Generalized Cross-Polytope certiﬁcation ( /lscript1and/lscriptΛ
1-norm certiﬁcates)
To investigate /lscript1-ball vs./lscriptΛ
1-generalized cross-polytope certiﬁcation via Uniform smoothing, we compare
AnCerto the/lscript1state-of-the-art results from RS4A(Yang et al., 2020). While the authors of the original
work report best certiﬁed accuracy based on 15 networks trained at diﬀerent σlevels between 0.15and
3.5on CIFAR-10 (WideResNet40) and ImageNet (ResNet50) and due to limited computational resources,
we perform the analysis on a subset of those networks with σ={0.25,0.5,1.0}. We reproduce the results
in Yang et al. (2020) as closely as possible, with details of the training procedure presented in Appendix E.2.
Figure 4 shows the top-1 certiﬁed accuracy as a function of the /lscript1radius (top) and of the /lscriptΛ
1-norm proxy
radius (bottom) for RS4A, and Table 2 shows an overview of the certiﬁed accuracy at various /lscript1radii, as
well as/lscript1ACRand/lscriptΛ
1AC˜R. As with the ellipsoid case, we notice that AnCeroutperforms both Fixed σ
and Istropic DD for most /lscript1radii, establishing new state-of-the-art results in CIFAR-10 at radii 0.5 and 1.0,
and ImageNet at radii 0.5 (compared to previous results reported in Yang et al. (2020)). Once more and as
expected, AnCersigniﬁcantly improves the /lscriptΛ
1AC˜Rfor all radii, pointing to substantially larger cerﬁcates
than the isotropic case. These results also establish that AnCer certiﬁes the /lscript1-ball region obtained by
previous work, in addition to the larger region obtained by the /lscriptΛ
1certiﬁcate, and thus we can consider it
superior (with respect to Deﬁnition 1) to Isotropic DD.
7.3 Why does AnCer improve upon Isotropic DD’s /lscriptpcertiﬁcates?
As observed in Sections 7.1 and 7.2, AnCer’s/lscript2and/lscript1certiﬁcates outperform the corresponding certiﬁcates
obtained by Isotropic DD. To explain this, we compare the /lscript2certiﬁed region obtained by AnCer, deﬁned in
Section 6 as{δ:/bardblδ/bardbl2≤miniσx
ir(x,Σx)}, to the one by Isotropic DD deﬁned as {δ:/bardblδ/bardbl2≤σxr(x,σx)}. We
observe that the radius of both of these certiﬁcates can be separated into a σ-factor (σxvs.σx
min=miniσx
i)
and agap-factor (r(x,σx)vs.r(x,Σx)). We posit the seemingly surprising result can be attributed to the
computation of the gap-factor rusing an anisotropic, optimized distribution. However, another potential
explanation would be that AnCerbeneﬁts from a prematurely stopped initialization provided by Isotropic
DD, thus achieving a better σx
minthan the isotropic σxwhen given further optimization iterations.
9Under review as submission to TMLR
Table 2: Comparison of top-1 certiﬁed accuracy at diﬀerent /lscript1radii,/lscript1average certiﬁed radius ( ACR) and
/lscriptΛ
1average certiﬁed proxy radius ( AC˜R) obtained by using the isotropic σused for training the networks
(Fixedσ); the isotropic data-dependent (Isotropic DD) optimization scheme from Alfarra et al. (2022); and
AnCer’s data-dependent anisotropic optimization.
CIFAR-10 CertiﬁcationAccuracy @ /lscript1radius (%)/lscript1ACR/lscriptΛ
1AC˜R0.0 0.25 0.5 0.75 1.0 1.5 2.0
RS4A
Yang et al. (2020)Fixedσ 92 83 75 71 46 0 0 0.775 0.775
Isotropic DD 92 89 82 76 58 6 2 0.946 0.946
AnCer 92 90 84 80 63 6 2 0.980 1.104
ImageNet
RS4A
Yang et al. (2020)Fixedσ 78 73 67 63 0 0 0 0.683 0.683
Isotropic DD 79 76 70 65 46 0 0 0.729 0.729
AnCer 78 76 70 66 48 0 0 0.730 1.513
0 2 4
r0.00.20.40.6 t (%)
0.2 0.4 0.6x or x
min
024100+ Isotropic DD ANCER
Figure 5: Histograms of the values of the
σ-factor (left) and gap r(right) obtained by
AnCer initialized with Isotropic DD, and
Isotropic DD when allowed to run for 100
iterations more than the baseline. Vertical
lines plot the median of the data.To investigate this, we take the optimized parameters from the
Isotropic DD experiments on SmoothAdv for an initial σ=
0.25on CIFAR-10, and run the optimization step of Isotropic
DD for 100 iterations more than its default number of iterations
from Alfarra et al. (2022), so as to match the total number
of optimization steps between Isotropic DD and AnCer. The
histograms of σxorσx
minand the gap-factor r,i.e.the two
factorsfromthe /lscript2certiﬁcationresults, arepresentedinFigure5.
Whileσxfor Isotropic DD is similar in distribution to AnCer’s
σx
min, the distribution of the two gaps, r(x,σx)andr(x,Σx),
are quite diﬀerent. In particular, the AnCercertiﬁcation gap is
signiﬁcantly larger when compared to Isotropic DD, and is the
main contributor to the improvement in the /lscript2-ball certiﬁcate
ofAnCer. That is to say, AnCergenerates Σxthat is better
aligned with the decision boundaries, and hence increases the
conﬁdence of the smooth classiﬁer.
7.4 Certiﬁcation Runtime
Table 3: Average certiﬁcation time for each
sample per architecture used: (a) ResNet18
(/lscript2,/lscriptΣ
2on CIFAR-10), (b) WideResNet40
(/lscript1,/lscriptΛ
1on CIFAR-10), and (c) ResNet50
(ImageNet).
FixedσIsotropic DD AnCer
(a) 1.6s 1.8s 2.7s
(b) 7.4s 9.5s 11.5s
(c) 109.5s 136.0s 147.0sThe certiﬁcation procedures of Isotropic DD and AnCertrade-
oﬀ improved certiﬁed accuracy for runtime, since they require a
sample-wise optimization to be run prior to the Certify step
described in Cohen et al. (2019), and a memory-based step as
per Alfarra et al. (2022). The runtime of the optimization and
certiﬁcation procedures is roughly equal for /lscript1,/lscript2,/lscriptΣ
2and/lscriptΛ
1
certiﬁcation, and mostly depends on network architecture. As
such, we report the average certiﬁcation runtime for a test set
sample on an NVIDIA Quadro RTX 6000 GPU for Fixed σ,
Isotropic DD and AnCer(including the isotropic initialization
step) in Table 3. We observe that the overall run time overhead
forAnCeris not signiﬁcant as compared to its certiﬁcation gains.
8 Conclusion
We lay the theoretical foundations for anisotropic certiﬁcation through a simple analysis, propose a metric
for comparing general robustness certiﬁcates, and introduce AnCer, a certiﬁcation procedure that estimates
the parameters of the anisotropic smoothing distribution to maximize the certiﬁcate. Our experiments show
thatAnCerachieves state-of-the-art /lscript1and/lscript2certiﬁed accuracy in the data-dependent setting.
10Under review as submission to TMLR
References
Motasem Alfarra, Adel Bibi, Philip Torr, and Bernard Ghanem. Data dependent randomized smoothing.
InThe 38th Conference on Uncertainty in Artiﬁcial Intelligence , 2022. URL https://openreview.net/
forum?id=rbNL4ILsqeq . 2, 3, 6, 7, 8, 9, 10, 14, 18, 19, 20, 23, 24
Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A uniﬁed view
of piecewise linear neural network veriﬁcation. In Advances in Neural Information Processing Systems
(NeurIPS) , 2018. 2
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning (ICML) , 2019. 1, 2, 3, 4, 5, 7, 8, 9, 10, 18,
24, 25
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2009. 1, 2, 7,
23
Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, András György, Kai Xiao,
Sven Gowal, and Pushmeet Kohli. A framework for robustness certiﬁcation of smoothed classiﬁers using
f-divergences. In International Conference on Learning Representations (ICLR) , 2020. 2
Marc Fischer, Maximilian Baader, and Martin Vechev. Certiﬁed defense to image transformations via
randomized smoothing. In Advances in Neural Information Processing Systems (NeurIPS) , 2020. 4
Igor Gilitschenski and Uwe D Hanebeck. A robust computational test for overlap of two arbitrary-dimensional
ellipsoids in fault-detection of kalman ﬁlters. In 2012 15th International Conference on Information Fusion ,
2012. 20, 21
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR) , 2015. 1, 2
Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato,
Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable veriﬁed training for provably robust
image classiﬁcation. In IEEE International Conference on Computer Vision (ICCV) , 2019. 2
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
IEEE Conference on Computer Vision and Pattern Recognition , 2016. 23
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep neural networks. In
International Conference on Computer Aided Veriﬁcation (CAV) , 2017. 2
Jongheon Jeong and Jinwoo Shin. Consistency regularization for certiﬁed robustness of smoothed classiﬁers.
InAdvances in Neural Information Processing Systems (NeurIPS) , 2020. 2
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu networks. In
Advances in Neural Information Processing Systems (NeurIPS) , 2020. 4
Hamid Karimi, Tyler Derr, and Jiliang Tang. Characterizing the decision boundary of deep neural networks.
arXiv preprint arXiv:1912.11460 , 2019. 3
Maurice G Kendall. A Course in the Geometry of n Dimensions . Courier Corporation, 2004. 5, 16
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 2, 7, 14, 23
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on randomized
smoothing for certiﬁable robustness. In International Conference on Machine Learning (ICML) , 2020. 1
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed robustness
to adversarial examples with diﬀerential privacy. In IEEE Symposium on Security and Privacy (SP) , 2019.
1, 2
11Under review as submission to TMLR
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. Tight certiﬁcates of adversarial robustness
for randomly smoothed classiﬁers. arXiv preprint arXiv:1906.04948 , 2019. 3
Alexander Levine and Soheil Feizi. Robustness certiﬁcates for sparse adversarial attacks by randomized
ablation. In Association for the Advancement of Artiﬁcial Intelligence (AAAI) , 2020. 2
Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certiﬁed robustness. arXiv
preprint arXiv:2103.10834 , 2021. 1
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certiﬁed adversarial robustness with additive
noise. In Advances in Neural Information Processing Systems (NeurIPS) , 2019. 2
Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Tao Xie, Ce Zhang, and Bo Li. Provable robust learning
based on transformation-speciﬁc smoothing. arXiv preprint arXiv:2002.12398 , 2020. 4
Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-order
certiﬁcation for randomized smoothing. Advances in Neural Information Processing Systems (NeurIPS) ,
2020. 3, 28
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via
curvature regularization, and vice versa. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 14
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems (NeurIPS) . 2019. 19, 23
Lluís Ros, Assumpta Sabater, and Federico Thomas. An ellipsoidal calculus based on propagation and fusion.
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 2002. 20, 21
Hadi Salman, Jerry Li, Ilya P Razenshteyn, Pengchuan Zhang, Huan Zhang, Sébastien Bubeck, and Greg
Yang. Provably robust deep learning via adversarially trained smoothed classiﬁers. In Advances in Neural
Information Processing Systems (NeurIPS) , 2019a. 2, 4, 5, 7, 8, 9, 15, 24, 25, 27, 29
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier
to tight robust veriﬁcation of neural networks. In Advances in Neural Information Processing Systems
(NeurIPS) , 2019b. 2
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations
(ICLR), 2014. 1, 2
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer
programming. In International Conference on Learning Representations (ICLR) , 2019. 2
Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453 , 2017. 3
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. En-
semble adversarial training: Attacks and defenses. In International Conference on Learning Representations
(ICLR), 2018. 3
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S Dhillon,
and Luca Daniel. Towards fast computation of certiﬁed robustness for relu networks. In International
Conference on Machine Learning (ICML) , 2018. 2
12Under review as submission to TMLR
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing
of all shapes and sizes. In International Conference on Machine Learning (ICML) , 2020. 1, 2, 3, 4, 5, 7, 9,
10, 24, 28
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei
Wang. Macer: Attack-free and scalable robust training via maximizing certiﬁed radius. In International
Conference on Learning Representations (ICLR) , 2019. 2, 4, 5, 7, 8, 9, 24, 25
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Filling the soap bubbles: Eﬃcient
black-box adversarial certiﬁcation with non-gaussian smoothing. https://openreview.net/forum?id=
Skg8gJBFvr , 2019. 2
13Under review as submission to TMLR
A Qualitative Motivation of Anisotropic Certiﬁcation
A.1 Visualizing CIFAR-10 Optimized Isotropic vs. Anisotropic Certiﬁcates
1
2
Anistropic
Isotropic
(a)
3072
1
Anistropic
Isotropic (b)
Figure 6: Illustration of the landscape of fyfor points
around an input point x, and two projections of an
isotropic/lscript2certiﬁed region and an anisotropic /lscriptΣ
22-
normregiononaCIFAR-10datasetexampletoasubset
of two eigenvectors of the Hessian of fy(blue regions
correspond to a higher conﬁdence in y).To extend the illustration in Figure 1 to a higher
dimensional input, we now analyze an example of the
isotropic/lscript2certiﬁcation of randomized smoothing
withN(0,σ2I), whereσis optimized per input Al-
farra et al. (2022), against AnCer, certifying an
anisotropic region characterized by a diagonal /lscriptΣ
2-
norm. To do so, we consider a CIFAR-10 Krizhevsky
(2009) dataset point x, where the input is of size
(32x32x3). We perform the 2D analysis by consid-
ering the regions closest to a decision boundary. To
do so, and following Moosavi-Dezfooli et al. (2019),
we compute the Hessian of fy(x)with respect to x
whereyis the true label for xwithfclassifying x
correctly, i.e.y=arg maxifi(x). In addition to the
Hessian, we also compute its eigenvector decomposi-
tion, yielding the eigenvectors {νi},i∈{1,..., 3072}
ordered in descending order of the absolute value of
the respective eigenvalues. In Figure 6a, we show
the projection of the landscape of fyin the highest
curvature directions, i.e.ν1andν2. Note that the
isotropic certiﬁcation, much as in Figure 1c, in these 2 dimensions is nearly optimal when compared to the
anisotropic region. However, if we take the same projection with respect to the eigenvectors with the lowest
and highest eigenvalues, i.e.ν1andν3072, the advantages of the anisotropic certiﬁcation become clear as
shown in Figure 6b.
B Anisotropic Certiﬁcation and Evaluation Proofs
Proposition 1 (restatement) .Consider a diﬀerentiable function g:Rn→R. Ifsupx/bardbl∇g(x)/bardbl∗≤Lwhere
/bardbl·/bardbl∗has a dual norm /bardblz/bardbl=maxxz/latticetopxs.t./bardblx/bardbl∗≤1, thengisL-Lipschitz under norm /bardbl·/bardbl∗, that is
|g(x)−g(y)|≤L/bardblx−y/bardbl.
Proof.Consider some x,y∈Rnand a parameterization in tasγ(t) = (1−t)x+ty∀t∈[0,1]. Note that
γ(0) =xandγ(1) =y. By the Fundamental Theorem of Calculus we have:
|g(y)−g(x)|=|g(γ(1))−g(γ(0))|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay1
0dg(γ(t))
dtdt/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay1
0∇g/latticetop∇γdt/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/integraldisplay1
0/vextendsingle/vextendsingle∇g/latticetop∇γ/vextendsingle/vextendsingledt
≤/integraldisplay1
0/bardbl∇g(x)/bardbl∗/bardbl∇γ(t)/bardbldt≤L/bardbly−x/bardbl
Theorem 1 (restatement) .Letg:Rn→RK,gibeL-Lipschitz continuous under norm /bardbl·/bardbl∗∀i∈{1,...,K},
andcA=argmaxigi(x). Then, we have arg maxigi(x+δ) =cAfor allδsatisfying:
/bardblδ/bardbl≤1
2L/parenleftBig
gcA(x)−max
cgc/negationslash=cA(x)/parenrightBig
.
Proof.TakecB= arg maxcgc/negationslash=cA(x). By Proposition 1, we get:
|gcA(x+δ)−gcA(x)|≤L/bardblδ/bardbl=⇒gcA(x+δ)≥gcA(x)−L/bardblδ/bardbl
|gcB(x+δ)−gcB(x)|≤L/bardblδ/bardbl=⇒gcB(x+δ)≤gcB(x) +L/bardblδ/bardbl
14Under review as submission to TMLR
By subtracting the inequalities and re-arranging terms, we have that as long as gcA(x)−L/bardblδ/bardbl>gcB(x)+L/bardblδ/bardbl,
i.e.the bound in the Theorem, then gcA(x+δ)>gcB(x+δ), completing the proof.
Proposition 2 (restatement) .ConsidergΣ(x) =E/epsilon1∼N(0,Σ)[f(x+/epsilon1)].Φ−1(gΣ(x))is1-Lipschitz (i.e. L= 1)
under the/bardbl·/bardbl Σ−1,2norm.
Proof.To prove Proposition 2, one needs to show that Φ−1(gi
Σ(x))∀iis 1-Lipschitz under the /bardbl·/bardblΣ−1,2norm.
For ease of notation, we drop the superscript gi
Σand use only g. We want to show that /bardbl∇Φ−1(gΣ(x))/bardblΣ−1,2=
/bardblΣ1/2∇Φ−1(gΣ(x))/bardbl2≤1. Following the argument presented in Salman et al. (2019a), it suﬃces to show that,
for any unit norm direction uandp=gΣ(x), we have:
u/latticetopΣ1
2∇gΣ(x)≤1√
2πexp/parenleftbigg
−1
2(Φ−1(p))2/parenrightbigg
. (2)
We start by noticing that:
u/latticetopΣ1
2∇gΣ(x) =1
(√
2π)n/radicalbig
|Σ|/integraldisplay
Rnf(t)u/latticetopΣ1
2Σ−1(t−x) exp/parenleftbigg
−1
2(x−t)Σ−1(x−t)/parenrightbigg
dnt
=Es∼N(0,I)[f(x+ Σ1
2s)u/latticetops] =Ev∼N(0,Σ)[f(x+v)u/latticetopΣ−1
2v].
We now need to ﬁnd the optimal f∗:Rn→[0,1]that satisﬁes gΣ(x) =Ev∼N(0,Σ)[f(x+v)] =pwhile
maximizing the left hand size Ev∼N(0,Σ)[f(x+v)u/latticetopΣ−1
2v]. We argue that the maximizer is the following
function:
f∗(x+v) = 1/braceleftBig
u/latticetopΣ−1
2v≥−Φ−1(p)/bracerightBig
.
To prove that f∗is indeed the optimal maximizer, we ﬁrst show feasibility. (i): It is clear that f∗:Rn→[0,1].
(ii) Note that:
Ev∼N(0,Σ))/bracketleftBig
1/braceleftBig
u/latticetopΣ−1
2v≥−Φ−1(p)/bracerightBig/bracketrightBig
=Px∼N(0,1)(x≥−Φ−1(p)) = 1−Φ(−Φ−1(p)) =p.
To show the optimality of f∗, we show that it attains the right upper bound:
Ev∼N(0,Σ))/bracketleftBig
u/latticetopΣ−1
2v 1/braceleftBig
u/latticetopΣ−1
2v≥−Φ−1(p)/bracerightBig/bracketrightBig
=Ex∼N(0,1)/bracketleftBig
x 1/braceleftbig
x≥−Φ−1(p)/bracerightbig/bracketrightBig
=1√
2π/integraldisplay∞
−Φ−1(p)xexp/parenleftbigg
−1
2x2/parenrightbigg
dx
=1√
2πexp/parenleftbigg
−1
2(Φ−1(p))2/parenrightbigg
obtaining the bound from Equation equation 2, and thus completing the proof.
Proposition 3 (restatement) .ConsidergΛ(x) =E/epsilon1∼U[−1,1]n[f(x+ Λ/epsilon1)]. The classiﬁer gi
Λ∀iis1/2-Lipschitz
(i.e.L=1/2) under the/bardblΛx/bardbl∞norm.
Proof.We begin by observing that the dual norm of /bardblx/bardblΛ,1=/bardblΛ−1x/bardbl1is/bardblx/bardbl∗=/bardblΛx/bardbl∞, since:
max
/bardblΛ−1x/bardbl1≤1x/latticetopy= max
/bardblz/bardbl1≤1y/latticetopΛz=/bardblΛy/bardbl∞.
Without loss of generality, we analyze ∂gi/∂x1. Let ˆx= [x2,...,xn]∈Rn−1, then:
λ1∂gi
∂x1=λ1
(2λ)n∂
∂x1/integraldisplay
[−1,1]n−1/integraldisplay1
−1fi(x1+λ1/epsilon11,ˆx+ˆΛˆ/epsilon1)d/epsilon11dn−1ˆ/epsilon1
=1
2n/integraldisplay
[−1,1]n−1(fi(x1+ 1,ˆx+ˆΛˆ/epsilon1)−fi(x1−1,ˆx+ˆΛˆ/epsilon1))dn−1ˆ/epsilon1
15Under review as submission to TMLR
Thus,/vextendsingle/vextendsingle/vextendsingle/vextendsingleλ1∂gi
∂x1/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2n/producttextn
j=2λj/integraldisplay
[−1,1]n−1/vextendsingle/vextendsingle/vextendsinglefi(x1+ 1,ˆx+ˆΛˆ/epsilon1)−fi(x1−1,ˆx+ˆΛˆ/epsilon1)/vextendsingle/vextendsingle/vextendsingledn−1ˆ/epsilon1≤1
2.
The second and last steps follow by the change of variable t=x1+λ1/epsilon11and Leibniz rule. Following
a symmetric argument,/vextendsingle/vextendsingleλj∂gi/∂xj/vextendsingle/vextendsingle≤1/2∀iresulting in having /bardblΛ∇gi(x)/bardbl∞=maxiλi/vextendsingle/vextendsingle∂gi/∂xi/vextendsingle/vextendsingle≤1/2∀i
concluding the proof.
Proposition 4 (restatement) .V/parenleftbig
{δ:/bardblΛ−1δ/bardbl1≤r}/parenrightbig
=(2r)n
n!/producttext
iλi.
Proof.TakeA=rΛ−1=diag(1/rλ1,..., 1/rλn) =diag(a1,...,an).
We can re-write the region as {x:/summationtext
iai|xi|≤1}, from which it is clear to see that this region is an origin
centered, axis-aligned simplex with the set of vertices V={±1/aiei}n
i=1, where eiis the standard basis vector
i.
Deﬁne the sets of vertices Vt=V\{− 1/anen}andVb=V\{ 1/anen}. Given the symmetry around the
origin, each of these sets deﬁnes an n-dimensional hyperpyramid with a shared baseBn−1given by the
n−1-dimensional hyperplane deﬁned by all vertices where xn= 0, and an apexat the vertex 1/anen(or
−1/anenin the case ofVb). The volume of each of these n−1-dimensional hyperpyramids is given by
V(Bn−1)/nan(Kendall (2004)), yielding a total volume of Vn=2
n1
anV(Bn−1). The same argument can be
applied to compute V(Bn−1)which is a union of two n−1-dimensional hyperpyramids. This forms a recursion
that completes the proof.
Proof.(Alternative Proof .) We consider the case that Λ−1is a general positive deﬁnite matrix that is not
necessarily diagonal. Note that V/parenleftbig
{δ:/bardblΛ−1δ/bardbl1≤r}/parenrightbig
=V/parenleftbig
{δ:/bardbl(rΛ)−1δ/bardbl1≤1}/parenrightbig
=rn|Λ|V({δ:/bardblδ/bardbl1≤1})
where|rΛ|denotes the determinant. The last equality follows by the volume of a set under a linear map and
noting that{δ:/bardbl(rΛ)−1δ/bardbl1≤1}={rΛδ:/bardblδ/bardbl1≤r}. At last,{δ:/bardblδ/bardbl1≤1}can be expressed as the disjoint
union of 2nsimplexes. Thus, we have V/parenleftbig
{δ:/bardblΛ−1δ/bardbl1≤r}/parenrightbig
=(2r)n/n!|Λ|since the volume of a simplex is
1/n!completing the proof.
For completeness, we supplement the previous result with bounds on the volume that may be useful for
future readers.
Proposition 5. For any positive deﬁnite Λ−1∈Rn×n, we have the following:
/parenleftbigg2r
n/parenrightbiggn
V/parenleftBig
Z(Λ)/parenrightBig
≤V/parenleftBig
{δ:/bardblΛ−1δ/bardbl1≤r}/parenrightBig
≤(2r)nV(Z(Λ))
whereV(Z(Λ)) =/radicalbig
|Λ/latticetopΛ|which is the volume of the zonotope with a generator matrix Λ.
Proof.LetS1={δ:/bardblΛ−1δ/bardbl1≤r},S∞={δ:/bardblΛ−1δ/bardbl∞≤r}andSn
∞={δ:n/bardblΛ−1δ/bardbl∞≤r}. Since
/bardblΛ−1δ/bardbl∞≤/bardblΛ−1δ/bardbl1≤n/bardblΛ−1δ/bardbl∞, thenS∞⊇S1⊇Sn
∞. Therefore, we have V(S∞)≥V(S1)≥V(Sn
∞). At
last note that, Sn
∞={r
nΛδ:/bardblδ/bardbl∞≤1}and that with the change of variables δ= 2u−1nwhere 1nis a
vector of all ones, we have Sn
∞=Z/parenleftbig2r
nΛ/parenrightbig
⊕−r
nΛ1nwhere⊕is a Minkowski sum and noting thatr
nΛ1nis a
single point in Rn. Therefore,V/parenleftBig
Z/parenleftBig
2r
nΛ/parenrightBig
⊕−r
nΛ1n/parenrightBig
=(2r/n)nV(Z(Λ)). The upper bound follows with a
similar argument completing the proof.
B.1 Certiﬁcation under Gaussian Mixture Smoothing Distribution
We consider a general, K-component, zero-mean Gaussian mixture smoothing distribution Gsuch that:
G({αi,Σi}K
i=1) :=K/summationdisplay
i=1αiN(0,Σi),s.t./summationdisplay
iαi= 1,0<αi≤1 (3)
16Under review as submission to TMLR
Givenfand as per the recipe described in Section 4, we are interested in the Lipschitz constant of the smooth
classiﬁergG(x) = (f∗G)(x) =/summationtextK
iαigΣi=/summationtextK
iαi(f∗N(0,Σi)) =/summationtext
iαigΣi(x)wheregΣiis deﬁned as in
the Gaussian case.
Note the weaker bound when compared to Proposition 2, for each of the Gaussian components presented in
the following proposition.
Proposition 6. gΣis/radicalbig
2/π-Lipschitz under/bardbl./bardblΣ−1,2norm.
Proof.Following a similar argument to the proof of Proposition 2, we get:
u/latticetopΣ1
2∇gΣ(x)≤1
(2π)n/2/radicalbig
|Σ|/integraldisplay
Rn|u/latticetopΣ−1
2(t−x)|exp/parenleftbigg
−1
2(x−t)/latticetopΣ−1(x−t)/parenrightbigg
dnt
=Es∼N(0,I)/bracketleftbig
|u/latticetops|/bracketrightbig
=Ev∼N(0,1)[|v|] =/radicalbig
2/π.
With Proposition 6, we obtain a Lipschitz constant for a Gaussian mixture smoothing distribution as:
Proposition 7. gGis/radicalbig
π/2-Lipschitz under/bardblδ/bardblB−1,2norm, whereB−1=/summationtextK
iαiΣ−1
i.
Proof.
|gG(x+δ)−gG(x)|≤/summationdisplay
iαi|gΣi(x+δ)−gΣi(x)|
≤/radicalbiggπ
2/summationdisplay
iαi/bardblδ/bardblΣi,2≤/radicalbiggπ
2/radicaltp/radicalvertex/radicalvertex/radicalbtδ/latticetop/parenleftBigg/summationdisplay
iαiΣ−1
i/parenrightBigg
δ=/radicalbiggπ
2/bardblδ/bardblB,2,
Obtained by ﬁrst applying the triangle inequality, then Proposition 2 followed by Jensen’s inequality.
Thus yielding the following certiﬁcate by combining Proposition 7 and Theorem 1.
Corollary 3. LetcA= arg maxigG(x), then arg maxigi
G(x+δ) =cAfor allδsatisfying:
/bardblδ/bardblB,2≤1√
2π/parenleftBig
gcA
G(x)−max
cgc/negationslash=cA
G(x)/parenrightBig
.
whereB−1=/summationtextK
iαiΣ−1
i.
C AnCer Optimization
In this section we detail the implementation choices required to solving Equation equation 1. For ease of
presentation, we restate the AnCeroptimization problem (with Θx=diag({θx
i}n
i=1)):
arg max
Θxrp(x,Θx)n/radicalBigg/productdisplay
iθx
is.t. min
iθx
irp(x,Θx)≥r∗
iso,
whererp(x,Θx)is the gap value under the anisotropic smoothing distribution, and r∗
isois the optimal isotropic
radius, i.e. ¯θxrp(x,¯θx)for¯θx∈R+. This is a nonlinear constrained optimization problem that is challenging
to solve. As such, we relax it, and solve instead:
arg max
Θxrp(x,Θx)n/radicalBigg/productdisplay
iθx
i+κmin
iθx
irp(x,Θx)s.t.θx
i≥¯θx
given a hyperparameter κ∈R+. While the constraint θx
i≥¯θxis not explicitly required to enforce the
superset condition over the isotropic case, it proved itself beneﬁcial from an empirical perspective. To sample
17Under review as submission to TMLR
from the distribution parameterized by Θx(in our case, either a Gaussian or Uniform), we make use of the
reparameterization trick , as in Alfarra et al. (2022). The solution of this optimization problem can be found
iteratively by performing projected gradient ascent.
A standalone implementation for the AnCeroptimization stage is presented in Listing 1, whereas the full
code integrated in our code base is available as supplementary material. To perform certiﬁcation, we simply
feed the output of this optimization to the certiﬁcation procedure from Cohen et al. (2019).
import torch
from torch . autograd import Variable
from torch . distributions . normal import Normal
class Certificate ():
def compute_proxy_gap (self , logits : torch . Tensor ):
raise NotImplementedError
def sample_noise (self , batch : torch . Tensor , repeated_theta : torch . Tensor ):
raise NotImplementedError
def compute_gap (self , pABar : float ):
raise NotImplementedError
class L2Certificate ( Certificate ):
def __init__ (self , batch_size : int , device : str = " cuda :0"):
self .m = Normal ( torch . zeros ( batch_size ).to( device ),
torch . ones ( batch_size ).to( device ))
self . device = device
self . norm = "l2"
def compute_proxy_gap (self , logits : torch . Tensor ):
return self .m. icdf ( logits [:, 0]. clamp_ (0.001 , 0.999) ) - \
self .m. icdf ( logits [:, 1]. clamp_ (0.001 , 0.999) )
def sample_noise (self , batch : torch . Tensor , repeated_theta : torch . Tensor ):
return torch . randn_like (batch , device = self . device ) * repeated_theta
def compute_gap (self , pABar : float ):
return norm .ppf ( pABar )
class L1Certificate ( Certificate ):
def __init__ (self , device =" cuda :0"):
self . device = device
self . norm = "l1"
def compute_proxy_gap (self , logits : torch . Tensor ):
return logits [:, 0] - logits [:, 1]
def sample_noise (self , batch : torch . Tensor , repeated_theta : torch . Tensor ):
return 2 * ( torch . rand_like (batch , device = self . device ) - 0.5) * repeated_theta
def compute_gap (self , pABar : float ):
return 2 * ( pABar - 0.5)
def ancer_optimization (
model : torch .nn. Module , batch : torch . Tensor ,
certificate : Certificate , learning_rate : float ,
isotropic_theta : torch . Tensor , iterations : int ,
samples : int , kappa : float , device : str = " cuda :0"):
""" Optimize batch using ANCER , assuming isotropic initialization point .
Args :
model : trained network
batch : inputs to certify around
certificate : instance of desired certification object
learning_rate : optimization learning rate for ANCER
isotropic_theta : initialization isotropic value per input in batch
iterations : number of iterations to run the optimization
samples : number of samples per input and iteration
kappa : relaxation hyperparameter
"""
batch_size = batch . shape [0]
img_size = np. prod ( batch . shape [1:])
# define a variable , the optimizer , and the initial sigma values
theta = Variable ( isotropic_theta , requires_grad = True ).to( device )
18Under review as submission to TMLR
optimizer = torch . optim . Adam ([ theta ], lr= learning_rate )
initial_theta = theta . detach (). clone ()
# reshape vectors to have ‘‘samples ‘‘ per input in batch
new_shape = [ batch_size * samples ]
new_shape . extend ( batch [0]. shape )
new_batch = batch . repeat ((1 , samples , 1, 1)). view ( new_shape )
# solve iteratively by projected gradient ascend
for _ in range ( iterations ):
theta_repeated = theta . repeat (1, samples , 1, 1). view ( new_shape )
# Reparameterization trick
noise = certificate . sample_noise ( new_batch , theta_repeated )
out = model (
new_batch + noise
). reshape ( batch_size , samples , -1). mean ( dim =1)
vals , _ = torch . topk (out , 2)
gap = certificate . compute_proxy_gap ( vals )
prod = torch . prod (
( theta . reshape ( batch_size , -1)) **(1/ img_size ), dim =1)
proxy_radius = prod * gap
radius_maximizer = - (
proxy_radius .sum () +
kappa *
( torch . min ( theta . view ( batch_size , -1) , dim =1) . values *gap).sum ()
)
radius_maximizer . backward ()
optimizer . step ()
# project to the initial theta
with torch . no_grad ():
torch .max(theta , initial_theta , out = theta )
return theta
Listing 1: Python implementation of the AnCeroptimization routine using PyTorch Paszke et al. (2019)
D Memory-based Certiﬁcation for AnCer
To guarantee the soundness of the AnCer classiﬁer, we use an adapted version of the data-dependent
memory-based solution presented in Alfarra et al. (2022). The modiﬁed algorithm involves a post-processing
certiﬁcation step that obtains adjusted certiﬁcation statistics based on the memory procedure from Alfarra
et al. (2022) (see the original paper for more details). We present an adapted version to AnCer of this
post-processing memory-based step in Algorithm 1.
Algorithm 1: Memory-Based Certiﬁcation
Input:input point xN+1, certiﬁed regionRN+1, predictionCN+1, and memoryM
Result: Prediction for xN+1and certiﬁed region at xN+1that does not intersect with any certiﬁed
region inM.
for(xi,Ci,Ri)∈Mdo
ifCN+1/negationslash=Cithen
ifxN+1∈Rithen
return Abstain , 0
else if MaxIntersect (RN+1,Ri) and Intersect (RN+1,Ri)then
R/prime
N+1=LargestOutSubset (Ri,RN+1);
RN+1←R/prime
N+1;
end
add(xN+1,CN+1,RN+1)toM;
returnCN+1,RN+1;
Note that the proposed certiﬁed region RN+1emerges from our certiﬁcation bounds presented in Sections 4.1
and 4.2. There are a few diﬀerences between our proposed Algorithm 1 with respect to the original variant
19Under review as submission to TMLR
presented in Alfarra et al. (2022). The ﬁrst is that we remove the computation of the largest certiﬁable subset
of a certiﬁed region RN+1when there exists an isuch thatxN+1∈Riwith a diﬀerent class prediction, i.e.
(LargestInSubset in Alfarra et al. (2022)) due to the complexity of the operation in the anisotropic case. As
an example, it is generally diﬃcult to ﬁnd the largest volume ellipsoid contained in another ellipsoid. Due to
this complexity, we choose to simply Abstain instead. Given the high dimensionality of the data, empirically,
we never found a certiﬁcate in this situation within our experiments. Further, to ease the computational
burden of the Intersect function, we introduce and instantiate the function MaxIntersect ﬁrst which
checks whether the /lscriptp-ball over-approximation of the region RN+1intersects with a /lscriptpover-approximation
ofRi. This follows since when the /lscriptpballs over-approximation to the anisotropic regions RN+1andRido
not intersect, then RN+1andRido not intersect either. Only in cases in which those over-approximation
regions intersect, we run the more expensive Intersect procedure. We present practical implementations
forMaxIntersect ,Intersect and LargestOutSubset for the ellipsoids and generalized cross-polytopes
considered in this paper.
D.1 Implementing MaxIntersect(RA,RB)in the Ellipsoid and Generalized Cross-Polytope Cases
Given the two regions RAandRB, consider/lscriptp-ball approximations of those regions, R˜A={x∈Rn:
/bardblx−a/bardblp≤ra}andR˜B={x∈Rn:/bardblx−b/bardblp≤rb}such thatRA⊆R ˜AandRB⊆R ˜B.
Lemma 1. If/bardbla−b/bardblp>ra+rb, thenRA∩RB=∅.
Proof.For the sake of contradiction, let /bardbla−b/bardblp>ra+rbandx∈R ˜A∩R ˜B. Then, we have that /bardblx−a/bardbl≤ra
and/bardblx−b/bardbl≤rb. However:
ra+rb</bardbla−b/bardblp≤/bardblx−a/bardblp+/bardblx−b/bardblp≤ra+rb,
forming a contradiction. Thus, R˜A∩R ˜B=∅, which in turn implies RA∩RB=∅sinceRAandRBare
subsets ofR˜AandR˜B, respectively.
This forms a fast, maximum intersection check for ellipsoids, i.e.p= 2, and generalized cross-polytopes, i.e.
p= 1. The MaxIntersect function returns Falseif/bardbla−b/bardblp>ra+rb, and Trueotherwise.
D.2 Implementing Intersect (RA,RB) in the Ellipsoid Case
The problem of eﬃciently checking if two ellipsoids intersect is not trivial. We rely on the work of Ros
et al. (2002); Gilitschenski & Hanebeck (2012) with missing proofs from Gilitschenski & Hanebeck (2012) for
completeness.
Lemma 2. LetRA={x∈Rn: (x−a)/latticetopA(x−a)≤1}andRB={x∈Rn: (x−b)/latticetopB(x−b)≤1}deﬁne two
ellipsoids centered at aandb, respectively. We have that R={x:t(x−a)/latticetopA(x−a)+(1−t)(x−b)/latticetopB(x−b)≤1}
for anyt∈[0,1]satisﬁesRA∩RB⊆R⊆R A∪RB.
Proof.By considering the convex combination of the left-hand side of the inequalities deﬁning the regions
RAandRB, it becomes obvious that x∈RA∩RB=⇒x∈R, concluding the left side of the property. As
for the right side, it suﬃces to show that if x /∈RAandx∈Rthenx∈RBand, similarly, that if x /∈RB
andx∈Rthenx∈RA. We show the ﬁrst case since the second follows by symmetry. Without loss of
generality, we assume that a=b=0n. Now, letxbe such that x/latticetopAx>1andtx/latticetopAx+ (1−t)x/latticetopBx≤1
sincex /∈RAandx∈R. Then, since x∈R, we have that (1−t)x/latticetopBx≤1−tx/latticetopAx≤1sincex/latticetopAx>1
which implies that x∈RB.
Note that the previous result holds without loss of generality when for the radius 1as the radius can be
absorbed in AandB. As the following Lemma was shown by Gilitschenski & Hanebeck (2012) without
proof, we complement it below for completeness.
Lemma 3. The setRis equivalent to the following ellipsoid R={x: (x−m)/latticetopEt(x−m)≤K(t)}where
Et=tA+ (1−t)B,m=E−1
t(tAa+ (1−t)Bb), andK(t) = 1−ta/latticetopAa−(1−t)b/latticetopBb+m/latticetopEtm.
20Under review as submission to TMLR
Proof.
t(x−a)/latticetopA(x−a) + (1−t)(x−b)/latticetopB(x−b)≤1
⇔x/latticetop(tA+ (1−t)B)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Etx−2x/latticetop(tAa+ (1−t)Bb)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Etm≤1−ta/latticetopAa−(1−t)b/latticetopBb
⇔(x−m)/latticetopEt(x−m)≤1−ta/latticetopAa−(1−t)b/latticetopBb+m/latticetopEtm
The last equality follows by adding and subtracting m/latticetopEtmand concluding the proof.
Proposition 8. The set of points satisfying Rfort∈(0,1)is either an empty set, a single point, or the
ellipsoidR.
Proof.We ﬁrst observe that since AandBare positive deﬁnite, then Etis positive deﬁnite. Then observe
that for a choice of t∈(0,1)such thatK(t)<0, the setRis an empty set, and since R⊇R A∩RB, the two
sets do not intersect. If K(t) = 0, then the only point satisfying Ris the center at m. Following a similar
argument, then the two ellipsoids intersect at a point. At last for a choice of tsuch thatK(t)>0, thenR
deﬁnes an ellipsoid.
As per Theorem 8, it suﬃces to ﬁnd some t∈[0,1]under which K(t)<0to guarantee that the ellipsoids do
not intersect. To that end, we solve the following convex optimization problem: t∗=argmint∈[0,1]K(t)and
check the condition if K(t∗)<0. Moreover, as shown by Ros et al. (2002); Gilitschenski & Hanebeck (2012)
K(t)is convex in the domain t∈(0,1). With several algebraic manipulations, one can show that K(t)has
the following equivalent forms:
K(t) = 1−ta/latticetopAa−(1−t)b/latticetopBb+m/latticetopEtm
K(t) = 1−t(1−t)(b−a)/latticetopBE−1
tA(b−a)
K(t) = 1−(b−a)/latticetop/parenleftbigg1
1−tB−1+1
tA−1/parenrightbigg−1
(b−a)
Observe that for ANCER, we have that both AandBto be diagonals with diagonal elements {Aii}n
i=1and
{Bii}n
i=1, respectively, resulting in the following simple form for K(t):
K(t) = 1−n/summationdisplay
i=1(bi−ai)2t(1−t)AiiBii
tAii+ (1−t)Bii.
The Intersect function in the ellipsoid case returns Falseif there exists a t∈(0,1)such thatK(t)<0,i.e.
ellipsoids do not intersect, and Trueotherwise.
D.3 Implementing Intersect (RA,RB) in the Generalized Cross-Polytope Case
LetRAandRBbe two generalized cross-polytopes RA={x∈Rn:/bardblA(x−a)/bardbl1≤1}andRB={x∈
Rn:/bardblB(x−b)/bardbl1≤1}, where AandBare positive deﬁnite diagonal matrices with elements {Aii}n
i=1and
{Bii}n
i=1, respectively. We are interested in deciding whether RAandRBintersect. However, given the
conservative context in which Intersect is used in Algorithm 1, we only need to make sure that the function
only returns Falseif it is guaranteed that RA∩RB=∅.
As such, we are able to simplify the complex problem of generalized cross-polytope intersection to the much
simpler one of ellipsoid over-approximation intersection. We do this by considering the over-approximation,
i.e.superset, ellipsoids R˜A={x∈Rn:/bardblA(x−a)/bardbl2≤1}andR˜B={x∈Rn:/bardblB(x−b)/bardbl2≤1}, and
perform the ellipsoid intersection check presented in Appendix D.2. If R˜A∩R ˜B=∅, then this implies
thatRA∩RB=∅and we can safely return False. Otherwise, we conservatively assume the generalized
cross-polytopes intersect, and return True, triggering the reduction procedure detailed in Appendix D.5.
21Under review as submission to TMLR
D.4 Implementing LargestOutSubset (RA,RB) in the Ellipsoid Case
Given two ellipsoids RA={x∈Rn: (x−a)/latticetopA(x−a)≤1}andRB={x∈Rn: (x−b)/latticetopB(x−b)≤1}that
do intersect where AandBare positive deﬁnite diagonal matrices, the task is to ﬁnd the largest possible
ellipsoidR˜Bcentered at bsuch thatR˜B⊆RBwhereRA∩R ˜B=∅.
Finding a maximum ellipsoid that satisﬁes those conditions is not trivial, so instead we consider a maximum
enclosing/lscript2-ball ofRB,R˜B={x∈Rn:/bardblx−b/bardbl2≤r}, that does not intersect RA. To obtain this ball, we
project the center of RB,b, to the ellipsoidRA. Particularly, we formulate the problem as the projection of
a vectory=b−aonto an ellipsoid with the same shape as RAcentered at 0n. This is equivalent to solving
the following optimization problem for a symmetric positive deﬁnite matrix A:
min
x1
2/bardblx−y/bardbl2
2s.t.x/latticetopAx≤1.
Note that the objective function is convex, and the constraint forms a convex set. Forming the Lagrangian to
this problem, we obtain:
L(x,λ) =1
2/bardblx−y/bardbl2
2+λ/parenleftbig
x/latticetopAx−1/parenrightbig
,
whereλ>0. Therefore, the global optimal solution must satisfy the KKT conditions below:
∂L
∂x= 0→x∗= (2λA+I)−1y,
∂L
∂λ= 0→y/latticetop(2λA+I)−/latticetopA(2λA+I)−1y−1/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
f(λ)= 0.
Thus, to project the vector yon our region the ellipsoid characterized by A, one needs to solve the scalar
optimization f(λ) = 0then substitute back in the formula of x∗. Further, given A=diag(A11,...,Ann), we
can simplify the problem to:
f(λ) =n/summationdisplay
i=1y2
iAii
(1 + 2λAii)2−1 = 0.
Oncex∗is obtained, we can deﬁne the maximum radius of the /lscript2-ball centered at bthat does not intersect
RAas:
r∗=/bardbl(x∗+a)−b/bardbl2−/epsilon1,
for an arbitrarily small /epsilon1. Finally, we obtain R˜Bas the maximum ball contained within RBthat has a radius
smaller than r∗, that is:
R˜B={x∈Rn:/bardblx−b/bardbl2≤min{r∗,min
iBii}}.
Note that while choosing the radius of R˜Bto ber∗guarantees thatR˜B∩RA=∅, this does not guarantee
thatR˜B⊆RB. To guarantee both properties, we take the minimum of both r∗andminiBii. This approach
ﬁnds the solution to the projection of the point to the ellipsoid {x∈Rn:x/latticetopAx≤1}; it does not work for
the case in which b∈RA, since the problem would be trivially solved by setting x∗=y. Thus, our classiﬁer
must abstain in that situation.
D.5 Implementing LargestOutSubset (RA,RB) in the Generalized Cross-Polytope Case
LetRAandRBbe two generalized cross-polytopes RA={x∈Rn:/bardblA(x−a)/bardbl1≤1}andRB={x∈
Rn:/bardblB(x−b)/bardbl1≤1}, where AandBare positive deﬁnite diagonal matrices with elements {Aii}n
i=1and
{Bii}n
i=1, respectively. The task is to ﬁnd the largest possible generalized cross-polytope R˜Bcentered at b
such thatR˜B⊆RBwhereRA∩R ˜B=∅.
As with the ellipsoid case, solving this problem for a generalized cross-polytope is not trivial, so instead we
consider a maximum enclosing cross-polytope (i.e., /lscript1-ball) ofR˜B={x∈Rn:/bardblx−b/bardbl1≤r}that does not
22Under review as submission to TMLR
intersectRAand is a subset of RB. To obtain this /lscript1-ball, we project the center of RB,b, to the generalized
cross-polytopeRAin a similar fashion to the ellipsoid case in Appendix D.4. We formulate the problem as
the projection of the vector y=b−ato the 0ncentered generalized cross-polytope {x∈Rn:/bardblAx/bardbl1≤1}.
Lemma 4. Consider the hyperplane H={x∈Rn:w/latticetopx−k= 0}and a point y∈Rn. The/lscript2projection of
yon the hyperplane is the point x∗=y−(w/latticetopy−k)w//bardblw/bardbl2
2.
Proof.We deﬁne the projection problem in a similar fashion to the ellipsoid case:
min
x1
2/bardblx−y/bardbl2
2s.t.w/latticetopx−k= 0,
and obtain the Lagrangian as L(x,λ) =1
2/bardblx−y/bardbl2
2+λ(w/latticetopx−k), from where we get (using the KKT
conditions): x∗=y−λ∗wandλ∗=w/latticetopy−k//bardblw/bardbl2
2; thus obtaining: x∗=y−(w/latticetopy−k)w
/bardblw/bardbl2
2.
While this formulation does not yield the closest point from a hyperplane when measured with the /lscript1norm,
the fact that/bardblx−x∗/bardbl1≥/bardblx−x∗/bardbl2implies the certiﬁcation set obtained in the /lscript1norm via this method is
a subset of the /lscript2-ball of the minimum projection point. Crucially, this /lscript2projection has the advantage of
having a closed-form solution, while an /lscript1one would require solving the problem using an iterative linear
programming solver. As such, for the sake of computational complexity, we decided to use this projection,
despite the sub-optimality of the result from the /lscript1perspective. Empirically, we have found this does not
aﬀect our results.
Since the set of vertices of the generalized cross-polytope {x∈Rn:/bardblAx/bardbl1≤1}is given by{ei/Aii,−ei/Aii}n
i=1,
and considering the distance between the projections and the original y, the hyperplane that minimizes it is
deﬁned by the set of vertices {sign(yi)ei/Aii}n
i=1. By writing it as a system of nequations, we obtain the
hyperplane deﬁned by w=[−sign(y1)A11,...,−sign(yn)Ann]andk= 1. Finally, after computing x∗as per
Lemma 4, we can deﬁne the maximum radius of the /lscript1-ball centered at bthat does not intersect RAas:
r∗=/bardbl(x∗+a)−b/bardbl1−/epsilon1,
for an arbitrarily small /epsilon1. Finally, and similar to the ellipsoids case, we obtain R˜Bas the maximum generalized
cross-polytope contained within RBthat has a radius smaller than r∗, that is:
R˜B={x∈Rn:/bardblx−b/bardbl1≤min{r∗,min
iBii}}
Similar to before, to guarantee that the /lscript1ballR˜Bis still a subset to RB, we take the minimum between r∗
andminiBiito be the radius of R˜B. As with the ellipsoid case, this approach does not work for the case
in whichb∈RA, since the assumption of the closest plane to ywould not hold. Thus, our classiﬁer must
abstain in that situation.
E Experimental Setup
The experiments reported in the paper used the CIFAR-10 Krizhevsky (2009)3and ImageNet Deng et al.
(2009)4datasets, and trained ResNet18, WideResNet40 and ResNet50 networks He et al. (2016). Experiments
used the typical data split for these datasets found in the PyTorch implementation Paszke et al. (2019). The
procedures to obtain the baseline networks used in the experiments are detailed in Appendix E.1 and E.2 for
ellipsoids and generalized cross-polytopes, respectively. Source code to reproduce the AnCeroptimization
and certiﬁcation results of this paper is available as supplementary material.
Isotropic DD Optimization. We used the available code of Alfarra et al. (2022)5to obtain the isotropic
data dependent smoothing parameters. To train our models from scratch, we used an adapted version of the
code provided in the same repository.
3Available here (url), under an MIT license.
4Available here (url), terms of access detailed in the Download page.
5Data Dependent Randomized Smoothing source code available here
23Under review as submission to TMLR
Certiﬁcation. Following Cohen et al. (2019); Salman et al. (2019a); Zhai et al. (2019); Yang et al. (2020);
Alfarra et al. (2022), all results were certiﬁed with N0= 100Monte Carlo samples for selection and
N= 100,000estimation samples, with failure a probability of α= 0.001.
E.1 Ellipsoid certiﬁcation baseline networks
In terms of ellipsoid certiﬁcation, the baselines we considered were Cohen Cohen et al. (2019)6,
SmoothAdv Salman et al. (2019a)7andMACER Zhai et al. (2019)8.
In the CIFAR-10 experiments, we used a ResNet18 architecture, instead of the ResNet110 used in Cohen
et al. (2019); Salman et al. (2019a); Zhai et al. (2019) due to constraints at the level of computation power.
As such, we had to train each of the networks from scratch following the procedures available in the source
code of each of the baselines. We did so under our own framework, and the training scripts are available in
the supplementary material. For the ImageNet experiments we used the ResNet50 networks provided by each
of the baselines in their respective open source repositories.
We trained the ResNet18 networks for 120 epochs, with a batch size of 256 and stochastic gradient descent
with a learning rate of 10−2, and momentum of 0.9.
E.2 Generalized Cross-Polytope certiﬁcation baseline networks
For the certiﬁcation of generalized cross-polytopes we considered RS4AYang et al. (2020)9. As described in
RS4AYang et al. (2020), we take λ=σ/√
3and report results as a function of σfor ease of comparison.
As with the baseline, we ran experiments on CIFAR-10 on a WideResNet40 architecture, and Ima-
geNet on a ResNet50 Yang et al. (2020). However, due to limited computational power, we were
not able to run experiments on the wide range of distributional parameters the original work consid-
ers,i.e.σ={0.15,0.25,0.5,0.75,1.0,1.125,1.5,1.75,2.0,2.25,2.5,2.75,3.0,3.25,3.5}on CIFAR-10 and
σ={0.25,0.5,0.75,1.0,1.125,1.5,1.75,2.0,2.25,2.5,2.75,3.0,3.25,3.5}on ImageNet. Instead, and matching
the requirements from the ellipsoid section, we choose a subset of σ={0.25,0.5,1.0}and performed our
analysis at that level.
While the trained models are available in the source code of RS4A, we ran into several issues when we
attempted to use them, the most problematic of which being the fact that the clean accuracy of such models
was very low in both the WideResNet40 and ResNet50 ones. To avoid these issues we trained the models
from scratch, but using the stability training loss as presented in the source code of RS4A. All of these
models achieved clean accuracy of over 70%.
Following the procedures described in the original work, we trained the WideResNet40 models with the
stability loss used in Yang et al. (2020) for 120 epochs, with a batch size of 128 and stochastic gradient
descent with a learning rate of 10−2, and momentum of 0.9, along with a step learning rate scheduler with a
γof 0.1. For the ResNet50 networks on ImageNet, we trained them from scratch with stability loss for 90
epochs with a learning rate of 0.1 that drops by a factor of 0.1 after each 30 epochs and a batch size of 256.
F Superset argument
The results we present in Section 7 support the argument that AnCerachieves, in general, a certiﬁcate that
is asuperset of the Fixed σand Isotropic DD ones. To conﬁrm this at an individual test set sample level, we
compare the /lscript2,/lscript1,/lscriptΣ
2and/lscriptΛ
1certiﬁcation results across the diﬀerent methods, and obtain the percentage of
the test set in which AnCerperforms at least as well as all other methods in each certiﬁcates of the samples.
Results of this analysis are presented in Tables 4 and 5.
6Cohensource code available here.
7SmoothAdv source code available here.
8MACER source code available here.
9RS4Asource code available here.
24Under review as submission to TMLR
For most networks and datasets, we observe that AnCerachieves a larger /lscriptpcertiﬁcate than the baselines in
a signiﬁcant portion of the dataset, showcasing the fact that it obtains a superset of the isotropic region per
sample. This is further conﬁrmed by the comparison with the anisotropic certiﬁcates, in which, for all trained
networks except MACER in CIFAR-10, AnCer’s certiﬁcate is superior in over 90% of the test set samples.
Table 4: Superset in top-1 /lscript2and/lscriptΣ
2(rounded to nearest percent)
%AnCer/lscript2is the best % AnCer/lscriptΣ
2is the best
CIFAR-10: Cohen 83 93
CIFAR-10: SmoothAdv 73 90
CIFAR-10: MACER 50 69
ImageNet: Cohen 94 96
ImageNet: SmoothAdv 90 93
Table 5: Superset in top-1 /lscript1and/lscriptΛ
1(rounded to nearest percent)
%AnCer/lscript1is the best % AnCer/lscriptΛ
1is the best
CIFAR-10: RS4A 100 100
ImageNet: RS4A 97 99
G Experimental Results per σ
G.1 Certifying Ellipsoids - /lscript2and/lscriptΣ
2certiﬁcation results per σ
In this section we report certiﬁed accuracy at various /lscript2radii and/lscriptΣ
2proxy radii, following the metrics
deﬁned in Section 7, for each training method ( CohenCohen et al. (2019), SmoothAdv Salman et al.
(2019a) and MACER Zhai et al. (2019)), dataset (CIFAR-10 and ImageNet) and σ(σ∈{0.12,0.25,0.5,1.0}).
Figures 7 and 8 shows certiﬁed accuracy at diﬀerent /lscript2radii for CIFAR-10 and ImageNet, respectively,
whereas Figures 9 and 10 plot certiﬁed accuracy and diﬀerent /lscriptΣ
2proxy radii for CIFAR-10 and ImageNet,
respectively.
G.2 Certifying Ellipsoids - /lscript1and/lscriptΛ
1certiﬁcation results per σ
In this section we report certiﬁed accuracy at various /lscript1radii and/lscriptΛ
1proxy radii, following the metrics deﬁned
in Section 7, for RS4a, dataset (CIFAR-10 and ImageNet) and σ(σ∈{0.25,0.5,1.0}). Figures 11 and 12
shows certiﬁed accuracy at diﬀerent /lscript1radii for CIFAR-10 and ImageNet, respectively, whereas Figures 13
and 14 plot certiﬁed accuracy and diﬀerent /lscriptΛ
1proxy radii for CIFAR-10 and ImageNet, respectively.
H Visual Comparison of Parameters in Ellipsoid Certiﬁcates
Anisotropic certiﬁcation allows for a better characterization of the decision boundaries of the base classiﬁer f.
For example, the directions aligned with the major axes of the ellipsoids /bardblδ/bardblΣ,2=r,i.e.locations where Σis
large, are, by deﬁnition, expected to be less sensitive to perturbations compared to the minor axes directions.
To visualize this concept, Figure 15 shows CIFAR-10 images along with their corresponding optimized /lscript2
isotropic parameters obtained by Isotropic DD, and /lscriptΣ
2anisotropic parameters obtained by AnCer. First, we
note the richness of information provided by the anisotropic parameters when compared to the /lscript2worst-case,
isotropic one. Interestingly, pixel locations where the intensity of Σis large (higher intensity in Figure 15) are
generally the ones corresponding least with the underlying true class and overlapping more with background
pixels.
A particular insight one can get from AnCercertiﬁcation is that the decision boundaries are not distributed
isotropically around each input. To quantify this in higher dimensions, we plot in Figure 16 a histogram
25Under review as submission to TMLR
0.00 0.25 0.50 0.75 1.00 1.25
2 radius
0.00.20.40.60.8Certified accuracyCohen (=0.12)
Fixed (ACR=0.263)
Isotropic (ACR=0.299)
ANCER (ACR=0.447)
0.0 0.5 1.0 1.5
2 radius
0.00.20.40.6Certified accuracyCohen (=0.25)
Fixed (ACR=0.402)
Isotropic (ACR=0.44)
ANCER (ACR=0.656)
0.0 0.5 1.0 1.5 2.0 2.5
2 radius
0.00.20.40.6Certified accuracyCohen (=0.5)
Fixed (ACR=0.492)
Isotropic (ACR=0.563)
ANCER (ACR=0.814)
0 1 2 3 4 5
2 radius
0.00.10.20.30.4Certified accuracyCohen (=1.0)
Fixed (ACR=0.497)
Isotropic (ACR=0.824)
ANCER (ACR=1.025)
0.00 0.25 0.50 0.75 1.00
2 radius
0.00.20.40.60.8Certified accuracySmoothAdv (=0.12)
Fixed (ACR=0.289)
Isotropic (ACR=0.466)
ANCER (ACR=0.508)
0.0 0.5 1.0 1.5
2 radius
0.00.20.40.6Certified accuracySmoothAdv (=0.25)
Fixed (ACR=0.454)
Isotropic (ACR=0.561)
ANCER (ACR=0.655)
0.0 0.5 1.0 1.5 2.0 2.5
2 radius
0.00.20.40.6Certified accuracySmoothAdv (=0.5)
Fixed (ACR=0.569)
Isotropic (ACR=0.635)
ANCER (ACR=0.761)
0 1 2 3 4 5
2 radius
0.00.10.20.30.4Certified accuracySmoothAdv (=1.0)
Fixed (ACR=0.565)
Isotropic (ACR=0.694)
ANCER (ACR=0.871)
0 1 2 3
2 radius
0.00.20.40.60.8Certified accuracyMACER (=0.12)
Fixed (ACR=0.272)
Isotropic (ACR=0.345)
ANCER (ACR=0.363)
0.0 0.5 1.0 1.5
2 radius
0.00.20.40.6Certified accuracyMACER (=0.25)
Fixed (ACR=0.429)
Isotropic (ACR=0.461)
ANCER (ACR=0.424)
0.0 0.5 1.0 1.5 2.0 2.5
2 radius
0.00.20.40.6Certified accuracyMACER (=0.5)
Fixed (ACR=0.57)
Isotropic (ACR=0.641)
ANCER (ACR=0.56)
0 1 2 3 4 5
2 radius
0.00.10.20.30.4Certified accuracyMACER (=1.0)
Fixed (ACR=0.668)
Isotropic (ACR=0.393)
ANCER (ACR=0.561)
Figure 7: CIFAR-10 certiﬁed accuracy as a function of /lscript2radius, per model and σ(used as initialization in
the isotropic data-dependent case and AnCer).
0.00 0.25 0.50 0.75 1.00 1.25
2 radius
0.00.20.40.6Certified accuracyCohen (=0.25)
Fixed (ACR=0.47)
Isotropic (ACR=0.527)
ANCER (ACR=0.653)
0.0 0.5 1.0 1.5 2.0
2 radius
0.00.20.40.6Certified accuracyCohen (=0.5)
Fixed (ACR=0.721)
Isotropic (ACR=0.795)
ANCER (ACR=1.125)
0 1 2 3 4
2 radius
0.00.10.20.30.4Certified accuracyCohen (=1.0)
Fixed (ACR=0.864)
Isotropic (ACR=0.995)
ANCER (ACR=1.439)
0.00 0.25 0.50 0.75 1.00 1.25
2 radius
0.00.20.40.6Certified accuracySmoothAdv (=0.25)
Fixed (ACR=0.518)
Isotropic (ACR=0.61)
ANCER (ACR=0.65)
0.0 0.5 1.0 1.5 2.0
2 radius
0.00.20.4Certified accuracySmoothAdv (=0.5)
Fixed (ACR=0.816)
Isotropic (ACR=0.921)
ANCER (ACR=1.083)
0 1 2 3 4
2 radius
0.00.10.20.30.4Certified accuracySmoothAdv (=1.0)
Fixed (ACR=1.011)
Isotropic (ACR=1.13)
ANCER (ACR=1.459)
Figure 8: ImageNet certiﬁed accuracy as a function of /lscript2radius, per model and σ(used as initialization in
the isotropic data-dependent case and AnCer).
of the ratio between the maximum and minimum elements of our optimized smoothing parameters for the
experiments on SmoothAdv (with an initial σ= 1.0) on CIFAR-10. We note that this ratio can be as high as
5 for some of the input points, meaning the decision boundaries in that case could be 5 times closer to a
given input for some directions than others.
26Under review as submission to TMLR
0.0 0.5 1.0 1.5
2 proxy volume
0.00.20.40.60.8Certified accuracyCohen (=0.12)
Fixed (ACR=0.263)
Isotropic (ACR=0.299)
ANCER (ACR=0.676)
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.6Certified accuracyCohen (=0.25)
Fixed (ACR=0.402)
Isotropic (ACR=0.44)
ANCER (ACR=0.873)
0 1 2 3
2 proxy volume
0.00.20.40.6Certified accuracyCohen (=0.5)
Fixed (ACR=0.492)
Isotropic (ACR=0.563)
ANCER (ACR=0.998)
0 1 2 3 4 5
2 proxy volume
0.00.10.20.30.4Certified accuracyCohen (=1.0)
Fixed (ACR=0.497)
Isotropic (ACR=0.824)
ANCER (ACR=1.163)
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.60.8Certified accuracySmoothAdv (=0.12)
Fixed (ACR=0.289)
Isotropic (ACR=0.466)
ANCER (ACR=0.821)
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.6Certified accuracySmoothAdv (=0.25)
Fixed (ACR=0.454)
Isotropic (ACR=0.561)
ANCER (ACR=0.896)
0 1 2 3
2 proxy volume
0.00.20.40.6Certified accuracySmoothAdv (=0.5)
Fixed (ACR=0.569)
Isotropic (ACR=0.635)
ANCER (ACR=0.996)
0 1 2 3 4 5
2 proxy volume
0.00.10.20.30.4Certified accuracySmoothAdv (=1.0)
Fixed (ACR=0.565)
Isotropic (ACR=0.694)
ANCER (ACR=0.992)
0 1 2 3 4 5
2 proxy volume
0.00.20.40.60.8Certified accuracyMACER (=0.12)
Fixed (ACR=0.272)
Isotropic (ACR=0.345)
ANCER (ACR=0.62)
0.0 0.5 1.0 1.5
2 proxy volume
0.00.20.40.6Certified accuracyMACER (=0.25)
Fixed (ACR=0.429)
Isotropic (ACR=0.461)
ANCER (ACR=0.581)
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.6Certified accuracyMACER (=0.5)
Fixed (ACR=0.57)
Isotropic (ACR=0.641)
ANCER (ACR=0.705)
0 1 2 3 4 5
2 proxy volume
0.00.10.20.30.4Certified accuracyMACER (=1.0)
Fixed (ACR=0.668)
Isotropic (ACR=0.393)
ANCER (ACR=0.609)
Figure 9: CIFAR-10 certiﬁed accuracy as a function of /lscriptΣ
2proxy radius, per model and σ(used as initialization
in the isotropic data-dependent case and AnCer).
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.6Certified accuracyCohen (=0.25)
Fixed (ACR=0.47)
Isotropic (ACR=0.527)
ANCER (ACR=0.989)
0.0 0.5 1.0 1.5 2.0 2.5
2 proxy volume
0.00.20.40.6Certified accuracyCohen (=0.5)
Fixed (ACR=0.721)
Isotropic (ACR=0.795)
ANCER (ACR=1.289)
0 1 2 3 4
2 proxy volume
0.00.10.20.30.4Certified accuracyCohen (=1.0)
Fixed (ACR=0.864)
Isotropic (ACR=0.995)
ANCER (ACR=1.531)
0 1 2 3
2 proxy volume
0.00.20.40.6Certified accuracySmoothAdv (=0.25)
Fixed (ACR=0.518)
Isotropic (ACR=0.61)
ANCER (ACR=0.977)
0 1 2 3
2 proxy volume
0.00.20.4Certified accuracySmoothAdv (=0.5)
Fixed (ACR=0.816)
Isotropic (ACR=0.921)
ANCER (ACR=1.268)
0 1 2 3 4
2 proxy volume
0.00.10.20.30.4Certified accuracySmoothAdv (=1.0)
Fixed (ACR=1.011)
Isotropic (ACR=1.13)
ANCER (ACR=1.547)
Figure 10: ImageNet certiﬁed accuracy as a function of /lscriptΣ
2proxy radius, per model and σ(used as initialization
in the isotropic data-dependent case and AnCer).
I Non data-dependent Anisotropic Certiﬁcation
As mentioned brieﬂy in Section 6, it is our intuition that anisotropic certiﬁcation requires a data-dependent
approach, as diﬀerent points will have fairly diﬀerent decision boundaries and the certiﬁed regions will extend
in diﬀerent directions (as exempliﬁed in Figure 1).
To validate this claim, we perform certiﬁcation of SmoothAdv Salman et al. (2019a) with an initial σ= 1on
CIFAR-10 using a Σwhich is the average of all the optimized Σx. The results of the certiﬁed accuracy, ACR
andAC˜Rare presented in Table 6, along with the same results for the methods reported in the main paper.
27Under review as submission to TMLR
0.00 0.25 0.50 0.75 1.00 1.25
1 radius
0.00.20.40.60.8Certified accuracyRS4A (=0.25)
Fixed (ACR=0.212)
Isotropic (ACR=0.479)
ANCER (ACR=0.496)
0.0 0.5 1.0 1.5
1 radius
0.00.20.40.60.8Certified accuracyRS4A (=0.5)
Fixed (ACR=0.397)
Isotropic (ACR=0.614)
ANCER (ACR=0.629)
0.0 0.5 1.0 1.5 2.0
1 radius
0.00.20.40.60.8Certified accuracyRS4A (=1.0)
Fixed (ACR=0.737)
Isotropic (ACR=0.88)
ANCER (ACR=0.91)
Figure 11: CIFAR-10 certiﬁed accuracy as a function of /lscript1radius perσ(used as initialization in the isotropic
data-dependent case and AnCer).
0.0 0.1 0.2 0.3 0.4
1 radius
0.00.20.40.6Certified accuracyRS4A (=0.25)
Fixed (ACR=0.176)
Isotropic (ACR=0.233)
ANCER (ACR=0.233)
0.0 0.2 0.4 0.6
1 radius
0.00.20.40.6Certified accuracyRS4A (=0.5)
Fixed (ACR=0.339)
Isotropic (ACR=0.378)
ANCER (ACR=0.377)
0.00 0.25 0.50 0.75 1.00
1 radius
0.00.20.40.6Certified accuracyRS4A (=1.0)
Fixed (ACR=0.642)
Isotropic (ACR=0.681)
ANCER (ACR=0.68)
Figure 12: ImageNet certiﬁed accuracy as a function of /lscript1radius perσ(used as initialization in the isotropic
data-dependent case and AnCer).
As can be observed, moving away from the data-dependent certiﬁcation in the anisotropic scenario leads to a
signiﬁcant performance drop in terms of robustness.
J Theoretical and Empirical Comparison with Mohapatra et al. (2020)
In regards to the theoretical results, unfortunately the certiﬁed regions of Mohapatra et al. (2020) do not
exhibit a closed form solution similarly to ours. Thus, a direct theoretical volume bound comparison is not
possible.
As for the empirical comparison, AnCer’s performance on both /lscript2and/lscript1certiﬁcates far out-does that of
Mohapatra et al. (2020). For example, with /lscript2certiﬁcates at a radius of 0.5, Cohen certiﬁed with AnCer
achieves 77%certiﬁed accuracy (see Table 1) while Mohapatra et al. (2020) achieves under 60%certiﬁed
accuracy. Note that Mohapatra et al. (2020) has only a marginal improvement over Cohen et al. As for
the/lscript1certiﬁcates, Mohapatra et al. (2020) uses the Gaussian distribution of Cohen et al, resulting in worse
performance than existing state-of-art in /lscript1Yang et al. (2020) that uses a uniform distribution. Our approach
improves further upon the performance of Yang et al. (2020). For example, as per Table 2, RS4A with
ANCER certiﬁcation achieves 84%certiﬁed accuracy at an /lscript1radius of 0.5, Yang et al. (2020) achieves
75%certiﬁed accuracy while Mohapatra et al. (2020) achieves below 60%. However, we believe that the
combination of both approaches, ANCER and Mohapatra et al. (2020) can further boost the performance as
also hinted on in the abstract of Mohapatra et al. (2020) on the use of data-dependent smoothing.
28Under review as submission to TMLR
0.00 0.25 0.50 0.75 1.00 1.25
1 proxy volume
0.00.20.40.60.8Certified accuracyRS4A (=0.25)
Fixed (ACR=0.212)
Isotropic (ACR=0.479)
ANCER (ACR=0.665)
0.0 0.5 1.0 1.5
1 proxy volume
0.00.20.40.60.8Certified accuracyRS4A (=0.5)
Fixed (ACR=0.397)
Isotropic (ACR=0.614)
ANCER (ACR=0.781)
0.0 0.5 1.0 1.5 2.0 2.5
1 proxy volume
0.00.20.40.60.8Certified accuracyRS4A (=1.0)
Fixed (ACR=0.737)
Isotropic (ACR=0.88)
ANCER (ACR=1.016)
Figure 13: CIFAR-10 certiﬁed accuracy as a function of /lscriptΛ
1proxy radius per σ(used as initialization in the
isotropic data-dependent case and AnCer).
0 1 2 3
1 proxy volume
0.00.20.40.6Certified accuracyRS4A (=0.25)
Fixed (ACR=0.176)
Isotropic (ACR=0.233)
ANCER (ACR=1.421)
0.0 0.5 1.0 1.5 2.0
1 proxy volume
0.00.20.40.6Certified accuracyRS4A (=0.5)
Fixed (ACR=0.339)
Isotropic (ACR=0.378)
ANCER (ACR=1.053)
0.0 0.5 1.0 1.5 2.0 2.5
1 proxy volume
0.00.20.40.6Certified accuracyRS4A (=1.0)
Fixed (ACR=0.642)
Isotropic (ACR=0.681)
ANCER (ACR=1.075)
Figure 14: ImageNet certiﬁed accuracy as a function of /lscriptΛ
1proxy radius per σ(used as initialization in the
isotropic data-dependent case and AnCer).
Figure 15: Visualization of an input CIFAR-10 image x(top), and the optimized parameters σ(middle)
andΣ(bottom) – higher intensity corresponds to higher σiin that pixel and channel – of the smoothing
distributions in the isotropic and anisotropic case, respectively.
1 2 3 4 5
x
max/x
min
0.00.10.20.30.40.50.6
Figure 16: Distribution of the maximum over the minimum AnCerσxat each dataset point for
SmoothAdv Salman et al. (2019a) on CIFAR-10 (for initial σ= 1.0)
29Under review as submission to TMLR
Table 6: Comparison of diﬀerent certiﬁcation methods on SmoothAdv with an initial σ= 1.0on CIFAR-10.
CIFAR-10 SmoothAdvAccuracy @ /lscript2radius (%)/lscript2ACR/lscriptΣ
2AC˜R0.0 0.25 0.5 1.0 1.5 2.0 2.5
σ= 1.0Fixedσ 45 40 35 25 16 9 5 0.565 0.565
Isotropic DD 41 39 36 29 21 14 7 0.694 0.694
AnCer 44 43 41 35 26 15 8 0.871 0.992
Average Σ29 25 21 14 9 5 2 0.329 0.379
30